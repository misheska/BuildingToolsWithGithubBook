{"\"Hacking with Github?\\nHacking on Github?\"":{"page":1,"text":"Hacking GitHub\n\n\nChris Dawson with Ben Straub","initials":"ew"},"\"This introduction is very artistic, it feels like it sets the wrong tone? Perhaps start with the first point in the second paragraph and then give a cut down version of the list? Otherwise the reader maybe left wondering if they picked up the wrong book..\\n.\"":{"page":9,"text":"                                               Preface            1\n\n\n\n\n\n\n\n\nIn the 1930s, The Eagle and Child, a pub in Oxford simmered with creativity as\nJRR Tolkien and CS Lewis philosophized and created their fantasy worlds.\nHeian court life in 11th century Kyoto cradled Murasaki Shibu and produced\n\nThe Tale of Genji, Japan’s greatest novel. In the 7th century during the\nUmayyad Caliphate of modern day Damascus, glittering Arabic palaces provid-\ned fertile ground for the creation of a new form of poetry, the ghazal (“love po-\nems”) the influence of which still courses through modern Arabic poetry.\n  Each era of major creative innovation has been backdropped by a unique\nand plumb space where collaboration and creativity flourished. Today, the col-\nlaborative gathering space for some of the world’s greatest artists (artists who\n\nwork as software developers) is a virtual town square, a site called GitHub.\n\n\nWho You Are\n\nThis book should be an interesting source of information for people who have\nused Git or GitHub and want to “level-up” their skills related to these technolo-\ngies. People without any experience using GitHub or Git should start with an in-\ntroductory book on these technologies.\n\n  You should have a good familiarity with at least one imperative modern pro-\ngramming language. You don’t need to be an expert programmer to read this\nbook, but having some programming experience and familiarity with at least\none language is essential.\n  You should understand the basics of the HTTP protocol. The GitHub team\nuses a very standard RESTful approach for its API. You should understand the\ndifference between a GET request and POST request and what HTTP status\ncodes mean at the very least.\n\n  Familiariy with web APIs is helpful, although this book simultaneously as-\npires to provide a guide showing how a well thought out, well designed, and\nwell tested web API creates a foundation for building fun and powerful tools. If\n\n\n\n\n                                                                         9","initials":"ew"},"\"phrasing send a bit strange\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n                        of APIs, this is good company.\n\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n                        projects and are now interested in using Git within a larger community, this\n\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n                        Hub.\n\n                           Like any seasoned developer, automation of your tools is important to you,\n                        and this book provides examples of mundane tasks that we then convert them\n\n                        into automated and repeatable processes, and we show how to do this using a\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n\n                        understanding of how to use it, and we bet you will find great power there. To\n                        make this book accessible to everyone, regardless of their editor or operating\n\n                        system, many of the programming samples work within the command line. If\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n                        very likely see a new way of working in the modern age. We focus on these\n\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n\n                        you can review the code discussed. Fork away and take these samples into your\n                        own projects and tools!\n\n                           Finally, we help you write testable API backed code. Even the most experi-\n                        enced developers often find that writing tests for their code is a challenge, de-\n\n                        spite the massive body of literature connecting quality code with tests. Testing\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n\n        10","initials":"ew"},"\"Not sure what this means, is this a joke?\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n                        of APIs, this is good company.\n\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n                        projects and are now interested in using Git within a larger community, this\n\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n                        Hub.\n\n                           Like any seasoned developer, automation of your tools is important to you,\n                        and this book provides examples of mundane tasks that we then convert them\n\n                        into automated and repeatable processes, and we show how to do this using a\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n\n                        understanding of how to use it, and we bet you will find great power there. To\n                        make this book accessible to everyone, regardless of their editor or operating\n\n                        system, many of the programming samples work within the command line. If\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n                        very likely see a new way of working in the modern age. We focus on these\n\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n\n                        you can review the code discussed. Fork away and take these samples into your\n                        own projects and tools!\n\n                           Finally, we help you write testable API backed code. Even the most experi-\n                        enced developers often find that writing tests for their code is a challenge, de-\n\n                        spite the massive body of literature connecting quality code with tests. Testing\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n\n        10","initials":"ew"},"\"This is slightly more amusing, humour is great to have but you have to be wary that it is not esoteric or offensive.\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n                        of APIs, this is good company.\n\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n                        projects and are now interested in using Git within a larger community, this\n\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n                        Hub.\n\n                           Like any seasoned developer, automation of your tools is important to you,\n                        and this book provides examples of mundane tasks that we then convert them\n\n                        into automated and repeatable processes, and we show how to do this using a\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n\n                        understanding of how to use it, and we bet you will find great power there. To\n                        make this book accessible to everyone, regardless of their editor or operating\n\n                        system, many of the programming samples work within the command line. If\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n                        very likely see a new way of working in the modern age. We focus on these\n\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n\n                        you can review the code discussed. Fork away and take these samples into your\n                        own projects and tools!\n\n                           Finally, we help you write testable API backed code. Even the most experi-\n                        enced developers often find that writing tests for their code is a challenge, de-\n\n                        spite the massive body of literature connecting quality code with tests. Testing\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n\n        10","initials":"ew"},"\"Great to give readers a solid benefit of this book.\"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\nto write code which interacts with the GitHub API and is testable.\n\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n\n      founders of GitHub.\n    • JavaScript: the only ubiquitous browser side programming language, its\n\n      importance has grown to new heights with the introduction of NodeJS,\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\ntween them you will have a solid toolset to begin exploration of the GitHub API.\n\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\nlearn at least a little bit will make the journey through this book richer for you.\n\n   Undoubtedly, many of you picking up this book already have familiarity with\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\n\nlanguages; we expect you have experience with other languages as a prerequi-\nsite and can read code from any imperative language regardless of the syntax.\n\nLater chapters which do discuss a facet of the API go into language details at\ntimes, and the code is readable regardless of your familiarity with that particu-\n\nlar language. These explanatory appendices discuss the history of these tools\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\n\ndifferent languages. We do this to describe the API from not only the way the\nGitHub team designed it to work, but the aspirational way that client library\n\nauthors made it work within diverse programming languages and communities.\nWe think there is a lot to learn from this approach, but if you are interested in\n\nonly a specific language and how it works with the GitHub API, this is not the\nbook for you.\n\n\n\n\n\n\n                                                                                            11","initials":"ew"},"\"The qualification that the code should be readable seems to sit at odds with the comment that the reader should know Ruby and JS.\\nI would be inclined to state they should have a working knowledge of both languages and leave it as that.\"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\nto write code which interacts with the GitHub API and is testable.\n\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n\n      founders of GitHub.\n    • JavaScript: the only ubiquitous browser side programming language, its\n\n      importance has grown to new heights with the introduction of NodeJS,\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\ntween them you will have a solid toolset to begin exploration of the GitHub API.\n\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\nlearn at least a little bit will make the journey through this book richer for you.\n\n   Undoubtedly, many of you picking up this book already have familiarity with\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\n\nlanguages; we expect you have experience with other languages as a prerequi-\nsite and can read code from any imperative language regardless of the syntax.\n\nLater chapters which do discuss a facet of the API go into language details at\ntimes, and the code is readable regardless of your familiarity with that particu-\n\nlar language. These explanatory appendices discuss the history of these tools\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\n\ndifferent languages. We do this to describe the API from not only the way the\nGitHub team designed it to work, but the aspirational way that client library\n\nauthors made it work within diverse programming languages and communities.\nWe think there is a lot to learn from this approach, but if you are interested in\n\nonly a specific language and how it works with the GitHub API, this is not the\nbook for you.\n\n\n\n\n\n\n                                                                                            11","initials":"ew"},"\"Good clear statement\"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\nto write code which interacts with the GitHub API and is testable.\n\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n\n      founders of GitHub.\n    • JavaScript: the only ubiquitous browser side programming language, its\n\n      importance has grown to new heights with the introduction of NodeJS,\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\ntween them you will have a solid toolset to begin exploration of the GitHub API.\n\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\nlearn at least a little bit will make the journey through this book richer for you.\n\n   Undoubtedly, many of you picking up this book already have familiarity with\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\n\nlanguages; we expect you have experience with other languages as a prerequi-\nsite and can read code from any imperative language regardless of the syntax.\n\nLater chapters which do discuss a facet of the API go into language details at\ntimes, and the code is readable regardless of your familiarity with that particu-\n\nlar language. These explanatory appendices discuss the history of these tools\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\n\ndifferent languages. We do this to describe the API from not only the way the\nGitHub team designed it to work, but the aspirational way that client library\n\nauthors made it work within diverse programming languages and communities.\nWe think there is a lot to learn from this approach, but if you are interested in\n\nonly a specific language and how it works with the GitHub API, this is not the\nbook for you.\n\n\n\n\n\n\n                                                                                            11","initials":"ew"},"\"presenting all this information in a bullet point list is a bit dry.\"":{"page":16,"text":"CHAPTER 2: Introduction\n\n\n                         these chapters are stories about building applications on top of the technolo-\n                         gies provided by GitHub. Within these stories you will learn the tradeoffs and\n\n                         considerations you will face when you use the GitHub API. Chapters in this book\n                         often cover multiple pieces of the API when appropriate for the story we are\n                         telling. We’ve generally tried to focus on a major API section and limit exposure\n\n                         to other pieces as much as possible, but most chapters do need to bring in\n                         small pieces of more than one section.\n\n\n                             • The “cURL” chapter: the chapter you are reading now covers a first look at\n                               the API through the command line HTTP client called cURL. We talk a bit\n                               about the response format and how to parse it within the command line,\n\n                               and also document authentication.\n\n                             • The “Gist” chapter: covers the Gist API, as well as command line tools and\n                               the Ruby language “Octokit” API client.\n\n                             • The “Gollum” chapter: explains usage of the Gollum command line tool\n                               and associated Ruby library (gem) which is backed by Grit, the C-\n\n                               language bindings for accessing Git repositories. We also document some\n                               details of the Git storage format and how it applies to storing large files\n\n                               inside of a Git repository, and show how to use the git command line\n                               tools to play with this information.\n\n                             • The “Search” chapter: we build a GUI search app using Python.\n\n                             • The “Commit Status” chapter: our final chapter documents a relatively\n                               new part of the API which documents the interactions between third par-\n\n                               ty tools and your code. This chapter builds an application using C# and\n                               the Nancy .NET GitHub API libraries.\n\n                             • The “Jekyll” chapter: if you push a specifically organized repository into\n                               GitHub, GitHub will host a fully featured blog, equivalent in most ways to\n\n                               a Wordpress site (well, except for the complexity part). This chapter docu-\n                               ments how to format your repository, how to use Markdown within Je-\n                               kyll, how to use programmatic looping constructs provided by Liquid\n\n                               Templates, and then shows how to import an entire web site from the In-\n                               ternet Archive into the Jekyll format using Ruby. We show how to respect-\n\n                               fully spider a site using caching, a valuable technique when using APIs or\n                               third party public information.\n\n                             • The “Android” chapter: in this chapter we create a mobile application tar-\n                               geting the Android OS. Our application reads and writes information into\n\n                               a Jekyll repository from the Git Data section of the API. We show how to\n                               create user interface tests for Android which verify GitHub API responses\n\n                               using the Calabash UI testing tool.\n\n                             • The “JavaScript” chapter: did you know you can host an entire “single\n                               page application” on GitHub? We show how you can build an application\n\n\n\n\n        16","initials":"ew"},"\"There is a little repetion from the 1st chapter here\"":{"page":17,"text":"                                                                                                Introduction\n\n\n      backed by a database called GitHub using the JavaScript language. Im-\n      portanly, we show how you can write a testable JavaScript application\n\n      that mocks out the GitHub API when needed.\n    • The “Hubot” chapter: Hubot is a JavaScript (NodeJS) chat robot enabling\n\n      technologists to go beyond developer operations (“DevOps”) to a new\n      frontier called “ChatOps.” The Hubot chapter illustrates using the Activi-\n\n      ties and Pull Requests section of the API. In addition we show how you\n      can simulate GitHub notifications and how to write testable Hubot exten-\n\n      sions (which is often a challenge when writing JavaScript code).\n\n   We don’t cover the organization API: this is a small facet of the API with only\nthe ability to list organizations and modify metadata about your organization;\n\nonce you have used other parts of the API this nook of the API will be very intu-\nitive.\n\n   We also don’t cover the users section of the API. While you might expect it to\nbe an important part of the API, the users API is really nothing more than an\nendpoint to list information about users, add or remove SSH keys, adjust email\n\naddresses and modify your list of followers.\n   There is not a specific chapter on issues. Historically GitHub used to group\n\nissues and pull requests into the same API section, but with the growing impor-\ntance of pull requests they have separated them in the API documentation. In\n\nfact, they are still internally stored in the same database and pull requests are,\nat least for now, just another type of issue. The Hubot chapter documents using\n\npull requests and is a good reference for issues in that way.\n   The enterprise API works almost exactly the same as the GitHub.com site\nAPI. We don’t have a chapter telling a story about the enterprise API, but we do\n\nprovide an appendix which provides a few notes about how to use it with a few\nAPI client libraries.\n\n   With these chapters we cover the entire API and hope to give you an inside\nlook into the inner workings of the brain of a developer building on top of the\n\nGitHub API.\n   As you might have noticed, this book will take you on an exploration of sev-\neral different language clients for the GitHub API. Along the way, we’ll point out\n\nthe different idioms and methodologies inherent to those client libraries and\nshed light on the darker corners of the GitHub API. Don’t be alarmed if you\n\nthumb through the chapters and see a language which you don’t know at all:\neach chapter is designed so that you can follow along without intimacy to the\n\nlanguage or toolkit. You will get the most value if you install the language and\nassociated tools, but the story behind the projects we will build will be interest-\n\ning even if you don’t actually type a line of code from the chapter.\n   Enough of the theoretical: let’s jump into using the API with the powerful\ncURL tool.\n\n\n\n\n\n                                                                                                 17","initials":"ew"},"\"All of this is very abstract, it feels like we should be discussing concrete pieces of code/APIs\"":{"page":19,"text":"                                                                      Breadcrumbs to Successive API Paths\n\n\n   We’ve abbreviated the response to make it more readable. A few salient\nthings to notice: there are a lot of URLs pointing to secondary information, pa-\nrameters are included in the URLs, and the response format is JSON.\n\n   What can we learn from this API response?\n\n\n\nBreadcrumbs to Successive API Paths\n\nThe GitHub API is a Hypermedia API. Though a discussion on what constitutes\n\nhypermedia deserves an entire book of its own (Check out O’Reilly’s “Hyperme-\ndia APIs with HTML5 and Node”), you can absorb much of what makes hyper-\n\nmedia interesting by just looking at a response. First, you can see from the API\nresponse above that by making a request to the API, you actually get back a\n\nmap of how you should make additional responses. Not all clients use this in-\nformation, of course, but one goal behind Hypermedia APIs is that clients can\ndynamically adjust their endpoints without recoding the client code. In other\n\nwords, the API should be able to adjust its map, and then clients will adjust\nthemselves, but you as the application developer using the client libraries will\n\nnot need to understand or even be aware of the changes. If the thought of Git-\nHub changing an API because clients should be written to handle new end-\n\npoints automatically sounds worriesome, don’t fret too much: GitHub is very\ndilligent about maintaining and supporting their API in a way that most compa-\nnies would do well to emulate. But, you should know that you can rely on hav-\n\ning a API reference inside the API itself, rather than hosted externally in docu-\nmentation which very easily could turn out to be out of date with the API itself.\n\n   This map includes not just URLs, but also information about how to provide\nparameters to the URLs. For example, the  code_search_url     key references a\n\nURL which obviously allows you to search within code on GitHub, but also tells\nyou how to structure the parameters passed to this URL. If you have an intelli-\ngent client who can follow this simple format, you could dynamically generate\n\nthe query without involving a developer who can read API documentation. At\nleast that is the dream that Hypermedia points us to; if you are skeptical, at\n\nleast know that APIs such as GitHub encode documentation into themselves,\nand you can bet GitHub has test coverage to prove that this documentation\n\nmatches the information delivered by the API endpoints. That’s a strong guar-\nantee that is sadly missing from many other APIs.\n   Now let’s briefly discuss the format of all GitHub API responses: JSON.\n\n\n\nThe JavaScript Object Notation (JSON) Format\n\n\nEvery response you get back from the GitHub API will be in the JSON format.\nJSON is a “lightweight data interchange format” (read more on the JSON.org\n\n\n\n\n                                                                                           19","initials":"ew"},"\"Could this be summarised by simply saying that the APIs are self documenting?\"":{"page":19,"text":"                                                                      Breadcrumbs to Successive API Paths\n\n\n   We’ve abbreviated the response to make it more readable. A few salient\nthings to notice: there are a lot of URLs pointing to secondary information, pa-\nrameters are included in the URLs, and the response format is JSON.\n\n   What can we learn from this API response?\n\n\n\nBreadcrumbs to Successive API Paths\n\nThe GitHub API is a Hypermedia API. Though a discussion on what constitutes\n\nhypermedia deserves an entire book of its own (Check out O’Reilly’s “Hyperme-\ndia APIs with HTML5 and Node”), you can absorb much of what makes hyper-\n\nmedia interesting by just looking at a response. First, you can see from the API\nresponse above that by making a request to the API, you actually get back a\n\nmap of how you should make additional responses. Not all clients use this in-\nformation, of course, but one goal behind Hypermedia APIs is that clients can\ndynamically adjust their endpoints without recoding the client code. In other\n\nwords, the API should be able to adjust its map, and then clients will adjust\nthemselves, but you as the application developer using the client libraries will\n\nnot need to understand or even be aware of the changes. If the thought of Git-\nHub changing an API because clients should be written to handle new end-\n\npoints automatically sounds worriesome, don’t fret too much: GitHub is very\ndilligent about maintaining and supporting their API in a way that most compa-\nnies would do well to emulate. But, you should know that you can rely on hav-\n\ning a API reference inside the API itself, rather than hosted externally in docu-\nmentation which very easily could turn out to be out of date with the API itself.\n\n   This map includes not just URLs, but also information about how to provide\nparameters to the URLs. For example, the  code_search_url     key references a\n\nURL which obviously allows you to search within code on GitHub, but also tells\nyou how to structure the parameters passed to this URL. If you have an intelli-\ngent client who can follow this simple format, you could dynamically generate\n\nthe query without involving a developer who can read API documentation. At\nleast that is the dream that Hypermedia points us to; if you are skeptical, at\n\nleast know that APIs such as GitHub encode documentation into themselves,\nand you can bet GitHub has test coverage to prove that this documentation\n\nmatches the information delivered by the API endpoints. That’s a strong guar-\nantee that is sadly missing from many other APIs.\n   Now let’s briefly discuss the format of all GitHub API responses: JSON.\n\n\n\nThe JavaScript Object Notation (JSON) Format\n\n\nEvery response you get back from the GitHub API will be in the JSON format.\nJSON is a “lightweight data interchange format” (read more on the JSON.org\n\n\n\n\n                                                                                           19","initials":"ew"},"\"The writing style is prone to run on sentences\"":{"page":19,"text":"                                                                      Breadcrumbs to Successive API Paths\n\n\n   We’ve abbreviated the response to make it more readable. A few salient\nthings to notice: there are a lot of URLs pointing to secondary information, pa-\nrameters are included in the URLs, and the response format is JSON.\n\n   What can we learn from this API response?\n\n\n\nBreadcrumbs to Successive API Paths\n\nThe GitHub API is a Hypermedia API. Though a discussion on what constitutes\n\nhypermedia deserves an entire book of its own (Check out O’Reilly’s “Hyperme-\ndia APIs with HTML5 and Node”), you can absorb much of what makes hyper-\n\nmedia interesting by just looking at a response. First, you can see from the API\nresponse above that by making a request to the API, you actually get back a\n\nmap of how you should make additional responses. Not all clients use this in-\nformation, of course, but one goal behind Hypermedia APIs is that clients can\ndynamically adjust their endpoints without recoding the client code. In other\n\nwords, the API should be able to adjust its map, and then clients will adjust\nthemselves, but you as the application developer using the client libraries will\n\nnot need to understand or even be aware of the changes. If the thought of Git-\nHub changing an API because clients should be written to handle new end-\n\npoints automatically sounds worriesome, don’t fret too much: GitHub is very\ndilligent about maintaining and supporting their API in a way that most compa-\nnies would do well to emulate. But, you should know that you can rely on hav-\n\ning a API reference inside the API itself, rather than hosted externally in docu-\nmentation which very easily could turn out to be out of date with the API itself.\n\n   This map includes not just URLs, but also information about how to provide\nparameters to the URLs. For example, the  code_search_url     key references a\n\nURL which obviously allows you to search within code on GitHub, but also tells\nyou how to structure the parameters passed to this URL. If you have an intelli-\ngent client who can follow this simple format, you could dynamically generate\n\nthe query without involving a developer who can read API documentation. At\nleast that is the dream that Hypermedia points us to; if you are skeptical, at\n\nleast know that APIs such as GitHub encode documentation into themselves,\nand you can bet GitHub has test coverage to prove that this documentation\n\nmatches the information delivered by the API endpoints. That’s a strong guar-\nantee that is sadly missing from many other APIs.\n   Now let’s briefly discuss the format of all GitHub API responses: JSON.\n\n\n\nThe JavaScript Object Notation (JSON) Format\n\n\nEvery response you get back from the GitHub API will be in the JSON format.\nJSON is a “lightweight data interchange format” (read more on the JSON.org\n\n\n\n\n                                                                                           19","initials":"ew"},"\"Feels a little redundant explaining JSON when the reader is expected to be proficient with JS.\"":{"page":20,"text":"CHAPTER 2: Introduction\n\n\n                       website). There are other competing and effective formats, such as XML or\n\n                       YAML, but JSON is quickly becoming the defacto standard for web services.\n                          A few of the reasons why JSON is so popular:\n\n                           • JSON is readable: JSON has a nice balance of human readability when\n\n                             compared to serialization formats like XML.\n\n                           • JSON can be used within JavaScript with very little modification (and\n                             cognitive processing on the part of the programmer). A data format which\n\n                             works equally well on both the client and server side was bound to be vic-\n                             torious, as JSON has been.\n\n                          You might expect that a site like GitHub, originally built on the Ruby on Rails\n\n                       stack (and some of that code is still live), would support specifying an alterna-\n                       tive format like XML, but XML is no longer supported. Long live JSON.\n\n                          JSON is very straightforward if you have used any other text based inter-\n                       change format. One note about JSON that is not always obvious or expected to\n\n                       people new to JSON: the format only supports using double-quotes, not single-\n                       quotes.\n\n                          We are using a command line tool, cURL, to retrieve data from the API. It\n                       would be handy to have a simple command line tool that also processes that\n\n                       JSON. Let’s talk about one such tool next.\n\n\n                       Parsing JSON from the Command Line\n\n\n                       JSON is a text format, so you could use any command line text processing tool,\n                       such as the venerable AWK, to process JSON responses. There is one fantastic\n\n                       JSON specific parsing tool which complements cURL that is worth knowing:\n                       “jq”. If you pipe JSON content (using t|echaracter for most shells) into jq, you\n\n                       can then easily extract pieces of the JSON using “filters.”\n\n\n                            INSTALLING JQ\n\n                            jq can be installed from source, using package managers brewor apt-\n                            get, and there are binaries on the downloads page for OSX, Linux, Win-\n\n                            dows and Solaris.\n\n\n                          Going deeper, in the prior example, let’s pull out something interesting from\n                       the API map that we receive when we access api.github.com.\n\n\n                          $ curl https://api.github.com | jq '.current_user_url'\n                            % Total     % Received % Xferd Average Speed        Time     Time      Time Current\n\n                                                              Dload Upload      Total    Spent     Left Speed\n                          100 2004 100 2004         0      0   4496       0 --:--:-- --:--:-- --:--:-- 4493\n                          \"https://api.github.com/user\"\n\n\n\n\n        20","initials":"ew"},"\"The parsing JSON topic is somewhat dry\"":{"page":22,"text":"CHAPTER 2: Introduction\n\n\n                         $ echo \"{ 'no' : 'bueno' }\" | jq \".\"\n                         parse error: Invalid numeric literal at line 1, column 7\n\n                         The first JSON we pass into jq works, while the second, because it uses inva-\n\n                      lid single quote characters, fails with an error. Jq filters are strings passed as\n                      arguments, and the shell which provides the string to jq does not care, howev-\n\n                      er, if you use single quotes or double quotes, as you see above. The echo com-\n                      mand, if you didn’t already know, prints out whatever string you provide to it;\n                      when we combine this with the pipe character we can easily provide that string\n\n                      to jq through standard input.\n                         Jq is a powerful tool for quickly retrieving content from an arbitray JSON re-\n\n                      quest.   Jq   has   many    other   powerful   features,   documented    at\n                      stedolan.github.io/jq.\n                         We now know how to retrieve some interesting information from the GitHub\n\n                      API and parse out bits of information from that response, all in a single line.\n                      But, there will be times when you incorrectly specify parameters to cURL or the\n\n                      API, and the data is not what you expect. Now we’ll learn about how to debug\n                      the cURL tool and the API service itself to provide more context when things go\n\n                      wrong.\n\n\n                      Debugging Switches for cURL\n\n\n                      As mentioned, cURL is a great tool when you are verifying that a response is\n                      what you expect it to be. The response body is important, but often you’ll want\n\n                      access to the headers as well. cURL makes getting these easy with t-i and -\n                      v switches. The -i switch prints out request headers, and th-v switch prints\n\n                      out both request and response headers (the> character indicates request data,\n                      and the < character indicates response data).\n\n\n                         $ curl -i https://api.github.com\n                         HTTP/1.1 200 OK\n                         Server: GitHub.com\n\n                         Date: Wed, 03 Jun 2015 19:39:03 GMT\n                         Content-Type: application/json; charset=utf-8\n                         Content-Length: 2004\n                         Status: 200 OK\n                         X-RateLimit-Limit: 60\n\n                         ...\n                         {\n                           \"current_user_url\": \"https://api.github.com/user\",\n                           ...\n                         }\n\n                         $ curl -v https://api.github.com\n                         * Rebuilt URL to: https://api.github.com/\n\n\n\n\n       22","initials":"ew"},"\"Talking about debugging switches for curl seems kind of far removed from hacking GitHub?\"":{"page":23,"text":"                                                                               All The Headers and Data\n\n\n   * Hostname was NOT found in DNS cache\n   *   Trying 192.30.252.137...\n   * Connected to api.github.com (192.30.252.137) port 443 (#0)\n   * successfully set certificate verify locations:\n\n   *   CAfile: none\n     CApath: /etc/ssl/certs\n   * SSLv3, TLS handshake, Client hello (1):\n   * SSLv3, TLS handshake, Server hello (2):\n   ...\n\n   * CN=DigiCert SHA2 High Assurance Server CA\n   *         SSL certificate verify ok.\n   > GET / HTTP/1.1\n   > User-Agent: curl/7.35.0\n\n   > Host: api.github.com\n   > Accept: */*\n   >\n   < HTTP/1.1 200 OK\n   * Server GitHub.com is not blacklisted\n\n   ...\n\n   With the -v switch you get everything: DNS lookups, information on the SSL\n\nchain, and the full request and response information.\n\n\n     Be aware that if you print out headers, a tool like jq will get confused be-\n     cause you are no longer providing it with pure JSON.\n\n\n   This section shows us that there is interesting information not only in the\n\nbody (the JSON data) but also in the headers. It is important to understand\nwhat headers are here and which ones are important. There are a lot of them\nwhich the HTTP specification requires, and we can often ignore those, but there\n\nare a few that are vital when you start making more than just a few isolated re-\nquest.\n\n\n\nAll The Headers and Data\n\n\nThree headers are present in every GitHub API response which tell you about\n\nthe GitHub API rate limits. They are X-RateLimit-Limit, X-RateLimit-Remaining,\nand X-RateLimit-Reset. These limits are explained in detail in “GitHub API Rate\nLimits”.\n\n   The X-GitHub-Media-Type header contains information that will come in\nhandy when you are starting to retrieve text or blob content from the API. when\n\nyou make a request to the GitHub API you can specify the format you want to\nwork with by sending an Accept header with your request.\n\n   Let’s look at the full response from the original request.\n\n\n\n\n                                                                                         23","initials":"ew"},"\"Displaying all of this is a bit daunting, it presents a wall of text that is of limited use to the reader. Reconsider if this is needed and how you might present it differently?\"":{"page":24,"text":"CHAPTER 2: Introduction\n\n\n                         $ curl -i https://api.github.com/\n                         HTTP/1.1 200 OK\n                         Server: GitHub.com\n                         Date: Sat, 25 Apr 2015 05:36:16 GMT\n\n                         Content-Type: application/json; charset=utf-8\n                         Content-Length: 2004\n                         Status: 200 OK\n                         X-RateLimit-Limit: 60\n                         X-RateLimit-Remaining: 58\n\n                         X-RateLimit-Reset: 1429943754\n                         Cache-Control: public, max-age=60, s-maxage=60\n                         ETag: \"a5c656a9399ccd6b44e2f9a4291c8289\"\n                         Vary: Accept\n\n                         X-GitHub-Media-Type: github.v3\n                         X-XSS-Protection: 1; mode=block\n                         X-Frame-Options: deny\n                         Content-Security-Policy: default-src 'none'\n                         Access-Control-Allow-Credentials: true\n\n                         Access-Control-Expose-Headers: ETag, Link, X-GitHub-OTP, X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Sc▯opes, X-Accepted-OAuth-Scopes, X-Poll-Interval\n                         Access-Control-Allow-Origin: *\n                         X-GitHub-Request-Id: C0F1CF9E:567A:9610FCB:553B27D0\n                         Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n                         X-Content-Type-Options: nosniff\n\n                         Vary: Accept-Encoding\n                         X-Served-By: 13d09b732ebe76f892093130dc088652\n                         {\n                           \"current_user_url\": \"https://api.github.com/user\",\n\n                           \"current_user_authorizations_html_url\":\n                         \"https://github.com/settings/connections/applications{/client_id}\",\n                           \"authorizations_url\": \"https://api.github.com/authorizations\",\n                           \"code_search_url\":\n                         \"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\n\n                           ...\n                           \"notifications_url\": \"https://api.github.com/notifications\",\n                           \"organization_repositories_url\":\n                         \"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\n\n                           \"organization_url\": \"https://api.github.com/orgs/{org}\",\n                           \"public_gists_url\": \"https://api.github.com/gists/public\",\n                           \"rate_limit_url\": \"https://api.github.com/rate_limit\",\n                           \"repository_url\": \"https://api.github.com/repos/{owner}/{repo}\",\n                           ...\n\n                         }\n\n                         Using this map, is there anything interesting we can retrieve, perhaps infor-\n\n                      mation about GitHub itself? We can use the organizational URL and substitute\n                      “github” in the placeholder.\n\n\n                         $ curl https://api.github.com/orgs/github\n                         {\n\n\n\n\n       24","initials":"ew"},"\"The flow of the chapter feels a bit abrupt here... we have gone from curl headers to http authentication suddenly. \\n\\nIt feels like there needs to be more context:\\n\\nWhat are we trying to teach the user? Why should they care? \\nWhere is this going? \\nWh\\nat will they be able to do once we finish this lesson?\"":{"page":26,"text":"CHAPTER 2: Introduction\n\n\n                        authentication supported by the   -u flag in curl. HTTP Basic Authentication is\n                        synonymous with username and password authentication.\n\n\n                           $ curl -u xrd https://api.github.com/rate_limit\n                           Enter host password for user 'xrd': xxxxxxxx\n                           {\n\n                             \"rate\": {\n                               \"limit\": 5000,\n                               \"remaining\": 4995,\n                               \"reset\": 1376251941\n\n                             }\n                           }\n\n                           This cURL command, authenticates into the GitHub API and then retrieves\n\n                        information about our own specific rate limits for our user account, protected\n                        information only available as a logged in user.\n\n\n                        BENEFITS OF USERNAME AUTHENTICATION\n\n\n                        Almost any client library you use will support HTTP Basic authentication. All the\n                        GitHub API clients we looked at support username and passwords. And, writing\n\n                        your own specific client is easy as this is a core feature of the HTTP standard, so\n                        if you use any standard HTTP library when building your own client, you will be\n\n                        able to access content inside the GitHub API.\n\n\n                        DOWNSIDES TO USERNAME AUTHENTICATION\n\n                        There are many reasons why username and password authentication is the\n\n                        wrong way to manage your GitHub API access.\n\n                            • HTTP Basic is an old protocol which never anticipated the granularity of\n                              web services. It is not possible to specify only certain features of a web\n\n                              service if you ask users to authenticate with username/passwords.\n\n                            • If you use a username and password to access GitHub API content from\n                              your cell phone, and then access API content from your laptop, you have\n                              no way to block access to one without blocking the other.\n\n                            • HTTP Basic authentication does not support extensions to the authenti-\n\n                              cation flow. Many modern services now support two-factor authentica-\n                              tion and there is no way to inject this into the process without changing\n\n                              the HTTP clients (web browsers, for example) or at least the flow they ex-\n                              pect (making the browser repeat the request).\n\n                           All of these problems are solved (or at least supported) with oAuth flows. For\n\n                        these reasons, there are very few reasons to use username and passwords. If\n\n\n\n\n        26","initials":"ew"},"\"Does the distinction between oauth 1 and 2 need to be highlighted here?\"":{"page":27,"text":"                                                                                         Authentication\n\n\nyou do need simple and quick access to the GitHub API (and you don’t use two\nfactor authentication) then HTTP basic authentication can help you in a small\n\nsubset of use cases.\n\n\noAuth\n\n\noAuth is an authentication mechanism where tokens are tied to functionality or\nclients. In other words, you can specify what features of a service you want to\n\npermit an oAuth token to carry with it, and you can issue multiple tokens and\ntie those to specific clients: a cell phone app, a laptop, a smart watch, or even\n\nan Internet of Things toaster. And, importantly, you can revoke tokens without\nimpacting other tokens.\n   The main downside to oAuth tokens is that they introduce a level of com-\n\nplexity that you may not be familiar with if you have only used HTTP Basic\nwhich generally only requires an extra header to the HTTP request, or an extra\n\nflag to a client tool like cURL.\n   oAuth solves the problems described above by linking tokens to scopes\n\n(specified subsets of functionality inside a webs service) and issuing as many\ntokens as you need to multiple clients.\n\n\nSCOPES: SPECIFIED ACTIONS TIED TO AUTHENTICATION TOKENS\n\n\nWhen you generate an oAuth token, you specify the access rights you require.\nDon’t be confused because we start with HTTP Basic to generate the oAuth to-\n\nken: once you have the token, you no longer need to use HTTP Basic in succes-\nsive requests. If this token is properly issued, the oAuth token will have permis-\nsions to read and write to public repositories owned by that user.\n\n\n   $ curl -u username -d '{\"scopes\":[\"public_repo\"]}' https://api.github.com/authorizations\n   {\n\n     \"id\": 1234567,\n     \"url\": \"https://api.github.com/authorizations/1234567\",\n     \"app\": {\n       \"name\": \"My app\",\n\n       \"url\": \"https://developer.github.com/v3/oauth_authorizations/\",\n       \"client_id\": \"00000000000000000000\"\n     },\n     \"token\": \"abcdef87654321\n     ...\n\n   }\n\n   The JSON response, upon success, has a token you can extract and use for\n\napplications that need access to the GitHub API.\n\n\n\n\n\n                                                                                           27","initials":"ew"},"\"Kind of confusing to go to all the trouble of introducing oauth but to then skim over the details about using it\"":{"page":28,"text":"CHAPTER 2: Introduction\n\n\n\n                            If you are using two factor authentication, this flow requires additional\n                        steps, all of which are documented within the ???.\n\n                            To use this token, you specify the token inside an authorization header. It is a\n                        little bit early to talk about exactly how to interact with the API, but the syntax\n\n                        in cURL looks like the following. For a full flow, check out the Hubot chapter\n                        which shows how to use cURL with an oAuth token.\n\n\n                            $ curl -H \"Authorization: token abcdef87654321\" ...\n\n\n                            Scopes clarify how a service or application will use data inside the GitHub\n                        API. This makes it easy to audit how you are using the information if this was a\n\n                        token issued for your own personal use. But, most importantly, this provides\n                        valuable clarity and protection for those times when a third party application\n\n                        wants to access your information: you can be assured the application is limited\n                        in what data it can access, and you can revoke access easily.\n\n\n                        SCOPE LIMITATIONS\n\n\n                        There is one major limitation of scopes to be aware of: you cannot do fine-\n\n                        grained access to certain repositories only. If you provide access to any of your\n                        private repositories, you are providing access to all repositories.\n\n                            It is likely that GitHub will change the way scopes work and address some of\n                        these issues. The great thing about the way oAuth works is that to support\n\n                        these changes you will simply need to request a new token with the scope\n                        modified, but otherwise, the application authentication flow will be unchaged.\n\n\n                              Be very careful about the scopes you request when building a service or\n\n                              application. Users are (rightly) paranoid about the data they are handing\n                              over to you, and will evaluate your application based on the scopes re-\n                              quested. If they don’t think you need that scope, be sure to remove it\n                              from the list you provide to GitHub when authorizing and consider esca-\n\n                              lation to a higher scope after you have developed some trust with your\n                              users.\n\n\n\n                        SCOPE ESCALATION\n\n\n                        You can ask for scope at one point which is very limited, and then later ask for a\n                        greater scope. For example, when a user first accesses your application, you\n\n                        could only get the user scope to create a user object inside your service, and\n                        only when your application needs repository information for a user, then re-\n\n                        quest to escalate privileges. At this point the user will need to approve or disap-\n                        prove your request, but asking for everything up front (before you have a rela-\n\n                        tionship with the user) often results in a user abandoning the login.\n\n\n\n\n        28","initials":"ew"},"\"Good sensible tip\"":{"page":28,"text":"CHAPTER 2: Introduction\n\n\n\n                            If you are using two factor authentication, this flow requires additional\n                        steps, all of which are documented within the ???.\n\n                            To use this token, you specify the token inside an authorization header. It is a\n                        little bit early to talk about exactly how to interact with the API, but the syntax\n\n                        in cURL looks like the following. For a full flow, check out the Hubot chapter\n                        which shows how to use cURL with an oAuth token.\n\n\n                            $ curl -H \"Authorization: token abcdef87654321\" ...\n\n\n                            Scopes clarify how a service or application will use data inside the GitHub\n                        API. This makes it easy to audit how you are using the information if this was a\n\n                        token issued for your own personal use. But, most importantly, this provides\n                        valuable clarity and protection for those times when a third party application\n\n                        wants to access your information: you can be assured the application is limited\n                        in what data it can access, and you can revoke access easily.\n\n\n                        SCOPE LIMITATIONS\n\n\n                        There is one major limitation of scopes to be aware of: you cannot do fine-\n\n                        grained access to certain repositories only. If you provide access to any of your\n                        private repositories, you are providing access to all repositories.\n\n                            It is likely that GitHub will change the way scopes work and address some of\n                        these issues. The great thing about the way oAuth works is that to support\n\n                        these changes you will simply need to request a new token with the scope\n                        modified, but otherwise, the application authentication flow will be unchaged.\n\n\n                              Be very careful about the scopes you request when building a service or\n\n                              application. Users are (rightly) paranoid about the data they are handing\n                              over to you, and will evaluate your application based on the scopes re-\n                              quested. If they don’t think you need that scope, be sure to remove it\n                              from the list you provide to GitHub when authorizing and consider esca-\n\n                              lation to a higher scope after you have developed some trust with your\n                              users.\n\n\n\n                        SCOPE ESCALATION\n\n\n                        You can ask for scope at one point which is very limited, and then later ask for a\n                        greater scope. For example, when a user first accesses your application, you\n\n                        could only get the user scope to create a user object inside your service, and\n                        only when your application needs repository information for a user, then re-\n\n                        quest to escalate privileges. At this point the user will need to approve or disap-\n                        prove your request, but asking for everything up front (before you have a rela-\n\n                        tionship with the user) often results in a user abandoning the login.\n\n\n\n\n        28","initials":"ew"},"\"The flow of oauth is pretty fundamental to know and should probably be covered up front\"":{"page":29,"text":"                                                                                         Status Codes\n\n\nSIMPLIFIED OAUTH FLOW\n\n\noAuth has many variants, but GitHub uses oAuth2. oAuth2 specifies a flow\nwhere:\n\n\n    • the application requests access\n    • the service provider (GitHub) requests authentication: username and\n\n      password usually.\n    • if two-factor authentication is enabled, ask for the OTP (one time pass-\n\n      word) code.\n\n    • GitHub responds with a token inside a JSON payload\n    • the application uses the oAuth token to make requests of the API.\n\n   A real world flow is described in full in the ???.\n\n   Now let’s review an important fundamental when using web services, the\nimportance of the HTTP status code.\n\n\n\nStatus Codes\n\n\nThe GitHub API uses HTTP status codes to tell you definitive information about\nhow your request was processed. If you are using a basic client like cURL, it will\n\nbe important to validate the status code before you look at any of the data re-\ntrieved. If you are writing your own API client, pay close attention to the status\ncode before anything else.\n\n\n\nSuccess (200 or 201)\n\nIf you have worked with any HTTP clients whatsoever, you know what the HTTP\n\nstatus code “200” means success. GitHub will respond with a 200 status code\nwhen your request destination URL and associated parameters are correct. If\n\nyour request creates content on the server, then you will get a 201 status code,\nindicating a successful creation on the server.\n\n\n   $ curl -s -i https://api.github.com | grep Status\n   Status: 200 OK\n\n\nNaughty JSON (400)\n\n\nIf your payload (the JSON you send to a request) is invalid, the GitHub API will\nrespond with a 400 error, as shown below.\n\n\n\n\n\n\n                                                                                          29","initials":"ew"},"\"Covering basic status codes seems really trivial\"":{"page":29,"text":"                                                                                         Status Codes\n\n\nSIMPLIFIED OAUTH FLOW\n\n\noAuth has many variants, but GitHub uses oAuth2. oAuth2 specifies a flow\nwhere:\n\n\n    • the application requests access\n    • the service provider (GitHub) requests authentication: username and\n\n      password usually.\n    • if two-factor authentication is enabled, ask for the OTP (one time pass-\n\n      word) code.\n\n    • GitHub responds with a token inside a JSON payload\n    • the application uses the oAuth token to make requests of the API.\n\n   A real world flow is described in full in the ???.\n\n   Now let’s review an important fundamental when using web services, the\nimportance of the HTTP status code.\n\n\n\nStatus Codes\n\n\nThe GitHub API uses HTTP status codes to tell you definitive information about\nhow your request was processed. If you are using a basic client like cURL, it will\n\nbe important to validate the status code before you look at any of the data re-\ntrieved. If you are writing your own API client, pay close attention to the status\ncode before anything else.\n\n\n\nSuccess (200 or 201)\n\nIf you have worked with any HTTP clients whatsoever, you know what the HTTP\n\nstatus code “200” means success. GitHub will respond with a 200 status code\nwhen your request destination URL and associated parameters are correct. If\n\nyour request creates content on the server, then you will get a 201 status code,\nindicating a successful creation on the server.\n\n\n   $ curl -s -i https://api.github.com | grep Status\n   Status: 200 OK\n\n\nNaughty JSON (400)\n\n\nIf your payload (the JSON you send to a request) is invalid, the GitHub API will\nrespond with a 400 error, as shown below.\n\n\n\n\n\n\n                                                                                          29","initials":"ew"},"\"it would be more interesting if we had a worked example so that the reader is actively working toward a goal. Talking about these codes individually and abstractly makes the topic very dry.\"":{"page":32,"text":"CHAPTER 2: Introduction\n\n\n                       that the data has not changed since the last time the same request was made.\n                       This is valuable information if you are concerned about your usage limits (and\n                       in most cases you will be). You need to trigger 304s manually by adding condi-\n\n                       tional headers to your request.\n\n\n\n                       Conditional Requests to Avoid Rate Limitations\n\n                       If you are querying the GitHub APIs to obtain activity data for a user or a reposi-\n\n                       tory, there’s a good chance that many of your requests won’t return much activ-\n                       ity. If you check for new activity once every few minutes, there will be time peri-\n\n                       ods over which no activity has occurred. These requests, these constant polls\n                       still use up requests in your rate limit even though there’s no new activity to be\n\n                       delivered.\n                          In these cases, you can send conditional HTTP headers If-Modified-Since and\n                       If-None-Match to tell GitHub to return an HTTP 304 response code telling you\n\n                       that nothing has been modified. When you send a request with a conditional\n                       header and the GitHub API responds with a HTTP 304 response code, this re-\n\n                       quest is not deducted from your rate limit.\n                          The following command listing is an example of passing in the If-Modified-\n\n                       Since HTTP header to the GitHub API. Here we’ve specified that we’re only in-\n                       terested in receiving content if the Twitter Boostrap repositories has been al-\n                       tered after 7:49 PM GMT on Sunday, August 11, 2013. The GitHub API responds\n\n                       with a HTTP 304 response code which also tells us that the last time this reposi-\n                       tory changed was a minute earlier than our cutoff date.\n\n\n                          $ curl -i https://api.github.com/repos/twbs/bootstrap \\\n                                     -H \"If-Modified-Since: Sun, 11 Aug 2013 19:48:59 GMT\"\n                          HTTP/1.1 304 Not Modified\n\n                          Server: GitHub.com\n                          Date: Sun, 11 Aug 2013 20:11:26 GMT\n                          Status: 304 Not Modified\n                          X-RateLimit-Limit: 60\n                          X-RateLimit-Remaining: 46\n\n                          X-RateLimit-Reset: 1376255215\n                          Cache-Control: public, max-age=60, s-maxage=60\n                          Last-Modified: Sun, 11 Aug 2013 19:48:39 GMT\n\n                          The GitHub API also understands HTTP caching tags. An ETag, or Entity Tag,\n\n                       is an HTTP header that is used to control whether or not content that you have\n                       previously cached is the most recent version. Here’s how your systems would\n\n                       use ETag:\n\n                           • Your server requests information from an HTTP server.\n\n\n\n\n\n        32","initials":"ew"},"\"Go straight to demoing squid?\"":{"page":33,"text":"                                                              Conditional Requests to Avoid Rate Limitations\n\n\n    • Server returns an ETag header for a version of a content item.\n\n    • Your server includes this ETag in all subsequent requests.\n         ◦ If the server has a newer version it returns new content + a new\n\n            ETag\n         ◦ If the server doesn’t have a newer version it returns an HTTP 304\n\n\n   The following command listing demonstrates to commands. The first curl\ncall to the GitHub API generates an ETag value, and the second value passes this\n\nETag value as an If-None-Match header. You’ll note that the second response is\nan HTTP 304 which tells the caller that there is no new content available.\n\n\n   $ curl -i https://api.github.com/repos/twbs/bootstrap\n   HTTP/1.1 200 OK\n   Cache-Control: public, max-age=60, s-maxage=60\n   Last-Modified: Sun, 11 Aug 2013 20:25:37 GMT\n\n   ETag: \"462c74009317cf64560b8e395b9d0cdd\"\n\n   {\n     \"id\": 2126244,\n     \"name\": \"bootstrap\",\n\n     \"full_name\": \"twbs/bootstrap\",\n     ....\n   }\n\n\n   $ curl -i https://api.github.com/repos/twbs/bootstrap \\\n              -H 'If-None-Match: \"462c74009317cf64560b8e395b9d0cdd\"'\n\n   HTTP/1.1 304 Not Modified\n   Status: 304 Not Modified\n\n   Cache-Control: public, max-age=60, s-maxage=60\n   Last-Modified: Sun, 11 Aug 2013 20:25:37 GMT\n   ETag: \"462c74009317cf64560b8e395b9d0cdd\"\n\n\n   If you are developing an application that needs to make a significant num-\nber of requests to the GitHub API over a long period of time, you can use a cach-\n\ning HTTP proxy like Squid to take care of automatically caching content, storing\ncontent alongside ETags, and injecting the “If-None-Match” header into GitHub\nAPI requests. If you do this, you’ll be automating the injection of conditional\n\nheaders and helping to reduce the overall load on the GitHub API.\n   Use of conditional request headers is encouraged to conserve resources and\n\nmake sure that the infrastructure that supports GitHub’s API isn’t asked to gen-\nerated content unnecessarily.\n\n   You might now be wondering: what are my rate limits and when should I\ncare about them?\n\n\n\n\n\n\n                                                                                          33","initials":"ew"},"\"This is an interesting bit of information\"":{"page":34,"text":"CHAPTER 2: Introduction\n\n\n                        GitHub API Rate Limits\n\n\n                        GitHub tries to limit the rate at which users can make requests to the API.\n\n                        Anonymous requests, requests that haven’t authenticated with either a user-\n                        name/password or OAuth information, are limited to 60 requests an hour. If you\n\n                        are developing a system to integrate with the GitHub API on behalf of users,\n                        clearly 60 requests per hour isn’t going to be sufficient.\n\n                           This rate limit is increased to 5000 requests per hour if you are making an\n                        authenticated request to the GitHub API, and while this rate is two orders of\n\n                        magnitude larger than the anonymous rate limit, it still presents problems if\n                        you intend to use your own GitHub credentials when making requests on behalf\n\n                        of many users.\n                           For this reason, if your web site or service uses the GitHub API to request in-\n\n                        formation from the GitHub API, you should consider using OAuth and make re-\n                        quests to the GitHub API using your user’s shared authentication information.\n\n\n                             There are actually two rate limits. The “core” rate limit and the “search”\n                             rate limit. The rate limits explained in the previous paragraphs were for\n\n                             the core rate limit. For search, requests are limited at 20 requests per mi-\n                             nute for authenticated user requests and 5 request per minute for anony-\n                             mous requests. The assumption here is that search is a more infrastruc-\n\n                             ture intensive request to satisfy and that tighter limits are placed on its\n                             usage.\n\n\n                           Note that GitHub tracks anonymous requests by IP address. This means that\n\n                        if you are behind a firewall with other users making anonymous requests, all\n                        those requests will be grouped together.\n\n\n\n                        Reading Your Rate Limits\n\n                        Reading your rate limit is straightforward, just make a GET request to /\n\n                        rate_limit. This will return a JSON document which tells you the limit you are\n                        subject to, the number of requests you have remaining, and the timestamp (in\n\n                        seconds since 1970). Note that this timestamp has a timezone in Coordinated\n                        Universal Time (UTC).\n\n                           The following command listing uses curl to retrieve the rate limit for an\n                        anonymous request. This response is abbreviated to save space in this book,\n\n                        but you’ll notice that the quota information is supplied twice: once in the HTTP\n                        response headers and again in the JSON response. The rate limit headers are\n\n                        returned with every request to the GitHub API, so there is little need to make a\n                        direct call to the /rate_limit API.\n\n\n\n\n\n\n\n        34","initials":"ew"},"\"doing something interesting - give an example, show the reader something interesting\"":{"page":35,"text":"                                                                         Accessing Content from the Web\n\n\n   $ curl https://api.github.com/rate_limit\n   {\n     \"resources\": {\n       \"core\": {\n\n          \"limit\": 60,\n          \"remaining\": 48,\n          \"reset\": 1433398160\n       },\n       \"search\": {\n\n          \"limit\": 10,\n          \"remaining\": 10,\n          \"reset\": 1433395543\n       }\n\n     },\n     \"rate\": {\n       \"limit\": 60,\n       \"remaining\": 48,\n       \"reset\": 1433398160\n\n     }\n   }\n\n   60 requests over the course of an hour isn’t very much, and if you plan on\n\ndoing anything interesting, you will likely exceed this limit quickly. If you are hit-\nting up against the 60 requests per minute limit, you will likely want to investi-\n\ngate making authenticated requests to the GitHub API. We’ll show that when\nwe discuss authenticated requests.\n\n\n     Calls to the Rate Limit API are not deducted from your Rate Limit. Isn’t\n     that nice of them?\n\n\n\n   At this point we have been accessing the GitHub API from a cURL client, and\nas long as our network permits it, we can do whatever we want. The GitHub API\nis accessible in other situations as well, like from within a browser context, and\n\ncertain restrictions apply there, so let’s discuss that next.\n\n\n\nAccessing Content from the Web\n\n\nIf you are using the GitHub API from a server side program or the command line\nthen you are free to issue any network calls as long as your network permits it.\n\nIf you are attempting to access the GitHub API from within a browser using\nJavaScript and the XHR (XmlHttpRequest) object, then you should be aware of\nlimitations imposed by the browser’s same-origin policy. In a nutshell, you are\n\nnot able to access domains from JavaScript using standard XHR requests out-\nside of the domain from which you retrieved the original page. There are two\n\n\n\n\n\n                                                                                          35","initials":"ew"},"\"All of this cross browser discussion seems really redundant given that we have not at all discussed designing a website. What has this got to do with hacking GitHub? do you plan to create a website with the reader?\\n\\nInformation needs context and you nee\\nd to consider what is needed/not needed.\"":{"page":38,"text":"CHAPTER 2: Introduction\n\n\n                          X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset,\n                          X-OAuth-Scopes, X-Accepted-OAuth-Scopes, X-Poll-Interval\n                          Access-Control-Allow-Origin: *\n                          X-GitHub-Request-Id: C0F1CF9E:07AD:3C493B:557107C7\n\n                          Strict-Transport-Security: max-age=31536000; includeSubdomains;\n                          preload\n\n                          We can see the “Access-Control-Allow-Credentials” header is set to true. It\n\n                       depends on the browser implementation, but some JavaScript host browsers\n                       will automatically make a “preflight” request to verify this header is set to true\n\n                       (and that other headers, like the “Access-Control-Allow-Origin” are set correctly\n                       and permit requests from that origin to proceed). Other JavaScript host brows-\n\n                       ers will need you to make that request. Once the browser has used the headers\n                       to confirm that CORS is permitted, you can make XHR requests to the GitHub\n                       API domain as you would any other XHR request going into the same domain.\n\n                          We’ve covered much of the details of connecting and dissecting the GitHub\n                       API, but there are a few other options to know about when using it. One of them\n\n                       is that you can use the GitHub API service to provide rendered content when\n                       you need it.\n\n\n\n                       Specifying Response Content Format\n\n                       When you send a request to the GitHub API, you have some ability to specify the\n\n                       format of the response you expect. For example, if you are requesting content\n                       that contains text from a commit’s comment thread, you can use the Accept\n\n                       header to ask for the raw markdown or for the HTML this markdown generates.\n                       You also have the ability to specify this version of the GitHub API you are using.\n\n                       At this point, you can specify either version 3 or beta of the API.\n\n\n                       RETRIEVING FORMATTED CONTENT\n\n                       The Accept header you send with a request can affect the format of text re-\n\n                       turned by the GitHub API. As an example, let’s assume you wanted to read the\n                       body of a GitHub Issue. An issue’s body is stored in markdown and will be sent\n\n                       back in the request by default. If we wanted to render the response as HTML\n                       instead of markdown, we could do this by sending a different accept header, as\n                       the following cURL commands demonstrate.\n\n\n                          $ URL='https://api.github.com/repos/rails/rails/issues/11819'\n                          $ curl -s $URL | jq '.body'\n\n                          \"Hi, \\r\\n\\r\\nI have a problem with strong....\"\n                          $ curl -s $URL | jq '.body_html'\n                          null\n                          $ curl -s $URL \\\n\n\n\n\n        38","initials":"ew"},"\"This seems like a really trivial point to make?\"":{"page":39,"text":"                                                                    GitHub Has Amazing API Documentation\n\n\n   -H \"Accept: application/vnd.github.html+json\" | jq '.body_html'\n   \"<p>Hi, </p>\\n\\n<p>I have a problem with...\"\n\n\n   Without specifying an extra header, we get the internal representation of the\n   data, sent as markdown.\n\n\n   Note that if we don’t request the HTML representation, we don’t see it in the\n   JSON by default.\n\n\n   If we use a customized accept header like in the third instance, then our\n\n   JSON is populated with a rendered version of the body in HTML.\n\n   Besides “raw” and “html” there are two other format options that influence\nhow Markdown content is delivered via the GitHub API. If you specify “text” as a\n\nformat, the issue body would have been returned as plaintext. If you specify\n“full” then the content will be rendered multiple times including the raw Mark-\ndown, rendered HTML, and rendered plaintext.\n\n   In addition to controlling the format of text content, you can also retrieve\nGitHub blobs either as raw binary or as a BASE64 encoded text. When retrieving\n\ncommits, you can also specify that the content be returned either as a diff or as\na patch. For more information about these fine-grained controls for formatting,\n\nsee the GitHub API documentation.\n\n\nGitHub API Terms of Service\n\n\nBefore you start building a system atop another service’s API, it is always wise\nto understand what, if any, limitations are placed on that API’s usage. Aside\n\nfrom the limitations on bandwidth, GitHub’s API is also covered by the overall\nGitHub Terms of Service. You can read these terms of service here: https://\nhelp.github.com/articles/github-terms-of-service\n\n\n\nGitHub Has Amazing API Documentation\n\n\nThe GitHub team has already provided very thorough documentation on their\nAPI with examples using cURL. Bookmark this URL: https://develop-\n\ner.github.com/v3/. You’ll use it often. Do note that this URL is tied, obviously, to\nthe current API “Version 3”, so this URL will change when a new version is re-\n\nleased.\n\n\n\n\n\n\n\n\n\n                                                                                           39","initials":"ew"},"\"This chapter feels like it bombards the reader with information. This maybe fine if this was simply a GitHub manual or reference book...\\n\\nOverall it is very hard to read and there is no context on how the reader might best use the tools discussed.\"":{"page":40,"text":"CHAPTER 2: Introduction\n\n\n\n                       Summary\n\n\n                       In this chapter we learned how to access the GitHub API from the simplest cli-\n                       ent available: the command line cURL HTTP tool. We also explored the API by\n                       looking at the JSON and played with a command line tool (jq) that when paired\n\n                       with cURL gives us the ability to quickly find information in the often large body\n                       of data the GitHub API provides. We learned about the different authentication\n\n                       schemes supported by GitHub, and also learned about the possibilities and\n                       tradeoffs when accessing the GitHub API from within a browser context.\n                          In the next chapter we will look at Gists and the Gist API. We’ll use Ruby to\n\n                       build a Gist display program, and host all source files for the application as a\n                       Gist itself.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       40","initials":"ew"},"\"Feels like there needs to be more of an introduction to what the chapter will be about and how it will be presented.\\n\\nI.e. follow the adage: \\\"Tell them what you are going to tell them, then tell them and finish by telling them what you told them.\\\"\"":{"page":41,"text":"                    Gists and the Gist API                          3\n\n\n\n\n\n\n\n\nGitHub revolutionized software development by responding to a deep desire to\nshare information. But calling it just “sharing” does a disservice to the tools Git-\nHub provides: these tools remove barriers to communication and streamline\nworkflows and these tools also arose at exactly the moment when more and\nmore companies permitted and more and more complementary technologies\nappeared to allow an emerging remote workforce. Gists service part of this\n\nneed: they permit intimate code sharing and reuse, refactoring and play in a\nway not served by heavyweight tools predating it.\n  Gists are easy to create and the interface is stripped down to the barest level.\nYou add a snippet of code and then share the URL. Gists autodetect the lan-\nguage in most cases and format it correctly. Gists can be used in more powerful\nways than might appear at first glance and this chapter will explore other ways\nto share code and amplify your team.\n\n  To create a gist, go to gist.github.com and enter in any textual data. You then\nchoose public or secret access and create the gist. After creating the gist, you\nreceive a shareable URL with the code. If the type of textual data is specified,\nusually the coding language type, then the code will be formatted in a pretty\nway for better readability. If you need to share a small bit of code, or write\nsomething and discuss it, gists are a great tool.\n\n  There are other services that do this: pastebin was the first, and there are\nmany others that offer variances on code sharing. But gists by GitHub are not\nsimply a pasting service. Gists are first class repositories, forkable, editable and\nexpansive. We’ll go over the basics of what gists are, and how to create them,\nand then show how they allow you to share code that is also a live application.\n\n\nGists are repositories\n\n\nEvery gist created is a tiny repository. You can update gists and see the history\nusinggit log. You can download gists, hack on the reposigit pushd\nthem back into the repository on gist.github.com (which will republish them\n\n\n\n                                                                           41","initials":"ew"},"\"Really feels like this should be a worked example with screenshots and links\"":{"page":41,"text":"                    Gists and the Gist API                          3\n\n\n\n\n\n\n\n\nGitHub revolutionized software development by responding to a deep desire to\nshare information. But calling it just “sharing” does a disservice to the tools Git-\nHub provides: these tools remove barriers to communication and streamline\nworkflows and these tools also arose at exactly the moment when more and\nmore companies permitted and more and more complementary technologies\nappeared to allow an emerging remote workforce. Gists service part of this\n\nneed: they permit intimate code sharing and reuse, refactoring and play in a\nway not served by heavyweight tools predating it.\n  Gists are easy to create and the interface is stripped down to the barest level.\nYou add a snippet of code and then share the URL. Gists autodetect the lan-\nguage in most cases and format it correctly. Gists can be used in more powerful\nways than might appear at first glance and this chapter will explore other ways\nto share code and amplify your team.\n\n  To create a gist, go to gist.github.com and enter in any textual data. You then\nchoose public or secret access and create the gist. After creating the gist, you\nreceive a shareable URL with the code. If the type of textual data is specified,\nusually the coding language type, then the code will be formatted in a pretty\nway for better readability. If you need to share a small bit of code, or write\nsomething and discuss it, gists are a great tool.\n\n  There are other services that do this: pastebin was the first, and there are\nmany others that offer variances on code sharing. But gists by GitHub are not\nsimply a pasting service. Gists are first class repositories, forkable, editable and\nexpansive. We’ll go over the basics of what gists are, and how to create them,\nand then show how they allow you to share code that is also a live application.\n\n\nGists are repositories\n\n\nEvery gist created is a tiny repository. You can update gists and see the history\nusinggit log. You can download gists, hack on the reposigit pushd\nthem back into the repository on gist.github.com (which will republish them\n\n\n\n                                                                           41","initials":"ew"},"\"\\\"important to understand\\\" not \\\"important the understand\\\"\"":{"page":42,"text":"CHAPTER 3: Gists and the Gist API\n\n\n                        onto the publicly facing web page). And, you can “fork” gists, just like any other\n                        repository.\n\n                           You are allowed to branch within gist repositories; however, branches are\n                        not displayed inside of gist.github.com. But, if you need the benefits of branch-\n                        ing when using GitHub gists you can branch normally inside a repository and\n\n                        keep the branch information on the upstream repository after you push it up.\n                           You can have an unlimited number of public and secret gists. Instead of cre-\n\n                        ating a new private repository from your limited amount in a paid GitHub ac-\n                        count, you can take a tiny bit of code and make a secret gist, sharing this with\n\n                        others through a URL instead of the more onerous process of adding collabora-\n                        tors to a regular repository. Or, you can make a gist public, and share that URL\n\n                        to mailing lists or anywhere you need public feedback.\n\n\n                             As there are two types of gists (public and secret), it is important the un-\n                             derstand the differences between them. Public gists are searchable. Se-\n                             cret gists are not searchable, but they are accessible to anyone who\n                             knows the URL. Don’t post any code to gist which you need to keep secret\n                             as once you put it there, it is only as safe as the URL is secret.\n\n\n\n                           Most people share gists through the URL. But, you can embed gists inside of\n                        other contexts (like blogs) and get a simple and pretty snippet of code.\n\n\n                        Embedding Gists Inside HTML\n\n\n                        To embed inside of an HTML page look for the “Embed this gist” box to the left\n\n                        of a gist. Copy the code listed there (which will look something like <script\n                        src=\"https://gist.github.com/xrd/8923697.js\"></script>                     and\n\n                        paste it into your HTML.\n                           If you wish to include only a particular file from the Gist (if it contains multi-\n\n                        ple files), then ad?file=hi.rb    to the end of the URL specified in the src at-\n                        tribute.\n\n\n\n                        Embedding Inside Jekyll blogs\n\n                        Though we have not yet explained how Jekyll works (the GitHub blogging tool),\n\n                        it seems valid to point out the ease in which you can publish gists into a blog if\n                        that blog happens to be Jekyll hosted on GitHub.\n\n                           Jekyll supports a fast shortcut code to embed a public gist inside of your Je-\n                        kyll blog hosted on GitHub, or on any site built on the “github-pages” branch\n\n                        mechanism (described in the Chapter 7 chapter). The shortcut         {% gist\n\n\n\n\n\n\n        42","initials":"ew"},"\"Not sure this makes sense, does this work?\"":{"page":47,"text":"                                                                                 Gists that render Gists\n\n\n     user = Octokit.user username\n     count = user.public_gists\n     erb :index, locals: { :count => count }\n   end\n\n\n   Our filesystem should look like this, with three files.\n\n   $ ls -1\n\n   Gemfile\n   hi.rb\n   index.erb\n\n\n   Run bundle  to install octokit and restart Sinatra by running ctrl-c, and then\nruby hi.rb   . If you vishttp://localhost:4567/xrd      in your browser, you\n\nwill see the count of public gists for uxrd ; modify the username in the URL\nto any specify any GitHub username and you will see their last five gists dis-\n\nplayed.\n\n\n\n                                                                                FIGURE 3-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoing deeper into the Gist API\n\nThe GitHub API uses hypermedia instead of basic resource driven APIs. If you\n\nuse a client like Octokit, the hypermedia details are hidden behind an elegant\nruby client. But, there is a benefit to understanding how hypermedia works\n\nwhen you need to retrieve deeper information from the GitHub API.\n   Most RESTful APIs come with a “sitemap”, generally a API reference docu-\nment which tells a user which endpoints to use. You view the resources avail-\n\nable from that API and then apply some HTTP verb to do something to them.\nHypermedia thinks of an API differently. Hypermedia APIs describe themselves\n\ninside their responses using “affordances.” What this means is that the API\nmight respond like this:\n\n\n\n\n\n\n                                                                                         47","initials":"ew"},"\"Good to show a worked example\"":{"page":47,"text":"                                                                                 Gists that render Gists\n\n\n     user = Octokit.user username\n     count = user.public_gists\n     erb :index, locals: { :count => count }\n   end\n\n\n   Our filesystem should look like this, with three files.\n\n   $ ls -1\n\n   Gemfile\n   hi.rb\n   index.erb\n\n\n   Run bundle  to install octokit and restart Sinatra by running ctrl-c, and then\nruby hi.rb   . If you vishttp://localhost:4567/xrd      in your browser, you\n\nwill see the count of public gists for uxrd ; modify the username in the URL\nto any specify any GitHub username and you will see their last five gists dis-\n\nplayed.\n\n\n\n                                                                                FIGURE 3-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoing deeper into the Gist API\n\nThe GitHub API uses hypermedia instead of basic resource driven APIs. If you\n\nuse a client like Octokit, the hypermedia details are hidden behind an elegant\nruby client. But, there is a benefit to understanding how hypermedia works\n\nwhen you need to retrieve deeper information from the GitHub API.\n   Most RESTful APIs come with a “sitemap”, generally a API reference docu-\nment which tells a user which endpoints to use. You view the resources avail-\n\nable from that API and then apply some HTTP verb to do something to them.\nHypermedia thinks of an API differently. Hypermedia APIs describe themselves\n\ninside their responses using “affordances.” What this means is that the API\nmight respond like this:\n\n\n\n\n\n\n                                                                                         47","initials":"ew"},"\"These bullet points are a little hard to follow, might be better if they were paired with code and more elaboration\"":{"page":49,"text":"                                                                                Gists that render Gists\n\n\n      links. In this case, cal.rels  shows a map of relationships, displayed\n\n      in   ruby    code    like:  #<Sawyer::Relation::Map:          [:ava-\n      tar, :self, :html, :followers, :following, :gists, :star-\n\n      red, :subscriptions, :organizations, :repos, :events, :re-\n\n      ceived_events]>\n    • Using one of these relationships starts by keying into the relationship\n\n      hash and then using the get and data methods to request that informa-\n      tion from the GitHub API:     followers    =   user.rels[:follow-\n\n      ers].get.data  .\n    • Once you call.get.data  you will have a new followers object populated\n\n      with an array of the followers (paged if it exceeds 100 items).\n\n   Let’s extend our Sinatra app to retrieve actual data about the user’s gists by\nusing hypermedia references.\n\n\n   require 'sinatra'\n   require 'octokit'\n\n\n   set :views, \".\"\n\n   helpers do\n     def h(text)\n       Rack::Utils.escape_html(text)\n\n     end\n   end\n\n   get '/:username' do |username|\n     gists = Octokit.gists username, :per_page => 5\n\n     erb :index, locals: { :gists => gists, username: username }\n   end\n\n   The index.erb  file contains code to iterate over each gist and pull the con-\n\ntent. You can see that our response object is an array of gists, each which has\nan attribute calledfields . This fields attribute specifies the filenames avail-\n\nable in each gist. If you reference that filename against the files, the response\nincludes a hypermedia ref  attribute. You can use this retreive raw content\n\nusing the Octokit method.get.data  .\n\n   <html>\n\n   <body>\n\n   <h2>User <%= username %>'s last five gists</h2>\n\n   <% gists.each do |g| %>\n   <% g[:files].fields.each do |f| %>\n\n\n\n\n                                                                                        49","initials":"ew"},"\"Looks like this functionality was referenced earlier by mistake\"":{"page":49,"text":"                                                                                Gists that render Gists\n\n\n      links. In this case, cal.rels  shows a map of relationships, displayed\n\n      in   ruby    code    like:  #<Sawyer::Relation::Map:          [:ava-\n      tar, :self, :html, :followers, :following, :gists, :star-\n\n      red, :subscriptions, :organizations, :repos, :events, :re-\n\n      ceived_events]>\n    • Using one of these relationships starts by keying into the relationship\n\n      hash and then using the get and data methods to request that informa-\n      tion from the GitHub API:     followers    =   user.rels[:follow-\n\n      ers].get.data  .\n    • Once you call.get.data  you will have a new followers object populated\n\n      with an array of the followers (paged if it exceeds 100 items).\n\n   Let’s extend our Sinatra app to retrieve actual data about the user’s gists by\nusing hypermedia references.\n\n\n   require 'sinatra'\n   require 'octokit'\n\n\n   set :views, \".\"\n\n   helpers do\n     def h(text)\n       Rack::Utils.escape_html(text)\n\n     end\n   end\n\n   get '/:username' do |username|\n     gists = Octokit.gists username, :per_page => 5\n\n     erb :index, locals: { :gists => gists, username: username }\n   end\n\n   The index.erb  file contains code to iterate over each gist and pull the con-\n\ntent. You can see that our response object is an array of gists, each which has\nan attribute calledfields . This fields attribute specifies the filenames avail-\n\nable in each gist. If you reference that filename against the files, the response\nincludes a hypermedia ref  attribute. You can use this retreive raw content\n\nusing the Octokit method.get.data  .\n\n   <html>\n\n   <body>\n\n   <h2>User <%= username %>'s last five gists</h2>\n\n   <% gists.each do |g| %>\n   <% g[:files].fields.each do |f| %>\n\n\n\n\n                                                                                        49","initials":"ew"},"\"This chapter feels very thin and unpolished. An example is good but it lacks real depth.\"":{"page":50,"text":"CHAPTER 3: Gists and the Gist API\n\n\n                         <b><%= f %></b>:\n\n                         <%= h g[:files][f.to_sym].rels[:raw].get.data %>\n\n                         <br/>\n\n                         <br/>\n\n                         <% end %>\n                         <% end %>\n\n\n                         </body>\n                         </html>\n\n                         Now we see the gists and the contents.\n\n\n\n\n FIGURE 3-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                      Summary\n\n\n                      In this chapter we looked at gists and learned how they can be used to share\n\n                      code snippets. We built a simple application and stored it as a gist. This appli-\n                      cation retrieves data from the GitHub API using our first higher level language\n\n                      client library (the Octokit library for Ruby). We also went deeper into how Hy-\n                      permedia works and how a client library implements using Hypermedia meta-\n                      data.\n\n                         In the next chapter we will look at Gollum, the GitHub wiki. This chapter pro-\n                      vides an introduction to the Rugged Ruby library for accessing Git repositories\n\n                      and the Ruby library for accessing GitHub.\n\n\n\n\n\n\n\n\n\n\n       50","initials":"ew"},"\"Good overview but the introduction feels a little long.\"":{"page":51,"text":"                                                Gollum             4\n\n\n\n\n\n\n\n\nWikis have revolutionized the way we create and digest information. It turns out\nthey are a great complement to technical projects (code repositories) because\nthey allow non-technical users to contribute information without disturbing de-\n\nvelopers. Gollum is GitHub’s open source version of a wiki. Just as Git has revo-\nlutionized collaborative editing of code, Gollum wikis layer the benefits of Git\nonto a proven publishing workflow. The true power of Gollum wikis reveal\nthemselves when you see how tightly integrated with GitHub they are. You can\nquickly build and associate a wiki with any repository, and create a collabora-\ntive documentation system around any repository hosted on GitHub. And, you\ncan pull in information from git repositories with ease, linking documentation\n\nwith live code.\n  In this chapter we’ll explore the basics of using Gollum, creating a wiki on\nGitHub and then understanding how to edit it on GitHub, and as a repostory on\nour local machine. We will then create a Gollum wiki by hand from the com-\nmand line, and show the bare minimum set of files to call something a Gollum\nrepository. Finally, we will build a simple image organization tool which allows\nus to edit a Gollum wiki in an entirely different way, but still publishes informa-\n\ntion into GitHub as a regular Gollum wiki, exploring a little bit of the internals of\nGit along the way.\n\n\n“The Story of Smeagol…”\n\nGollum wikis are simply an agreed upon file structure. At its most basic form, a\nGollum wiki is a git repository with aHome.ext(extwould be any of\nthe supported wiki markup formats, which we will talk about later).\n\n  First, let’s learn how to create a gollum wiki from the GitHub interface, and\nthen later we’ll move into creating one from scratch as a git repository.\n\n\n\n\n\n\n                                                                          51","initials":"ew"},"\"This is good, example driven explanation with screenshots.\"":{"page":53,"text":"                                                                                  “The Story of Smeagol…”\n\n\n\n                                                                                    FIGURE 4-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Your wiki is now as public as your repository is public. Public repositories\n\nhave public wikis, accessible to anyone. Private repositories have private wikis,\naccessible only to those users or organizations which have rights to edit the\nrepository data.\n\n   Wikis are powerful collaboration tools because they use a special language\nwhich compiles into HTML. Let’s review the options for Gollum wikis.\n\n\n\nMarkup and Structure\n\nGollum files can be written in any of the supported “Github Markup” formats,\n\nwhich includes ASCIIdoc, Creole, Markdown, Org Mode, Pod, RDoc, ReStructur-\nedText, Textile, and MediaWiki. The variety of markup languages brings flexibili-\n\nty but it can be confusing to know which one to use. Markdown (and its variant\ncousins) is the most popular markup language on GitHub, and is well liked on\n\nother popular sites like Stack Overflow. If you are unsure which language to\nuse, Markdown is a safe bet because it is ubiquitous across GitHub. The Chap-\n\nter 7 chapter has a much deeper overview of Markdown.\n   If you do choose Markdown, In addition to the standard vanilla Markdown\nlanguage tags, Gollum adds its own set wiki specific tags. There are often subtle\n\n\n\n\n\n                                                                                              53","initials":"ew"},"\"Good concise explanation on why to use Markdown\"":{"page":53,"text":"                                                                                  “The Story of Smeagol…”\n\n\n\n                                                                                    FIGURE 4-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Your wiki is now as public as your repository is public. Public repositories\n\nhave public wikis, accessible to anyone. Private repositories have private wikis,\naccessible only to those users or organizations which have rights to edit the\nrepository data.\n\n   Wikis are powerful collaboration tools because they use a special language\nwhich compiles into HTML. Let’s review the options for Gollum wikis.\n\n\n\nMarkup and Structure\n\nGollum files can be written in any of the supported “Github Markup” formats,\n\nwhich includes ASCIIdoc, Creole, Markdown, Org Mode, Pod, RDoc, ReStructur-\nedText, Textile, and MediaWiki. The variety of markup languages brings flexibili-\n\nty but it can be confusing to know which one to use. Markdown (and its variant\ncousins) is the most popular markup language on GitHub, and is well liked on\n\nother popular sites like Stack Overflow. If you are unsure which language to\nuse, Markdown is a safe bet because it is ubiquitous across GitHub. The Chap-\n\nter 7 chapter has a much deeper overview of Markdown.\n   If you do choose Markdown, In addition to the standard vanilla Markdown\nlanguage tags, Gollum adds its own set wiki specific tags. There are often subtle\n\n\n\n\n\n                                                                                              53","initials":"ew"},"\"Bit of overflow here.\"":{"page":55,"text":"                                                                               “The Story of Smeagol…”\n\n\n   ```ruby\n   def hello\n     puts \"hello\"\n   end\n\n   ```\n\n   A more interesting way of embedding code inside a Gollum repository is to\n\nuse the file include syntax. Again, use a triple backtick, followed by the file type\nand then a reference to the code snippet inside a GitHub repository. You’ll need\nto include the branch as well.\n\n\n   ```html:github:xrd/TeddyHyde/blob/master/Gemfile```\n\n\n   This will pull the file on GitHub located inside the “TeddyHyde” repository\nfor user “xrd” on the master branch named “Gemfile” and publish it with syntax\n\nhilighted into your wiki as if you had used this markup.\n\n   ```ruby\n\n   source 'https://rubygems.org'\n\n   gem \"nokogiri\"\n   gem \"rspec\"\n   gem 'calabash-android', :git => 'git://github.com/calabash/calabash-android.git'\n\n   ```\n\n   Unfortunately, you cannot specify a specific SHA commit to retrieve mo-\n\nments in history within the git repository, but this is still a powerful way to ex-\npose a file inside a Gollum wiki. If you need to do that, the best way might be to\n\ncreate a branch from a specific SHA commit, and then reference that branch\nwhen including the file.\n\n\n   $ git checkout 0be3e4475db2697b8\n   $ git checkout -b at_sha_0be3e4475db2697b8\n   $ echo \"gem 'rails' # Adding rails to Gemfile\" >> Gemfile\n   $ git commit -m \"Added rails to Gemfile\" -a\n\n   $ git push origin at_sha_0be3e4475db2697b8\n\n   This would generate a new branch based on an older commit, and push up\n\nthe branch. Then, you could reference this inside your wiki with the following\ninclude\n\n\n   ```html:github:xrd/TeddyHyde/blob/at_sha_0be3e4475db2697b8/Gemfile```\n\n   Note that we’ve referenced the branch named after the specific SHA hash we\n\nwant.\n\n\n\n\n\n                                                                                          55","initials":"ew"},"\"The branch name seems a bit long here but the method is sound\"":{"page":55,"text":"                                                                               “The Story of Smeagol…”\n\n\n   ```ruby\n   def hello\n     puts \"hello\"\n   end\n\n   ```\n\n   A more interesting way of embedding code inside a Gollum repository is to\n\nuse the file include syntax. Again, use a triple backtick, followed by the file type\nand then a reference to the code snippet inside a GitHub repository. You’ll need\nto include the branch as well.\n\n\n   ```html:github:xrd/TeddyHyde/blob/master/Gemfile```\n\n\n   This will pull the file on GitHub located inside the “TeddyHyde” repository\nfor user “xrd” on the master branch named “Gemfile” and publish it with syntax\n\nhilighted into your wiki as if you had used this markup.\n\n   ```ruby\n\n   source 'https://rubygems.org'\n\n   gem \"nokogiri\"\n   gem \"rspec\"\n   gem 'calabash-android', :git => 'git://github.com/calabash/calabash-android.git'\n\n   ```\n\n   Unfortunately, you cannot specify a specific SHA commit to retrieve mo-\n\nments in history within the git repository, but this is still a powerful way to ex-\npose a file inside a Gollum wiki. If you need to do that, the best way might be to\n\ncreate a branch from a specific SHA commit, and then reference that branch\nwhen including the file.\n\n\n   $ git checkout 0be3e4475db2697b8\n   $ git checkout -b at_sha_0be3e4475db2697b8\n   $ echo \"gem 'rails' # Adding rails to Gemfile\" >> Gemfile\n   $ git commit -m \"Added rails to Gemfile\" -a\n\n   $ git push origin at_sha_0be3e4475db2697b8\n\n   This would generate a new branch based on an older commit, and push up\n\nthe branch. Then, you could reference this inside your wiki with the following\ninclude\n\n\n   ```html:github:xrd/TeddyHyde/blob/at_sha_0be3e4475db2697b8/Gemfile```\n\n   Note that we’ve referenced the branch named after the specific SHA hash we\n\nwant.\n\n\n\n\n\n                                                                                          55","initials":"ew"},"\"Feels like an alternative should be offered her and/or a quick overview of Markdown formatting alternatives?\"":{"page":56,"text":"CHAPTER 4: Gollum\n\n\n                       GOLLUM STRUCTURAL COMPONENTS\n\n\n                       Gollum includes capabilities to add sidebars, headers, and footers. If you in-\n                       clude a file calle_Sidebar.ext    inside your repository, you’ll see it as a side-\n\n                       bar for every file rendered. Sidebars are automatically added to any file and any\n                       file from subdirectories that do not have their own sidebar files. If you wanted\n\n                       to add sidebars specific to a subdirectory, add another file in the subdirectory\n                       and this file will override the top level sidebar file.\n\n\n                       NO STYLING OR JAVASCRIPT\n\n\n                       Finally, for security reasons, Gollum strips out all CSS and JavaScript from raw\n\n                       markup files. You can include your own JavaScript or CSS file when running\n                       Gollum locally using the --custom-css   or --custom-js    switches, but there is\n\n                       no way to include these files on a Wiki when your Gollum wiki is hosted on Git-\n                       Hub.\n\n                          Now that we have investigated the structure and format of using Gollum wi-\n                       kis, we can dig into the power tools that come with Gollum.\n\n\n\n                       Moving Gollum to Your Laptop\n\n\n                       Though many people edit Gollum wikis exclusively from within the GitHub on-\n                       line editor, there is a real flexibility and power when hosting your wiki locally\n\n                       and editing it from your laptop. To do this you need to install the command line\n                       tools for Gollum.\n\n\n                          $ gem install gollum\n\n\n                          You will then see thegollum  command in your path.\n\n\n                            There is a difference between thgollumcommand (what you run from\n                            the command line) and the suite of technologies that make up Gollum as\n\n                            a project. To differentiate between them, remember that we are talking\n                            about the suite when the word is capitalized (“Gollum”), and the com-\n                            mand line tool when the word is lowercased and fixed width fogol-\n                            lum).\n\n\n\n                          What additional options are opened up when running locally? Let’s take a\n                       deeper look.\n\n\n\n\n\n\n\n\n\n        56","initials":"ew"},"\"Unfortunate new line here, in the middle of the word gollum, could confuse things\"":{"page":56,"text":"CHAPTER 4: Gollum\n\n\n                       GOLLUM STRUCTURAL COMPONENTS\n\n\n                       Gollum includes capabilities to add sidebars, headers, and footers. If you in-\n                       clude a file calle_Sidebar.ext    inside your repository, you’ll see it as a side-\n\n                       bar for every file rendered. Sidebars are automatically added to any file and any\n                       file from subdirectories that do not have their own sidebar files. If you wanted\n\n                       to add sidebars specific to a subdirectory, add another file in the subdirectory\n                       and this file will override the top level sidebar file.\n\n\n                       NO STYLING OR JAVASCRIPT\n\n\n                       Finally, for security reasons, Gollum strips out all CSS and JavaScript from raw\n\n                       markup files. You can include your own JavaScript or CSS file when running\n                       Gollum locally using the --custom-css   or --custom-js    switches, but there is\n\n                       no way to include these files on a Wiki when your Gollum wiki is hosted on Git-\n                       Hub.\n\n                          Now that we have investigated the structure and format of using Gollum wi-\n                       kis, we can dig into the power tools that come with Gollum.\n\n\n\n                       Moving Gollum to Your Laptop\n\n\n                       Though many people edit Gollum wikis exclusively from within the GitHub on-\n                       line editor, there is a real flexibility and power when hosting your wiki locally\n\n                       and editing it from your laptop. To do this you need to install the command line\n                       tools for Gollum.\n\n\n                          $ gem install gollum\n\n\n                          You will then see thegollum  command in your path.\n\n\n                            There is a difference between thgollumcommand (what you run from\n                            the command line) and the suite of technologies that make up Gollum as\n\n                            a project. To differentiate between them, remember that we are talking\n                            about the suite when the word is capitalized (“Gollum”), and the com-\n                            mand line tool when the word is lowercased and fixed width fogol-\n                            lum).\n\n\n\n                          What additional options are opened up when running locally? Let’s take a\n                       deeper look.\n\n\n\n\n\n\n\n\n\n        56","initials":"ew"},"\"Rolling our own editor feels like reinventing the wheel. There are plently of Markdown editors readily available already\"":{"page":59,"text":"                                                                                      Building a Gollum Editor\n\n\n\n     A word of caution when using the gollum command in server mode to\n     edit files locally inside a web browser. If you start the gollum server from\n\n     the command line you do have the capability to edit the files from any\n     computer within the same network. In other words, you could find your\n     IP address and use that address from your Chromebook or your tablet to\n\n     view and edit your wiki. However, remember that the gollum server com-\n     mand does not have an authentication system built into it, which means\n\n     that gollum thinks anyone accessing the wiki is the same user that start-\n     ed the gollum command. This is fine if you are in the other room editing\n\n     on your tablet while gollum runs on your work laptop. However, the gol-\n     lum server is not a good solution for offering a wiki up to many people\n     within a subnet. If multiple people edit files, there is no way that gollum\n\n     can track the different user contributions in the change log. This is not a\n     problem when editing your Gollum wiki inside GitHub.com: the GitHub\n\n     site knows who you are and properly assigns your changes to your user-\n     name inside the change log.\n\n\n\n   We’ve played a bit with the gollum command line tools. Let’s put these skills\nto use and make our own special gollum tool.\n\n\n\n\nBuilding a Gollum Editor\n\n\nOnce you understand Git repositories, you can see the power of Gollum as a wi-\n\nki format: as everything is built on Git, you can manage your wiki using all the\ngreat tools that come with Git. We’ve explored how easy it is to edit Gollum wi-\n\nkis: from within the command line, from the web browser, or from within Git-\n\nHub. However, there might be times when you need to provide an entirely cus-\ntomized editing modality. As long as you write files into the repository in the\n\nway the gollum libraries understand, you can write your own editing interface\n\nto suit your own needs. Let’s experiment with this idea and build a new editing\ninterface for Gollum wikis.\n\n\n\n     Gollum is a tool that provides flexibility by allowing local usage: this can\n     be very handy when you are on a plane and don’t want to pay for Wi-Fi.\n\n     However, at the time of this writing there is a bug where images are not\n     displayed, so although you can fully edit files using the logollum com-\n\n     mand, you will not be be able to view them when viewing your wiki on\n     you local machine. To view image files correctly, publish them into Git-\n     Hub.\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                 59","initials":"ew"},"\"This story is really out of the blue and feels very generic and biased.\"":{"page":60,"text":"CHAPTER 4: Gollum\n\n\n                       Hacking Gollum\n\n                       Team software development often revolves around this idealized scenario: a\n\n                       business person develops the structure of the application with higher-up stake-\n                       holders, these ideas are passed down to a UI/UX designer who then creates\n\n                       wireframes and mockups of the interactions, and then a software developer\n                       takes these wireframes and builds the software. Put another way, program\n                       managers figure out what features provide the most value to users, which then\n\n                       trickles down into the wireframes as real interactions. Many hidden blocking\n                       paths are fleshed out here, places where the application would confuse the\n\n                       user, and time is saved because the software developer does not have to waste\n                       time building something that would not work anyway. By the time it reaches\n                       the developer, the UI interaction is streamlined and the costly and inflexible\n\n                       stage of building software has all the inefficiencies optimized away. The devel-\n                       oper can simply work on a piece of software and know there are no changes,\n\n                       changes which would be extremely costly to implement.\n                          In practice, this process is almost never so smooth. What typically happens\n\n                       is the business people don’t completely understand all the requirements when\n                       they document the structure they want, so after they have committed to a\n                       structure they later ask for changes, which trickle down into the designs. The\n\n                       “final and approved” mockups have to be changed and this then needs to be\n                       communicated to the developer, who has already started building something\n\n                       that was “set in stone.” Or, the developer, as she is building the software, real-\n                       izes there are missing steps to get to places inside the application, and needs to\n\n                       communicate this back to the designer. If you have multiple people doing soft-\n                       ware development on a project, this information then needs to fan out to them\n                       if their areas are affected by these changes. This information must traverse\n\n                       many different people, with many different methods of communication.\n                          Wikis are a great way to store this kind of information. Information which\n\n                       changes. Information which must be retrieved by many people and edited by\n                       many people. What better than to manage these informational transitions than\n\n                       a change tracking system like Git, and what better way to absorb this informa-\n                       tion than a Wiki built on top of Git, hosted on GitHub.\n\n\n                       Wireframe Review Tool\n\n\n                       Let’s build a simple tool which stores these types of changes. We’ll build an im-\n                       age editor that hosts changes to UI mockups. This will give our executives a\n\n                       place where they can see changes and updates. This will allow our UI designer a\n                       place to store their images and annotate them with vital information. And, we’ll\n\n                       have a place where developers can retrieve information without reviewing their\n                       email and wondering “Do I have the most up-to-date mockups?” We’ll buid a\n\n\n\n\n        60","initials":"ew"},"\"Why can't the reader just fork a seed repo?\"":{"page":61,"text":"                                                                                Building a Gollum Editor\n\n\nspecial interface which allows quickly editing and reviewing these files locally.\nAnd all of it can be published into GitHub for review (though we won’t allow ed-\n\niting of the information there, since GitHub has its own editing modality.)\n   Gollum is built on Ruby and uses the Grit library underneath. Using Ruby\n\nmakes sense because we can leverage the existing Grit and Gollum libraries.\nWe’ll also use Sinatra, the same web application library we used in the last\n\nchapter.\n\n\n     The gollumcommand is, in fact, a customized wrapper around Sinatra.\n\n\n   This will be a dual purpose repository. We can use the repository with gol-\nlum as a standard wiki. And, we can use it with our application to enter data in\n\na more powerful way than gollum permits from its default interface. The data\nwill still be compatible with gollum and will be hosted on GitHub.\n\n   To begin, initialize our repository.\n\n\n   $ mkdir images\n   $ cd images\n   $ git init .\n   $ printf \"### Our home\" > Home.md\n   $ git add Home.md\n\n   $ git commit -m \"Initial checking\"\n\n   We’ve just created a wiki compatible with gollum. Let’s see what it looks like\n\ninside gollum. run the  gollum   command then open      http://localhost:\n4567/  in your browser.\n\n\n\n\n                                                                                  FIGURE 4-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                          61","initials":"ew"},"\"This is just throwing a load of code and commands at the reader. It is confusing and there doesn't read like there is a lesson here.\"":{"page":62,"text":"CHAPTER 4: Gollum\n\n\n                         As you can see, this tiny set of commands was enough to create the basics of\n                      the gollum wiki structure.\n\n                         Create our sinatra script callimage.rb  , and then we can install the neces-\n                      sary gems and run our server application.\n\n\n                         require 'sinatra'\n                         require 'gollum-lib'\n                         wiki = Gollum::Wiki.new(\".\")\n                         get '/pages' do\n                            \"All pages: \\n\" + wiki.pages.collect { |p| p.path }.join( \"\\n\" )\n\n                         end\n\n                         $ printf \"source 'https://rubygems.org'\\n\\ngem 'sinatra'\\ngem 'gollum-lib'\" >> Gemfile\n                         $ bundle install\n\n                         $ ruby image.rb\n                         $ open http://localhost:4567 # or whatever URL is reported from Sinatra\n\n                         Once you open this in your browser, you’ll see a report of the files that exist\n\n                      in our Gollum wiki right now. We’ve only added one file, Home.md  file.\n\n\n                      Programmatically Handling Images\n\n\n                      Let’s add to our server. We want to support uploading ZIP files into our system\n                      that we will then unpack and add to our repository, as well as adding a list of\n\n                      these files to our wiki. Modify our image.rb script to look like this:\n\n                         require 'sinatra'\n\n                         require 'gollum-lib'\n                         require 'tempfile'\n                         require 'zip/zip'\n\n\n                         def index( message=nil )\n                            response = File.read(File.join('.', 'index.html'))\n                            response.gsub!( \"<!-- message -->\\n\", \"<h2>Received and unpacked #{message}</h2>\" ) if message\n                            response\n                         end\n\n\n                         wiki = Gollum::Wiki.new(\".\")\n                         get '/' do\n                            index()\n                         end\n\n\n                         post '/unpack' do\n                            @repo = Rugged::Repository.new('.')\n                            @index = Rugged::Index.new\n\n\n                            zip = params[:zip][:tempfile]\n\n\n\n       62","initials":"ew"},"\"This is very dry, we're looking at config files and discussing random minutia.\"":{"page":66,"text":"CHAPTER 4: Gollum\n\n\n                          [user]\n                                   name = Chris Dawson\n                                   email = xrdawson@gmail.com\n                          [credential]\n\n                                   helper = cache --timeout=3600\n                          ...\n\n                          Just to double check that everything worked properly, let’s verify that things\n\n                       are working correctly after uploading a ZIP file. Jumping into a terminal win-\n                       dow after uploading a new file, imagine running these commands:\n\n\n                          $ git status\n\n\n                          To our surprise, we will see something like this:\n\n                          $ git status\n\n                          On branch master\n                          Changes to be committed:\n                            (use \"git reset HEAD <file>...\" to unstage)\n\n                            deleted:     images/3190a7759f7f6688b5e08526301e14d115292a6e/IMG_20120825_164703.jpg\n\n                            deleted:     images/3190a7759f7f6688b5e08526301e14d115292a6e/IMG_20130704_151522.jpg\n                            deleted:     images/3190a7759f7f6688b5e08526301e14d115292a6e/IMG_20130704_174217.jpg\n\n                          We just added those files; why is Git reporting them as deleted?\n\n                          To understand why this happens, remember that in Git there are three\n                       places where files can reside: the working directory, the staging area or index,\n\n                       and the repository itself. Your working directory is the set of local files which\n                       you are working on. The  git status   command describes itself as “show the\n\n                       working tree status.” Rugged operates on the repository itself, and our Rugged\n                       calls above operated on the index and then built a commit. This is important to\n\n                       note because our files will not exist in our working directory if we only write\n                       them using the Rugged calls, and if we do this, we cannot reference them inside\n                       our wiki page when we are running Gollum locally. We’ll fix this in the next sec-\n\n                       tion.\n                          We’ve now added the files to our repository, but we have not exposed these\n\n                       files inside our wiki. Let’s modify our server script to write out each file to a wiki\n                       page for review. As we mentioned in the previous section, we need to make sure\n\n                       that we write the files to both the working index and the repository (using the\n                       Rugged library write call). Then we can generate a Review file which details all\n\n                       the images uploaded.\n\n\n\n\n\n\n\n\n\n        66","initials":"ew"},"\"Again, this is very dry and I don't understand why we are discussing it\"":{"page":69,"text":"                                                                               Building a Gollum Editor\n\n\n   count: 0\n   size: 0\n   in-pack: 21\n   packs: 1\n   size-pack: 2578\n\n   prune-packable: 0\n   garbage: 0\n   size-garbage: 0\n\n   As you can see, our packed-size has barely changed, an indication that the\n\nonly changes were a new Git tree object and commit object. We still do have the\nfiles located in our repository at a variety of paths so our review pages will work\n\nno matter what revision we are accessing.\n\n   $ find images\n\n   images\n   images/7507409915d00ad33d03c78af0a4004797eec4b4\n   images/7507409915d00ad33d03c78af0a4004797eec4b4/IMG_20120825_164703.jpg\n   images/7507409915d00ad33d03c78af0a4004797eec4b4/IMG_20130704_151522.jpg\n   images/7507409915d00ad33d03c78af0a4004797eec4b4/IMG_20130704_174217.jpg\n\n   images/7f9505a4bafe8c8f654e22ea3fd4dab8b4075f75\n   images/7f9505a4bafe8c8f654e22ea3fd4dab8b4075f75/IMG_20120825_164703.jpg\n   images/7f9505a4bafe8c8f654e22ea3fd4dab8b4075f75/IMG_20130704_151522.jpg\n   images/7f9505a4bafe8c8f654e22ea3fd4dab8b4075f75/IMG_20130704_174217.jpg\n   images/b4be28e5b24bfa46c4942d756a3a07efd24bc234\n\n   images/b4be28e5b24bfa46c4942d756a3a07efd24bc234/IMG_20130704_151522.jpg\n   images/b4be28e5b24bfa46c4942d756a3a07efd24bc234/IMG_20130704_174217.jpg\n\n   Git and Gollum can efficiently store the same file at different paths without\n\noverloading the repository.\n\n\nReviewing on GitHub\n\n\nThe raison d’etre for this wiki is to annotate a development project. If you fol-\nlow the instructions above and create a new wiki for a repository, you’ll then be\n\nable to push up the changes we’ve made using our  image.rb  script. Once you\nhave created a new wiki, look for a box on the right which says “Clone this wiki\n\nlocally”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                         69","initials":"ew"},"\"Good to go over the pull request system, the subject seems a bit odd though\"":{"page":71,"text":"                                                                                     Building a Gollum Editor\n\n\n\n                                                                                      FIGURE 4-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Not sure why our designer is providing us with an image of a couch, but I am\n\nsure he has his reasons.\n   Once have published the file, we can click on the “Review” link in the sidebar\n\nto see the most current version of the “Review” page. We also can review the\nrevisions of this file by clicking on the “3 Commits” (or whatever number of\n\ncommits have occurred with this file). link right underneath the page title.\nJumping onto that page shows us the full history of this file.\n\n\n\n\n                                                                                      FIGURE 4-6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                71","initials":"ew"},"\"You can see this from the history though?\\nSeems strange for us to be covering this\"":{"page":72,"text":"CHAPTER 4: Gollum\n\n\n                          Clicking on any of the SHA hashes will display the page at that revision in our\n                       history and show us the state of the document at any given moment in history.\n                       Unfortunately, jumping back and forth between revisions requires two clicks,\n\n                       one from the review page to the list of revisions, and then another click to jump\n                       into the revision we want, but this permits us to review changes between the\n\n                       comps provided from our designer.\n                          It would be nice if GitHub provided a simple way to jump from a revision to\n\n                       the parent (older) revision, but they don’t expose this in their site as of this writ-\n                       ing. We can fix this, however, by generating our own special link inside the re-\n                       view page itself which will magically know about how to navigate to a previous\n\n                       version of the page.\n\n\n                       Improving Revision Navigation\n\n\n                       In our example, we only have three revisions right now, and all share the same\n                       commit message (“Adding new images”). This is not very descriptive and makes\n\n                       it challenging to understand the differences between revisions, critical when we\n                       are trying to understand how things have changed between comps. We can im-\n\n                       prove this easily.\n                          First, let’s add a commit message field to our upload form.\n\n\n                          <html>\n                          <body>\n                          <!-- message -->\n                          <form method='POST' enctype='multipart/form-data' action='/unpack'>\n                          Choose a zip file:\n\n                          <input type='file' name='zip'/>\n                          <input type='text' name='message' placeholder='Enter commit message'/>\n                          <input type='submit' name='submit'>\n                          </form>\n\n                          </body>\n                          </html>\n\n                          Then, let’s adjust the commit message inside our image.rb   script, which is\n\n                       a one line change to the options hash, setting the value of it to the parameter\n                       we are now passing in for “commit”.\n\n\n                            ...\n                          options[:committer] = { :email => @email, :name => @name, :time => Time.now }\n                          options[:message] = params[:message]\n\n                          options[:parents] = @repo.empty? ? [] : [ @repo.head.target ].compact\n                            ...\n\n\n\n\n\n\n\n        72","initials":"ew"},"\"Again this functionality seems to have limited use\"":{"page":73,"text":"                                                                                 Building a Gollum Editor\n\n\n   Now, if our designer posts a new version of the UI comps, they can specify\nwhat changes were made, and we have a record of that in our change log, ex-\nposed on the revisions section of our wiki hosted on GitHub.\n\n\n\nFixing Linking Between Comp Pages\n\n\nWe noted that there is no quick way to jump between comps once we are inside\na review revision. However, if you recall we used the parent SHA hash to build\nout our image links. We can use this to build out a navigation inside our comp\n\npage when we are on a revision page while viewing the history.\n   Again, it is a simple change: one line within twrite_review_file     meth-\n\nod. After the block which creates each link to the image files, add a line which\nbuilds a link to the parent document via its SHA hash using the parent SHA\n\nfound in our Rugged object under  @repo.head.target    . This link will allow us\nto navigate to prior revisions in our history.\n\n\n     ...\n   files.each do |f|\n     contents += \"### #{f} \\n[[#{dir}/#{f}]]\\n\\n\"\n   end\n\n   contents += \"[Prior revision (only when viewing history)](#{@repo.head.target})\\n\\n\"\n\n   File.write review_filename, contents\n   oid = @repo.write( contents, :blob )\n     ...\n\n\n   Now, when we view the Review file history, we see a file with a link at the\nbottom to the link to each prior version. Is it possible to provide a link to the\n\nnext version in our history? Unfortunately, we have no way to predict the SHA\nhash of the next commit made to the repository, so we cannot build this link\n\ninside our Review.md  file with our ruby script. However, we do get something\njust as good for free because we can simply use the back button to jump back\n\nto the prior page in the history stack of our browser. It would be nice if we could\ngenerate this link alongside the link we placed into the wiki markup, and we\ncould do this using a link that runs an onclick handler delegating to a Java-\n\nScript command like   window.history.back()     , but Gollum foils us again by\nstripping JavaScript from our markup files as we noted before. This is a good\n\nthing generally, as we don’t want to permit rogue markup inside our wiki pages,\nbut it does limit our options in this situation.\n\n   Unfortunately, these links do not work when you are viewing the review file\nitself (clicking on them brings you to a page which asks you to create this as a\n\nnew page). Gollum, unlike Jekyll, does not support Liquid tags which would\npermit building a link using the username and repository. Right now we don’t\n\n\n\n\n                                                                                           73","initials":"ew"},"\"I would say that writing/modifying a wiki isn't something unique to GitHub. The contents of this chapter don't necessarily all contribute to this idea of a custom editor... more so it is an assortment of features that have been hacked in.\\nI'd reconsider \\nwhat the goal of this chapter is and possibly cut it down.\"":{"page":74,"text":"CHAPTER 4: Gollum\n\n\n                       have access to these variables, so our link needs to be relative, which works\n\n                       when we are in history review, but not in the normal review. It does not affect\n                       viewing the files so this would require educating your stakeholders on the limi-\n\n                       tations of this link.\n\n\n                       Summary\n\n\n                       In this chapter we learned how to create a Gollum wiki from scratch, both on\n\n                       GitHub and as a fresh repository from the comman line. We then looked at the\n                       different ways to use thgollum  command line tool and learned why this is a\n                       nice option when we want to run our own Gollum server. Finally, we built a cus-\n\n                       tomized Gollum image-centric editor using the Rugged library for Ruby.\n                          Our next chapter explores the GitHub API from what might be an unexpected\n\n                       vantage point: JavaScript. In this chapter we will show you how to host an en-\n                       tire application on GitHub, no server required, that still allows modification of\n                       data inside of GitHub by its users.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       74","initials":"ew"},"\"Make this clearer that it is a list of operations.\"":{"page":75,"text":"           Python and the Search API                                   5\n\n\n\n\n\n\n\n\nOnce you have enough data, no amount of organization will make everything\neasy to find. As Google has taught us, the only system that works at this scale is\na search box. When you use GitHub, you’re exposed to both sides of this phe-\nnomenon: the repositories you have direct access to — which are relatively\n\nsmall in number — are given a single level of hierarchy, so you can keep them\nstraight in your head. For the rest, the uncountable millions of public reposito-\nries that belong to other people, there’s a search box, with powerful features to\nhelp you find what you’re looking for.\n   Helpfully, GitHub also exposes this capability as an API you can consume\n\nfrom your own applications. GitHub’s search API gives you access to the full\npower of the built-in search function. This includes the use of logical and scop-\ning operators, loreand user. By integrating this feature with your applica-\ntion, you can provide your users a very powerful way of finding what they’re\nlooking for.\n\n   In this chapter we’ll take a close look at this API, and try building something\nwith it. We’ll see how the search API is structured, what kind of results come\nback, and how it can help us create a feature for someone on our team.\n\n\nGeneral Principles\n\nThe search API is split into four separate parts:\n\n   • Repositories\n   • Code\n\n   • Issues\n   • Users\n\n   These APIs all have separate subject matter, and have different formats for\ntheir results, but they all behave the same in a few key ways. We’re covering\nthese first, because they’ll help you understand the results coming back from\n\n\n\n\n                                                                              75","initials":"ew"},"\"This introduction is good, it is short and to the point.\"":{"page":75,"text":"           Python and the Search API                                   5\n\n\n\n\n\n\n\n\nOnce you have enough data, no amount of organization will make everything\neasy to find. As Google has taught us, the only system that works at this scale is\na search box. When you use GitHub, you’re exposed to both sides of this phe-\nnomenon: the repositories you have direct access to — which are relatively\n\nsmall in number — are given a single level of hierarchy, so you can keep them\nstraight in your head. For the rest, the uncountable millions of public reposito-\nries that belong to other people, there’s a search box, with powerful features to\nhelp you find what you’re looking for.\n   Helpfully, GitHub also exposes this capability as an API you can consume\n\nfrom your own applications. GitHub’s search API gives you access to the full\npower of the built-in search function. This includes the use of logical and scop-\ning operators, loreand user. By integrating this feature with your applica-\ntion, you can provide your users a very powerful way of finding what they’re\nlooking for.\n\n   In this chapter we’ll take a close look at this API, and try building something\nwith it. We’ll see how the search API is structured, what kind of results come\nback, and how it can help us create a feature for someone on our team.\n\n\nGeneral Principles\n\nThe search API is split into four separate parts:\n\n   • Repositories\n   • Code\n\n   • Issues\n   • Users\n\n   These APIs all have separate subject matter, and have different formats for\ntheir results, but they all behave the same in a few key ways. We’re covering\nthese first, because they’ll help you understand the results coming back from\n\n\n\n\n                                                                              75","initials":"ew"},"\"Would it be more useful to compare all the different types of search up front? perhaps with a table, with ticks to show off the different features?\"":{"page":82,"text":"CHAPTER 5: Python and the Search API\n\n\n                            \"items\": [\n\n                              {\n                                \"login\": \"ben\",\n                                \"id\": 39902,\n                                \"avatar_url\": \"…\",\n                                \"gravatar_id\": \"\",\n                                \"url\": \"…\",\n\n                                \"html_url\": \"…\",\n                                …\n                                \"score\": 98.24275\n                              },\n                              {\n\n                                \"login\": \"bengottlieb\",\n                                \"id\": 53162,\n                                \"avatar_url\": \"…\",\n                                \"gravatar_id\": \"\",\n                                \"url\": \"…\",\n\n                                \"html_url\": \"…\",\n                                …\n                                \"score\": 35.834213\n                              },\n                            ]\n                          }\n\n\n                          The list of items in this case look like the results from a query users/\n\n                       <name>  endpoint. Useful items here are the user’s avatavatar_url   ), several\n                       links to other API endpoints repos_url  ,url ), and the type of result (user or\n\n                       organization, itype ).\n\n\n                       Our example application\n\n\n                       Now that we know a bit about how this API behaves, let’s do something useful\n\n                       with it.\n                          Imagine your development team uses GitHub to store their Git repositories,\n                       and that there are lots of little repositories for parts of the application that work\n\n                       together at runtime. This kind of situation ends up being fairly difficult to work\n                       with for your non-technical colleagues; if they want to report an issue, they\n\n                       don’t know where to go, and they don’t know how to find issues that already\n                       exist.\n                          Search can make this possible, but doing a search across an entire organiza-\n\n                       tion’s repositories involves using user:<organization>     operator, which is\n                       obtusely named, and kind of scary for non-programmers.\n\n                          The Search API can make this a bit easier. Let’s make a GUI application with\n                       just a single search box, which makes it dead simple for a non-technical user to\n\n\n\n\n\n       82","initials":"ew"},"\"This style of showing off the API in use is great, I'd just be wary of being too repetitive.\"":{"page":82,"text":"CHAPTER 5: Python and the Search API\n\n\n                            \"items\": [\n\n                              {\n                                \"login\": \"ben\",\n                                \"id\": 39902,\n                                \"avatar_url\": \"…\",\n                                \"gravatar_id\": \"\",\n                                \"url\": \"…\",\n\n                                \"html_url\": \"…\",\n                                …\n                                \"score\": 98.24275\n                              },\n                              {\n\n                                \"login\": \"bengottlieb\",\n                                \"id\": 53162,\n                                \"avatar_url\": \"…\",\n                                \"gravatar_id\": \"\",\n                                \"url\": \"…\",\n\n                                \"html_url\": \"…\",\n                                …\n                                \"score\": 35.834213\n                              },\n                            ]\n                          }\n\n\n                          The list of items in this case look like the results from a query users/\n\n                       <name>  endpoint. Useful items here are the user’s avatavatar_url   ), several\n                       links to other API endpoints repos_url  ,url ), and the type of result (user or\n\n                       organization, itype ).\n\n\n                       Our example application\n\n\n                       Now that we know a bit about how this API behaves, let’s do something useful\n\n                       with it.\n                          Imagine your development team uses GitHub to store their Git repositories,\n                       and that there are lots of little repositories for parts of the application that work\n\n                       together at runtime. This kind of situation ends up being fairly difficult to work\n                       with for your non-technical colleagues; if they want to report an issue, they\n\n                       don’t know where to go, and they don’t know how to find issues that already\n                       exist.\n                          Search can make this possible, but doing a search across an entire organiza-\n\n                       tion’s repositories involves using user:<organization>     operator, which is\n                       obtusely named, and kind of scary for non-programmers.\n\n                          The Search API can make this a bit easier. Let’s make a GUI application with\n                       just a single search box, which makes it dead simple for a non-technical user to\n\n\n\n\n\n       82","initials":"ew"},"\"This seemed sort of redundant given that we could just search on the GitHub site itself?\"":{"page":83,"text":"                                                                                       Our example application\n\n\nsearch all the issues in all the repositories in a single organization. It’ll end up\n\nlooking a bit like Figure 5-1.\n\n\n\n                                                                                        FIGURE 5-1\n\n                                                                                        GitHub search\n                                                                                        application\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUser flow\n\n\nThat’s the overall goal, but let’s dig in to more detail about how the user experi-\nences the application.\n   The first thing we’ll do is require the user to log in with GitHub credentials.\n\nWhy? Partly because the search API is throttled pretty aggressively, and the rate\nlimits are higher with authenticated access. But also because our user is going\n\nto need the ability to search issues in private repositories. To make this easier,\nour program will try to get GitHub credentials from Git’s credential store, but\n\nit’ll fall back to a login form, which looks like Figure 5-2.\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                  83","initials":"ew"},"\"Feels a bit weird to introduce all of this when the reader hasn't begun coding any of it yet?\"":{"page":84,"text":"CHAPTER 5: Python and the Search API\n\n\n\n FIGURE 5-2\n\n Login UI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Once the user logs in, they’ll be shown a search box. Typing in a search query\n                        and hitting enter will result in a scrollable list of search results, with titles and\n\n                        the first line of the description. Clicking on a search result opens the issue in the\n                        user’s browser.\n\n                           That’s about it. This application only has two main screens from the user’s\n                        point of view. It’s a simple, focused tool to solve a very tightly-defined problem,\n                        so the code shouldn’t be too hard.\n\n\n\n                        Python\n\n\n                        Now that we know how the program should act, let’s decide how it should\n\n                        work.\n                           We’ll use Python for our implementation language, for several reasons. First,\n\n                        because we haven’t yet seen it in this book, and we like to expose you to a wide\n                        variety of languages. One of our goals is to help the reader explore technologies\n                        they might not have seen before.\n\n                           Secondly, there’s a library for building GUI applications that run without\n                        modification on Mac OS X, Linux, and Windows. Surprisingly, this is fairly\n\n                        unique feature among modern high-level programming languages. If you want\n                        this capability elsewhere, you usually have to use a high-complexity frame-\n\n                        work, a lower-level language like C++, or both.\n                           Thirdly, this will help make it easy to distribute. Python has a package avail-\n                        able which There exists a Python package which bundles an entire Python pro-\n\n                        gram and all of its dependencies into a single file (.app  bundle on OS X). So\n                        giving this program to a colleague is as easy as emailing her a ZIP file.\n\n                           Let’s take a quick look at the libraries we’ll be using in our application’s\n                        code. We’ll see them in action later on, but a quick overview will help you un-\n\n\n\n\n\n        84","initials":"ew"},"\"This is a terrible reason as it is superficial; covering lots of languages doesn't make for a better book. \\n\\nIn fact, an arbitrary tour of different programming languages as a tick box exercise just serves to confuse the reader and disrupt the flow of t\\nhe book.\"":{"page":84,"text":"CHAPTER 5: Python and the Search API\n\n\n\n FIGURE 5-2\n\n Login UI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Once the user logs in, they’ll be shown a search box. Typing in a search query\n                        and hitting enter will result in a scrollable list of search results, with titles and\n\n                        the first line of the description. Clicking on a search result opens the issue in the\n                        user’s browser.\n\n                           That’s about it. This application only has two main screens from the user’s\n                        point of view. It’s a simple, focused tool to solve a very tightly-defined problem,\n                        so the code shouldn’t be too hard.\n\n\n\n                        Python\n\n\n                        Now that we know how the program should act, let’s decide how it should\n\n                        work.\n                           We’ll use Python for our implementation language, for several reasons. First,\n\n                        because we haven’t yet seen it in this book, and we like to expose you to a wide\n                        variety of languages. One of our goals is to help the reader explore technologies\n                        they might not have seen before.\n\n                           Secondly, there’s a library for building GUI applications that run without\n                        modification on Mac OS X, Linux, and Windows. Surprisingly, this is fairly\n\n                        unique feature among modern high-level programming languages. If you want\n                        this capability elsewhere, you usually have to use a high-complexity frame-\n\n                        work, a lower-level language like C++, or both.\n                           Thirdly, this will help make it easy to distribute. Python has a package avail-\n                        able which There exists a Python package which bundles an entire Python pro-\n\n                        gram and all of its dependencies into a single file (.app  bundle on OS X). So\n                        giving this program to a colleague is as easy as emailing her a ZIP file.\n\n                           Let’s take a quick look at the libraries we’ll be using in our application’s\n                        code. We’ll see them in action later on, but a quick overview will help you un-\n\n\n\n\n\n        84","initials":"ew"},"\"Python wasn't really mentioned in the introduction of the book. If we are insisting on building this app, why wasn't Ruby on Rails used? The reader was expected to know Ruby well after all?\"":{"page":84,"text":"CHAPTER 5: Python and the Search API\n\n\n\n FIGURE 5-2\n\n Login UI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Once the user logs in, they’ll be shown a search box. Typing in a search query\n                        and hitting enter will result in a scrollable list of search results, with titles and\n\n                        the first line of the description. Clicking on a search result opens the issue in the\n                        user’s browser.\n\n                           That’s about it. This application only has two main screens from the user’s\n                        point of view. It’s a simple, focused tool to solve a very tightly-defined problem,\n                        so the code shouldn’t be too hard.\n\n\n\n                        Python\n\n\n                        Now that we know how the program should act, let’s decide how it should\n\n                        work.\n                           We’ll use Python for our implementation language, for several reasons. First,\n\n                        because we haven’t yet seen it in this book, and we like to expose you to a wide\n                        variety of languages. One of our goals is to help the reader explore technologies\n                        they might not have seen before.\n\n                           Secondly, there’s a library for building GUI applications that run without\n                        modification on Mac OS X, Linux, and Windows. Surprisingly, this is fairly\n\n                        unique feature among modern high-level programming languages. If you want\n                        this capability elsewhere, you usually have to use a high-complexity frame-\n\n                        work, a lower-level language like C++, or both.\n                           Thirdly, this will help make it easy to distribute. Python has a package avail-\n                        able which There exists a Python package which bundles an entire Python pro-\n\n                        gram and all of its dependencies into a single file (.app  bundle on OS X). So\n                        giving this program to a colleague is as easy as emailing her a ZIP file.\n\n                           Let’s take a quick look at the libraries we’ll be using in our application’s\n                        code. We’ll see them in action later on, but a quick overview will help you un-\n\n\n\n\n\n        84","initials":"ew"},"\"This is supposed to be a simple educational example though? Why are we worried about distributing it when Google and GitHub search are so readily available?\"":{"page":84,"text":"CHAPTER 5: Python and the Search API\n\n\n\n FIGURE 5-2\n\n Login UI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Once the user logs in, they’ll be shown a search box. Typing in a search query\n                        and hitting enter will result in a scrollable list of search results, with titles and\n\n                        the first line of the description. Clicking on a search result opens the issue in the\n                        user’s browser.\n\n                           That’s about it. This application only has two main screens from the user’s\n                        point of view. It’s a simple, focused tool to solve a very tightly-defined problem,\n                        so the code shouldn’t be too hard.\n\n\n\n                        Python\n\n\n                        Now that we know how the program should act, let’s decide how it should\n\n                        work.\n                           We’ll use Python for our implementation language, for several reasons. First,\n\n                        because we haven’t yet seen it in this book, and we like to expose you to a wide\n                        variety of languages. One of our goals is to help the reader explore technologies\n                        they might not have seen before.\n\n                           Secondly, there’s a library for building GUI applications that run without\n                        modification on Mac OS X, Linux, and Windows. Surprisingly, this is fairly\n\n                        unique feature among modern high-level programming languages. If you want\n                        this capability elsewhere, you usually have to use a high-complexity frame-\n\n                        work, a lower-level language like C++, or both.\n                           Thirdly, this will help make it easy to distribute. Python has a package avail-\n                        able which There exists a Python package which bundles an entire Python pro-\n\n                        gram and all of its dependencies into a single file (.app  bundle on OS X). So\n                        giving this program to a colleague is as easy as emailing her a ZIP file.\n\n                           Let’s take a quick look at the libraries we’ll be using in our application’s\n                        code. We’ll see them in action later on, but a quick overview will help you un-\n\n\n\n\n\n        84","initials":"ew"},"\"Example seems really basic\"":{"page":86,"text":"CHAPTER 5: Python and the Search API\n\n\n                      configuration options. If you’ve written GUI applications before, you’ll know\n\n                      how hard each of these problems are.\n                         For information on this project, you can visit http://pythonhosted.org/\n\n                      PyInstaller. You can install it using Python’s package manager, by runnipip\n                      install pyinstaller     .\n\n\n\n                      The Code\n\n\n                      Alright, now you have an idea of which parts of the Python ecosystem will be\n                      helping us on our journey. Let’s get started looking at the code that brings them\n\n                      all together. We’ll start with this skeleton file:\n\n                         #!/usr/bin/env python\n\n\n                         import os, subprocess\n                         import wx\n                         from agithub import Github\n\n\n                         class SearchFrame(wx.Frame):\n                              pass\n\n                         if __name__ == '__main__':\n                              app = wx.App()\n\n                              SearchFrame(None)\n                              app.MainLoop()\n\n                         If you run this program, you should get an empty window, which is always a\n\n                      hopeful start to any project. Let’s take a look at a few key things:\n\n                         The “shebang” specifies that this is a Python 2.7 program\n\n\n                         Here we import our handy libraries. We import WxPython ( wx ) whole cloth,\n\n                         but with agithub  we only need the  Github  (note the capitalization) class.\n                         os and subprocess   come from the Python standard library.\n\n\n                         This is the class for our main window. We’ll walk through the particulars lat-\n                         er on when we discuss the real implementation.\n\n\n                         In Python, you create the main entry point of an application using this syn-\n                         tax.\n\n\n\n\n\n\n\n\n\n       86","initials":"ew"},"\"The code here uses a load of new functions/conventions that are likely to confuse the reader.\"":{"page":87,"text":"                                                                                            The Code\n\n\n\n   And this is how you write a “main” function in WxPython. We instantiate an\n   App instance, create an instance of our top-level frame, and run the app’s\n\n   main loop.\n\n\nGit credential helper\n\n\nThat’s how most of the UI code is going to be structured, but before we go any\nfurther, we should define a function to help us get the user’s GitHub credentials.\n\nWe’ll be cheating a bit, by asking Git if it has the user’s login and password.\n   We’ll leverage thegit credential fill     command. This is used internally\n\nby Git to avoid having to ask the user for their GitHub password every time they\ninteract with a GitHub remote. The way it works is by accepting all the known\nfacts about a connection through      stdin , as text lines in the format\n\n“<key>=<value>”. Once the caller has supplied all the facts it knows, it can close\nthe stdin stream (or supply an empty line), and Git will respond with all the\n\nfacts it knows about this connection. With any luck, this will include the user’s\nlogin and password. The whole interaction looks a bit like this:\n\n\n   $ echo \"host=github.com\" | git credential fill\n   host=github.com\n   username=ben\n   password=(redacted)\n\n\n\n   This passes a single line tgit credential     and closes stdin , which Git\n   will recognize as the end of input.\n\n\n   Git responds with all the facts it knows about the connection. This includes\n   the input values, as well as the username and password if Git knows them.\n\n   One other thing that you should know about git-credential    is that by de-\n\nfault, if it doesn’t know anything about the host, it’ll ask the user at the termi-\nnal. That’s bad for a GUI app, so we’re going to be disabling that feature\n\nthrough the use of theGIT_ASKPASS   environment variable.\n   Here’s what our helper looks like:\n\n\n   GITHUB_HOST = 'github.com'\n   def git_credentials():\n       os.environ['GIT_ASKPASS'] = 'true'\n       p = subprocess.Popen(['git', 'credential', 'fill'],\n\n                              stdout=subprocess.PIPE,\n                              stdin=subprocess.PIPE)\n       stdout,_ = p.communicate('host={}\\n\\n'.format(GITHUB_HOST))\n\n\n\n\n\n                                                                                          87","initials":"ew"},"\"Having a comma here looks wrong - this is symptomatic of a lack of explanation. We are assuming the reader is fluent in Python?\"":{"page":89,"text":"                                                                                            The Code\n\n\n\n            self.SetTitle('GitHub Issue Search')\n            self.Show()\n\n\n   The __init__   method is the constructor, so this is where we start when the\nmain function callsSearchFrame()   . Here’s what’s happening at a high level –\n\nwe’ll dig into the details in a bit:\n\n   1. Set up some layout dimensions and pass to the parent class’s constructor\n\n   2. Create the UI controls\n\n   3. Retrieve the credentials from the user using the credential helper we de-\n      scribed earlier\n\n   4. Change the title and display the application to the user\n\n   Before we get to how all those things are done, let’s step back a bit and talk\nabout this class’s job. It’s responsible for maintaining the top-level “frame” (a\n\nwindow with a title bar, a menu, and so on), and deciding what’s displayed in\nthat frame. In this case, we want to show a login UI first, and when we get valid\n\ncredentials (either from Git or the user), we’ll switch to a searching UI.\n   Alright, enough background. Let’s walk through the code for getting and\n\nchecking credentials.\n\n       def login_accepted(self, username, password):\n\n            self.credentials['username'] = username\n            self.credentials['password'] = password\n            if self.test_credentials():\n                self.switch_to_search_panel()\n\n\n       def test_credentials(self):\n            if any(k not in self.credentials for k in ['username', 'password']):\n                return False\n            g = Github(self.credentials['username'], self.credentials['password'])\n\n            status,data = g.user.orgs.get()\n            if status != 200:\n                print('bad credentials in store')\n                return False\n            self.orgs = [o['login'] for o in data]\n\n            return True\n\n       def switch_to_search_panel(self):\n            self.login_panel.Destroy()\n            self.search_panel = SearchPanel(self,\n\n                                               orgs=self.orgs,\n                                               credentials=self.credentials)\n            self.sizer.Add(self.search_panel, 1, flag=wx.EXPAND | wx.ALL, border=10)\n            self.sizer.Layout()\n\n\n\n\n\n                                                                                          89","initials":"ew"},"\"This line looks like it could do with some explanation\"":{"page":89,"text":"                                                                                            The Code\n\n\n\n            self.SetTitle('GitHub Issue Search')\n            self.Show()\n\n\n   The __init__   method is the constructor, so this is where we start when the\nmain function callsSearchFrame()   . Here’s what’s happening at a high level –\n\nwe’ll dig into the details in a bit:\n\n   1. Set up some layout dimensions and pass to the parent class’s constructor\n\n   2. Create the UI controls\n\n   3. Retrieve the credentials from the user using the credential helper we de-\n      scribed earlier\n\n   4. Change the title and display the application to the user\n\n   Before we get to how all those things are done, let’s step back a bit and talk\nabout this class’s job. It’s responsible for maintaining the top-level “frame” (a\n\nwindow with a title bar, a menu, and so on), and deciding what’s displayed in\nthat frame. In this case, we want to show a login UI first, and when we get valid\n\ncredentials (either from Git or the user), we’ll switch to a searching UI.\n   Alright, enough background. Let’s walk through the code for getting and\n\nchecking credentials.\n\n       def login_accepted(self, username, password):\n\n            self.credentials['username'] = username\n            self.credentials['password'] = password\n            if self.test_credentials():\n                self.switch_to_search_panel()\n\n\n       def test_credentials(self):\n            if any(k not in self.credentials for k in ['username', 'password']):\n                return False\n            g = Github(self.credentials['username'], self.credentials['password'])\n\n            status,data = g.user.orgs.get()\n            if status != 200:\n                print('bad credentials in store')\n                return False\n            self.orgs = [o['login'] for o in data]\n\n            return True\n\n       def switch_to_search_panel(self):\n            self.login_panel.Destroy()\n            self.search_panel = SearchPanel(self,\n\n                                               orgs=self.orgs,\n                                               credentials=self.credentials)\n            self.sizer.Add(self.search_panel, 1, flag=wx.EXPAND | wx.ALL, border=10)\n            self.sizer.Layout()\n\n\n\n\n\n                                                                                          89","initials":"ew"},"\"seems like a bit of a cop out not to do this properly?\"":{"page":90,"text":"CHAPTER 5: Python and the Search API\n\n\n                          Each of these three methods comes in at a different point during our pro-\n                       gram’s execution. If our credentials are coming from Git, we proceed straight to\n\n                       test_credentials    ; if they’re coming from the login panel (see below), they go\n                       through the  login_accepted    callback first, which then calltest_creden-\n\n                       tials .\n                          Either way, what we do is try to fetch a list of the user’s organizations, to see\n\n                       if they work. Here you can see the usage pattern foagithub   – the URL path is\n                       mapped to object-property notation on an instance of the   Github  class, and\n\n                       the HTTP verb is mapped to a method call. The return values are a status code\n                       and the data, which has been decoded into a dictionary object. If it fails —\n\n                       meaning the returned status is not200 — we send the user to the login panel. If\n                       it succeeds, we calswitch_to_search_panel     .\n\n\n                            We’re doing a synchronous network call on the UI thread. This is usually\n\n                            a bad idea, because the UI will become unresponsive until the network\n                            call completes. Ideally we’d move this out onto another thread, and get\n                            the return value with a message, but for this simple example (and use\n                            case), it’ll do.\n\n\n                          The last method handles the UI switch. The login panel is referenced by two\n\n                       things: the SearchFrame   instance (the parent window), and the sizer that’s\n                       controlling its layout. Fortunately, calling Destroy()   method cleans both\n\n                       of those up, so we can then create theSearchPanel   instance and add it to our\n\n                       sizer. Doing this requires a specific call to the siLayout()  method; other-\n                       wise the sizer won’t know that it needs to adjust the position and size of the\n                       new panel.\n\n\n                              def create_controls(self):\n                                   # Set up a menu. This is mainly for \"Cmd-Q\" behavior on OSX\n\n                                   filemenu = wx.Menu()\n                                   filemenu.Append(wx.ID_EXIT, '&Exit')\n                                   menuBar = wx.MenuBar()\n                                   menuBar.Append(filemenu, '&File')\n                                   self.SetMenuBar(menuBar)\n\n\n                                   # Start with a login UI\n                                   self.login_panel = LoginPanel(self, onlogin=self.login_accepted)\n\n\n                              def do_layout(self):\n                                   self.sizer = wx.BoxSizer(wx.VERTICAL)\n                                   self.sizer.Add(self.login_panel, 1, flag=wx.EXPAND | wx.ALL, border=10)\n                                   self.SetSizer(self.sizer)\n\n\n\n\n\n\n\n        90","initials":"ew"},"\"Feels like if we are going up cover all this UI code, that there should be more screenshots involved\"":{"page":91,"text":"                                                                                            The Code\n\n\n   create_controls    is fairly straightforward. It instantiates a menu that only\n\ncontains File>Exit, and a login panel, whose implementation we’ll cover a bit\nlater on. Note that when we create a visible control, we passself  as the first\n\nparameter to the constructor. That’s because the SearchFrame   instance we’re\nconstructing is the parent window of that control.\n\n   do_layout   uses a WxWidgets feature called “sizers” to do some automated\nlayout. Sizers are a complex topic, but here’s all you need to know about this\n\nsnippet:\n\n    • A BoxSizer  stacks widgets in a single direction, in this case vertically.\n\n    • The second parameter to   sizer.Add   is a scaling factor. If it’s zero, the\n      widget you’re adding will always stay the same size if the parent window\n\n      resizes; if it’s anything else, all the things the sizer is controlling will ad-\n      just to fill their container. There’s only one control in this sizer, but we still\n      want it to take up the full area of the window, so we p1.s\n\n    • The border  parameter tells the sizer how much area to leave around the\n\n      widget as padding.\n    • The wx.EXPAND   flag tells the sizer that we want the widget to expand in\n\n      the direction the sizer isn’t stacking. In this case, we’re stacking vertically,\n      but we also want this widget to expand horizontally.\n\n    • The wx.ALL  flag specifies which edges of the widget should have the bor-\n\n      der area.\n\n   That’s it! Aside from managing a couple of fields, most of this code is manag-\ning the UI, which is almost exactly what we’d want from a UI class. Let’s write\n\nthe first of the two panels that we swap in and out.\n\n\nGitHub login\n\n\nThe LoginPanel   class is similar in structure to tSearchFrame   class, with a\ncouple of key differences, which we’ll describe after the wall of code.\n\n\n   class LoginPanel(wx.Panel):\n       def __init__(self, *args, **kwargs):\n            self.callback = kwargs.pop('onlogin', None)\n            wx.Panel.__init__(self, *args, **kwargs)\n\n\n            self.create_controls()\n            self.do_layout()\n\n       def create_controls(self):\n            self.userLabel = wx.StaticText(self, label='Username:')\n\n            self.userBox = wx.TextCtrl(self, style=wx.TE_PROCESS_ENTER)\n\n\n\n                                                                                          91","initials":"ew"},"\"Given all the talk about needing to know JS at the start of the book, seeing all this code to do layout seems really strange.\"":{"page":92,"text":"CHAPTER 5: Python and the Search API\n\n\n                                  self.passLabel = wx.StaticText(self, label='Password (or token):')\n                                  self.passBox = wx.TextCtrl(self, style=wx.TE_PROCESS_ENTER)\n                                  self.login = wx.Button(self, label='Login')\n                                  self.error = wx.StaticText(self, label='')\n                                  self.error.SetForegroundColour((200,0,0))\n\n\n                                  # Bind events\n                                  self.login.Bind(wx.EVT_BUTTON, self.do_login)\n                                  self.userBox.Bind(wx.EVT_TEXT_ENTER, self.do_login)\n                                  self.passBox.Bind(wx.EVT_TEXT_ENTER, self.do_login)\n\n\n                             def do_layout(self):\n                                  # Grid arrangement for controls\n                                  grid = wx.GridBagSizer(3,3)\n                                  grid.Add(self.userLabel, pos=(0,0),\n\n                                           flag=wx.TOP | wx.LEFT | wx.BOTTOM, border=5)\n                                  grid.Add(self.userBox, pos=(0,1),\n                                           flag=wx.EXPAND | wx.LEFT | wx.RIGHT, border=5)\n                                  grid.Add(self.passLabel, pos=(1,0),\n                                           flag=wx.TOP | wx.LEFT | wx.BOTTOM, border=5)\n\n                                  grid.Add(self.passBox, pos=(1,1),\n                                           flag=wx.EXPAND | wx.LEFT | wx.RIGHT, border=5)\n                                  grid.Add(self.login, pos=(2,0), span=(1,2),\n                                           flag=wx.EXPAND | wx.LEFT | wx.RIGHT, border=5)\n                                  grid.Add(self.error, pos=(3,0), span=(1,2),\n\n                                           flag=wx.EXPAND | wx.LEFT | wx.RIGHT, border=5)\n                                  grid.AddGrowableCol(1)\n\n                                  # Center the grid vertically\n                                  vbox = wx.BoxSizer(wx.VERTICAL)\n\n                                  vbox.Add((0,0), 1)\n                                  vbox.Add(grid, 0, wx.EXPAND)\n                                  vbox.Add((0,0), 2)\n                                  self.SetSizer(vbox)\n\n\n                             def do_login(self, _):\n                                  u = self.userBox.GetValue()\n                                  p = self.passBox.GetValue()\n                                  g = Github(u, p)\n                                  status,data = g.issues.get()\n\n                                  if status != 200:\n                                      self.error.SetLabel('ERROR: ' + data['message'])\n                                  elif callable(self.callback):\n                                      self.callback(u, p)\n\n\n                         There’s some structure that’s similar to above. We’ll start with the construc-\n                      tor.\n\n                         Recall that this panel is created with a keyword argument in thSearch-\n                      Frame  class, likLoginPanel(self, onlogin=self.login_accepted)         . In\n\n\n\n\n       92","initials":"ew"},"\"Explaining all of this seems really heavy handed, annotating a screenshot would be easier?\"":{"page":93,"text":"                                                                                             The Code\n\n\nthe constructor definition, we pull that callback out and store it for later. After-\n\nward, we just call the two other construction functions and return.\n   create_controls    has more to it thanSearchFrame   ’s version, because this\n\npanel has more controls. Every static-text, text-input, and button control gets\nits own line of code. Thewx.TE_PROCESS_ENTER     style tells the library that we\n\nwant an event to be triggered if the user presses the enter key while the cursor\nis inside that text box.\n   The next block binds control events to method calls. Every event in WxPy-\n\nthon will call the handler with a single argument, an object which contains in-\nformation about the event. That means we can use the same event handler for\n\nany number of different kinds of events, so we do – thENTER  handlers for both\ntext boxes and theBUTTON  handler for the button all go throughself.do_log-\n\nin .\n   do_layout   uses adifferent kind of sizer –GridBagSizer   . Again, the topic\n\nof sizers is way outside the scope of this chapter, but just know that this kind\narranges things in a grid, and you can allow some of the rows or columns to\nstretch to fill the container. Here we drop all of the controls into their positions\n\nwith the pos=(r,c)   notation (here “rows” come first, which isn’t like most co-\nordinate systems), and cause one control to span two columns with the    span\n\nparameter. The  flags and  border  parameters mostly mean the same thins as\nbefore, and the AddGrowableCol    function tells the layout engine which parts\n\nof the grid should be allowed to stretch.\n   Then we do something curious: we put the  GridBagSizer    into another sizer.\n\nSizer nesting is a powerful feature, and allows almost any window layout to be\npossible — although perhaps not easy or simple. The vertical box sizer also con-\n\ntains some bare tuples; this special form is called “adding a spacer.” In this case,\nwe sandwich the sizer with all the controls between two spacers with different\nweights, making it float about a third of the way down the window. The effect is\n\nlike Figure 5-3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                           93","initials":"ew"},"\"This all seems really far removed from \\\"Hacking GitHub\\\"\"":{"page":94,"text":"CHAPTER 5: Python and the Search API\n\n\n\n FIGURE 5-3\n\n Resizing behavior of\n login UI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                          Then comes the  do_login   method, which tests out the given credentials,\n\n                       and if they work, passes them back through the callback set at construction\n                       time. If they don’t work, it sets the text of a label, whose foreground color has\n\n                       been set to a nice, alarming shade of red.\n\n\n                       GitHub search\n\n\n                       Once the user has successfully logged in, we destroy thLoginPanel   instance\n\n                       and show the SearchPanel  .\n\n                          class SearchPanel(wx.Panel):\n                              def __init__(self, *args, **kwargs):\n\n                                  self.orgs = kwargs.pop('orgs', [])\n                                  self.credentials = kwargs.pop('credentials', {})\n                                  wx.Panel.__init__(self, *args, **kwargs)\n\n\n                                  self.create_controls()\n                                  self.do_layout()\n\n                              def create_controls(self):\n                                  self.results_panel = None\n\n                                  self.orgChoice = wx.Choice(self, choices=self.orgs, style=wx.CB_SORT)\n                                  self.searchTerm = wx.TextCtrl(self, style=wx.TE_PROCESS_ENTER)\n                                  self.searchTerm.SetFocus()\n                                  self.searchButton = wx.Button(self, label=\"Search\")\n\n\n                                  # Bind events\n                                  self.searchButton.Bind(wx.EVT_BUTTON, self.do_search)\n                                  self.searchTerm.Bind(wx.EVT_TEXT_ENTER, self.do_search)\n\n\n                              def do_layout(self):\n\n\n\n\n       94","initials":"ew"},"\"The variables/parameters could have more descriptive names.\\nWhat exactly is kwargs,?\"":{"page":94,"text":"CHAPTER 5: Python and the Search API\n\n\n\n FIGURE 5-3\n\n Resizing behavior of\n login UI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                          Then comes the  do_login   method, which tests out the given credentials,\n\n                       and if they work, passes them back through the callback set at construction\n                       time. If they don’t work, it sets the text of a label, whose foreground color has\n\n                       been set to a nice, alarming shade of red.\n\n\n                       GitHub search\n\n\n                       Once the user has successfully logged in, we destroy thLoginPanel   instance\n\n                       and show the SearchPanel  .\n\n                          class SearchPanel(wx.Panel):\n                              def __init__(self, *args, **kwargs):\n\n                                  self.orgs = kwargs.pop('orgs', [])\n                                  self.credentials = kwargs.pop('credentials', {})\n                                  wx.Panel.__init__(self, *args, **kwargs)\n\n\n                                  self.create_controls()\n                                  self.do_layout()\n\n                              def create_controls(self):\n                                  self.results_panel = None\n\n                                  self.orgChoice = wx.Choice(self, choices=self.orgs, style=wx.CB_SORT)\n                                  self.searchTerm = wx.TextCtrl(self, style=wx.TE_PROCESS_ENTER)\n                                  self.searchTerm.SetFocus()\n                                  self.searchButton = wx.Button(self, label=\"Search\")\n\n\n                                  # Bind events\n                                  self.searchButton.Bind(wx.EVT_BUTTON, self.do_search)\n                                  self.searchTerm.Bind(wx.EVT_TEXT_ENTER, self.do_search)\n\n\n                              def do_layout(self):\n\n\n\n\n       94","initials":"ew"},"\"Generally speaking, this code is very difficult to understand\"":{"page":94,"text":"CHAPTER 5: Python and the Search API\n\n\n\n FIGURE 5-3\n\n Resizing behavior of\n login UI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                          Then comes the  do_login   method, which tests out the given credentials,\n\n                       and if they work, passes them back through the callback set at construction\n                       time. If they don’t work, it sets the text of a label, whose foreground color has\n\n                       been set to a nice, alarming shade of red.\n\n\n                       GitHub search\n\n\n                       Once the user has successfully logged in, we destroy thLoginPanel   instance\n\n                       and show the SearchPanel  .\n\n                          class SearchPanel(wx.Panel):\n                              def __init__(self, *args, **kwargs):\n\n                                  self.orgs = kwargs.pop('orgs', [])\n                                  self.credentials = kwargs.pop('credentials', {})\n                                  wx.Panel.__init__(self, *args, **kwargs)\n\n\n                                  self.create_controls()\n                                  self.do_layout()\n\n                              def create_controls(self):\n                                  self.results_panel = None\n\n                                  self.orgChoice = wx.Choice(self, choices=self.orgs, style=wx.CB_SORT)\n                                  self.searchTerm = wx.TextCtrl(self, style=wx.TE_PROCESS_ENTER)\n                                  self.searchTerm.SetFocus()\n                                  self.searchButton = wx.Button(self, label=\"Search\")\n\n\n                                  # Bind events\n                                  self.searchButton.Bind(wx.EVT_BUTTON, self.do_search)\n                                  self.searchTerm.Bind(wx.EVT_TEXT_ENTER, self.do_search)\n\n\n                              def do_layout(self):\n\n\n\n\n       94","initials":"ew"},"\"It feels a bit random which lines of code are explained and which are not.\"":{"page":96,"text":"CHAPTER 5: Python and the Search API\n\n\n                          When we add the search bar to the sizer, we use   0 as a scale factor. This\n\n                          means that it shouldn’t exand to fit the available size, but keep its own size\n                          instead, to leave room to add a results panel later on.\n\n\n                          Here’s where the actual search is being done.\n\n\n                          We pass the search results into another class, then add it to the main sizer\n                          with parameters to fill the remaining available space.\n\n\n                          If an error is returned from the search call instead, we display it here. There’s\n                          some code to adjust the wrap width of the text, based on the laid-out width\n\n                          of the control. This isn’t a great approach; doing it better is left as an exercise\n                          for the reader.\n\n                          Again, there’s a fair amount of code here, but most of it should look familiar.\n\n\n                       Displaying results\n\n\n                       So now we have our login panel, and a way for the user to enter a search query,\n\n                       but no way to display results. Let’s fix that.\n                          Whenever search results are retrieved, we create a new instance of Sear-\n\n                       chResultsPanel   , which then creates a series of  SearchResult    instances.\n                       Let’s look at both of them together:\n\n\n                          class SearchResultsPanel(wx.ScrolledWindow):\n                              def __init__(self, *args, **kwargs):\n                                  results = kwargs.pop('results', [])\n                                  wx.PyScrolledWindow.__init__(self, *args, **kwargs)\n\n\n                                  # Layout search result controls inside scrollable area\n                                  vbox = wx.BoxSizer(wx.VERTICAL)\n                                  if not results:\n\n                                       vbox.Add(wx.StaticText(self, label=\"(no results)\"), 0, wx.EXPAND)\n                                  for r in results:\n                                       vbox.Add(SearchResult(self, result=r),\n                                                 flag=wx.TOP | wx.BOTTOM, border=8)\n                                  self.SetSizer(vbox)\n\n                                  self.SetScrollbars(0, 4, 0, 0)\n\n                          class SearchResult(wx.Panel):\n                              def __init__(self, *args, **kwargs):\n                                  self.result = kwargs.pop('result', {})\n\n                                  wx.Panel.__init__(self, *args, **kwargs)\n\n                                  self.create_controls()\n\n\n\n\n       96","initials":"ew"},"\"This is perhaps an example of where extracting out a method can help readability... I.e. a \\\"performSearch()\\\" method\"":{"page":96,"text":"CHAPTER 5: Python and the Search API\n\n\n                          When we add the search bar to the sizer, we use   0 as a scale factor. This\n\n                          means that it shouldn’t exand to fit the available size, but keep its own size\n                          instead, to leave room to add a results panel later on.\n\n\n                          Here’s where the actual search is being done.\n\n\n                          We pass the search results into another class, then add it to the main sizer\n                          with parameters to fill the remaining available space.\n\n\n                          If an error is returned from the search call instead, we display it here. There’s\n                          some code to adjust the wrap width of the text, based on the laid-out width\n\n                          of the control. This isn’t a great approach; doing it better is left as an exercise\n                          for the reader.\n\n                          Again, there’s a fair amount of code here, but most of it should look familiar.\n\n\n                       Displaying results\n\n\n                       So now we have our login panel, and a way for the user to enter a search query,\n\n                       but no way to display results. Let’s fix that.\n                          Whenever search results are retrieved, we create a new instance of Sear-\n\n                       chResultsPanel   , which then creates a series of  SearchResult    instances.\n                       Let’s look at both of them together:\n\n\n                          class SearchResultsPanel(wx.ScrolledWindow):\n                              def __init__(self, *args, **kwargs):\n                                  results = kwargs.pop('results', [])\n                                  wx.PyScrolledWindow.__init__(self, *args, **kwargs)\n\n\n                                  # Layout search result controls inside scrollable area\n                                  vbox = wx.BoxSizer(wx.VERTICAL)\n                                  if not results:\n\n                                       vbox.Add(wx.StaticText(self, label=\"(no results)\"), 0, wx.EXPAND)\n                                  for r in results:\n                                       vbox.Add(SearchResult(self, result=r),\n                                                 flag=wx.TOP | wx.BOTTOM, border=8)\n                                  self.SetSizer(vbox)\n\n                                  self.SetScrollbars(0, 4, 0, 0)\n\n                          class SearchResult(wx.Panel):\n                              def __init__(self, *args, **kwargs):\n                                  self.result = kwargs.pop('result', {})\n\n                                  wx.Panel.__init__(self, *args, **kwargs)\n\n                                  self.create_controls()\n\n\n\n\n       96","initials":"ew"},"\"It is great that this code is annotated but it still feels like a massive wall of text that is very daunting for the reader.\"":{"page":97,"text":"                                                                                      The Code\n\n\n        self.do_layout()\n\n    def create_controls(self):\n        titlestr = self.result['title']\n\n        if self.result['state'] != 'open':\n             titlestr += ' ({})'.format(self.result['state'])\n        textstr = self.first_line(self.result['body'])\n        self.title = wx.StaticText(self, label=titlestr)\n        self.text = wx.StaticText(self, label=textstr)\n\n\n        # Adjust the title font\n        titleFont = wx.Font(16, wx.FONTFAMILY_DEFAULT,\n                              wx.FONTSTYLE_NORMAL, wx.FONTWEIGHT_BOLD)\n\n        self.title.SetFont(titleFont)\n\n        # Bind click and hover events on this whole control\n        self.Bind(wx.EVT_LEFT_UP, self.on_click)\n\n        self.Bind(wx.EVT_ENTER_WINDOW, self.enter)\n        self.Bind(wx.EVT_LEAVE_WINDOW, self.leave)\n\n    def do_layout(self):\n        vbox = wx.BoxSizer(wx.VERTICAL)\n\n        vbox.Add(self.title, flag=wx.EXPAND | wx.BOTTOM, border=2)\n        vbox.Add(self.text, flag=wx.EXPAND)\n        self.SetSizer(vbox)\n\n    def enter(self, _):\n\n        self.title.SetForegroundColour(wx.BLUE)\n        self.text.SetForegroundColour(wx.BLUE)\n\n    def leave(self, _):\n\n        self.title.SetForegroundColour(wx.BLACK)\n        self.text.SetForegroundColour(wx.BLACK)\n\n    def on_click(self, event):\n        import webbrowser\n\n        webbrowser.open(self.result['html_url'])\n\n    def first_line(self, body):\n        return body.split('\\n')[0].strip() or '(no body)'\n\n\n\nThe containing panel is simple enough that it only consists of a constructor.\nThis class’s job is to contain the results, and present them in a scroll window.\n\n\nA SearchResult   comprises two static text controls, which contain the is-\nsue’s title and the first line of its body.\n\n\n\n\n\n\n\n                                                                                    97","initials":"ew"},"\"Justifications like this tend to indicate that things ARE that bad.\"":{"page":98,"text":"CHAPTER 5: Python and the Search API\n\n\n\n                          We’re binding the click handler for this entire panel, but also the mouse-\n                          enter and mouse-leave events, so we can make it behavior more like a link in\n\n                          a browser.\n\n                          Here’s how you open the default browser to a URL in Python.\n\n\n                          Overall, WxPython isn’t so bad, once you get used to it. It lacks some facili-\n                       ties of newer frameworks, but there’s nothing better for getting a basic cross-\n                       platform UI out the door quickly.\n\n                          That’s all of the code! If you’ve been following along, you can run this code\n                       file and do issue searches. However, our use case has a non-technical user run-\n\n                       nnig this; let’s see what can be done to make it easy for them.\n\n\n                       Packaging\n\n\n                       What we’re not going to do is require anyone to install Python 2.7 and a bunch\n\n                       of packages. We’ll use PyInstaller to bundle our application into something\n                       that’s easy to distribute and run.\n                          Let’s assume you wrote all the code above into a file calsearch.py   , and\n\n                       agithub.py   is sitting in the same directory. Here’s how to tell PyInstaller to\n                       generate a single application for you:\n\n\n                          $ pyinstaller -w search.py\n\n\n                          That’s it! Th-w  flag tells PyInstaller to create a “windowed” build of your\n                       application, rather than the default console build. On OS X, this generates a\n\n                       search.app   application bundle, and on Windows this generates asearch.exe\n                       file. You can take either of these to a computer with no Python installed, and\n                       they’ll run perfectly.\n\n                          That’s because PyInstaller has copied everything necessary for your pro-\n                       gram to run, from the Python interpreter on up, inside that file. The one I just\n\n                       generated is 67MB, which seems large for such a simple program, but that num-\n                       ber is more reasonable when you consider what’s inside the package.\n\n\n\n                       Summary\n\n\n                       Whew! This chapter was quite a journey. Let’s take a breath, and look at what\n                       we’ve learned.\n                          The main bulk of the code in this chapter had to do with defining a graphical\n\n                       interface. Code for this task is always pretty verbose, because of the sheer com-\n                       plexity of the task. With WxPython in your tool belt, however, you can now write\n\n\n\n\n       98","initials":"ew"},"\"It begs the question why non technical users would be directly searching through GitHub? Surely they would be searching Google for solutions to their problems? Or they would be asking their technical team about any technical questions they are hoping to a\\nnswer?\"":{"page":98,"text":"CHAPTER 5: Python and the Search API\n\n\n\n                          We’re binding the click handler for this entire panel, but also the mouse-\n                          enter and mouse-leave events, so we can make it behavior more like a link in\n\n                          a browser.\n\n                          Here’s how you open the default browser to a URL in Python.\n\n\n                          Overall, WxPython isn’t so bad, once you get used to it. It lacks some facili-\n                       ties of newer frameworks, but there’s nothing better for getting a basic cross-\n                       platform UI out the door quickly.\n\n                          That’s all of the code! If you’ve been following along, you can run this code\n                       file and do issue searches. However, our use case has a non-technical user run-\n\n                       nnig this; let’s see what can be done to make it easy for them.\n\n\n                       Packaging\n\n\n                       What we’re not going to do is require anyone to install Python 2.7 and a bunch\n\n                       of packages. We’ll use PyInstaller to bundle our application into something\n                       that’s easy to distribute and run.\n                          Let’s assume you wrote all the code above into a file calsearch.py   , and\n\n                       agithub.py   is sitting in the same directory. Here’s how to tell PyInstaller to\n                       generate a single application for you:\n\n\n                          $ pyinstaller -w search.py\n\n\n                          That’s it! Th-w  flag tells PyInstaller to create a “windowed” build of your\n                       application, rather than the default console build. On OS X, this generates a\n\n                       search.app   application bundle, and on Windows this generates asearch.exe\n                       file. You can take either of these to a computer with no Python installed, and\n                       they’ll run perfectly.\n\n                          That’s because PyInstaller has copied everything necessary for your pro-\n                       gram to run, from the Python interpreter on up, inside that file. The one I just\n\n                       generated is 67MB, which seems large for such a simple program, but that num-\n                       ber is more reasonable when you consider what’s inside the package.\n\n\n\n                       Summary\n\n\n                       Whew! This chapter was quite a journey. Let’s take a breath, and look at what\n                       we’ve learned.\n                          The main bulk of the code in this chapter had to do with defining a graphical\n\n                       interface. Code for this task is always pretty verbose, because of the sheer com-\n                       plexity of the task. With WxPython in your tool belt, however, you can now write\n\n\n\n\n       98","initials":"ew"},"\"If you joke about the arduous nature of reading your work, you need to make your work more concise.\"":{"page":98,"text":"CHAPTER 5: Python and the Search API\n\n\n\n                          We’re binding the click handler for this entire panel, but also the mouse-\n                          enter and mouse-leave events, so we can make it behavior more like a link in\n\n                          a browser.\n\n                          Here’s how you open the default browser to a URL in Python.\n\n\n                          Overall, WxPython isn’t so bad, once you get used to it. It lacks some facili-\n                       ties of newer frameworks, but there’s nothing better for getting a basic cross-\n                       platform UI out the door quickly.\n\n                          That’s all of the code! If you’ve been following along, you can run this code\n                       file and do issue searches. However, our use case has a non-technical user run-\n\n                       nnig this; let’s see what can be done to make it easy for them.\n\n\n                       Packaging\n\n\n                       What we’re not going to do is require anyone to install Python 2.7 and a bunch\n\n                       of packages. We’ll use PyInstaller to bundle our application into something\n                       that’s easy to distribute and run.\n                          Let’s assume you wrote all the code above into a file calsearch.py   , and\n\n                       agithub.py   is sitting in the same directory. Here’s how to tell PyInstaller to\n                       generate a single application for you:\n\n\n                          $ pyinstaller -w search.py\n\n\n                          That’s it! Th-w  flag tells PyInstaller to create a “windowed” build of your\n                       application, rather than the default console build. On OS X, this generates a\n\n                       search.app   application bundle, and on Windows this generates asearch.exe\n                       file. You can take either of these to a computer with no Python installed, and\n                       they’ll run perfectly.\n\n                          That’s because PyInstaller has copied everything necessary for your pro-\n                       gram to run, from the Python interpreter on up, inside that file. The one I just\n\n                       generated is 67MB, which seems large for such a simple program, but that num-\n                       ber is more reasonable when you consider what’s inside the package.\n\n\n\n                       Summary\n\n\n                       Whew! This chapter was quite a journey. Let’s take a breath, and look at what\n                       we’ve learned.\n                          The main bulk of the code in this chapter had to do with defining a graphical\n\n                       interface. Code for this task is always pretty verbose, because of the sheer com-\n                       plexity of the task. With WxPython in your tool belt, however, you can now write\n\n\n\n\n       98","initials":"ew"},"\"This line has a few problems...\\n\\n1 - having slogged through Python, you are now telling the reader you want to completely change tact and go into DotNet... This plays back to what we discussed earlier about using too many languages.\\n2 - such a standalo\\nne and cold line right at the end of the chapter is very jarring. Leave out talking about the next chapter, the introduction to the next chapter can cover itself.\"":{"page":99,"text":"                                                                                               Summary\n\n\nGUI applications using Python, with code that’s no harder to write than with\nother toolkits, and get the ability to run on every major platform for free.\n\n   We saw how to ask Git for credentials to a Git server usingit credential    .\nThis feature is quite capable, and includes the ability to write a custom creden-\ntial storage back-end, but we at least saw a peek into how it works. Using this\n\nknowledge, you can piggy-back on your users’ existing habits to avoid having to\nask them for the same things over and over again.\n\n   We also saw a rather nice HTTP API abstraction with   agithub  . We authenti-\ncated and queried the issue-search API endpoint, using what looked like object-\n\nmethod notation.  agithub   is a great example of how a library package can be\nboth future-proof and idiomatic – the library constructs a query URL by looking\n\nat the chain of properties and methods used in the call. This is a great jumping-\noff point for querying other REST APIs using the same pattern.\n   Finally, the main thrust of this chapter was using the GitHub search API.\n\nYou’ve learned about its general behavior, the different categories of search,\nhow to interpret and sort results, and ways of focusing a search to reduce the\n\nnumber of uninteresting results. Using this knowledge you should be able to\nfind anything you’re looking for on GitHub or GitHub Enterprise. You also know\nthat the search UI on GitHub is just a thin layer over the search API, so the same\n\ntricks and techniques will serve you whether you’re writing code or using a\nbrowser.\n\n   In the next chapter we will look at using DotNET with the Commit Status API.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                              99","initials":"ew"},"\"It isn't entirely clear what has passed or failed?\"":{"page":101,"text":" DotNet and the Commit Status\n\n                                                 API       6\n\n\n\n\n\n\n\nAt the risk of oversimplifying things too much, one way to look at a Git reposito-\nry is as just a long series of commits. Each commit contains quite a bit of infor-\nmation: the contents of the source files, who created the commit and when, the\nauthor’s comments on what changes the commit introduces, and so on. This is\nall good stuff, and works very well for Git’s main use case: controlling the histo-\nry of a software project.\n  GitHub’s commit-status API adds another layer of metadata to a commit:\nwhat various services say about that commit. This capability primarily shows\nitself in the pull-request UI, as shown in Figure 6-1. Each commit in the pull re-\nquest is annotated with a symbol indicating its status - a red “×” for failure or\nerror, a green “✓” for success, or an amber “•” to indicate that a decision is in\nthe process of being made. This feature also surfaces at the bottom of the pull-\nrequest; if the last commit in the branch is not marked as successful, you get a\nwarning about merging the request.\n\n\n                                                           FIGURE 6-1\n\n                                                           pull-request UIn the\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                101","initials":"ew"},"\"Good examples\"":{"page":102,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                           The most obvious application for this feature is a continuous-integration ser-\n                        vice. A program like Jenkins will get a notification when new commits are push-\n                        ed to a branch, run a build/test cycle using the new code, and post the results\n\n                        through the commit-status API. An application like this can even include a link\n                        back to the build results, so the user can find out which tests failed. This is a\n\n                        great way to bring together everything needed to make a decision about a pro-\n                        posal: what code has changed, what do people think about it, and does this\n\n                        change break anything? The answer to all of these questions is available on the\n                        same page: the pull-request conversation view.\n                           Building and testing is only the beginning, though; the status of a commit\n\n                        can be used for other purposes as well. For example, open-source projects\n                        often have a license agreement that you must sign in order to submit a contri-\n\n                        bution. These are called “contributor license agreements,” and usually contain\n                        language about licensing the contribution to the maintainers of the project. But\n                        it’s tedious to manually check every incoming pull request to see if the author\n\n                        has signed the CLA, so a continuous-integration-style service can be used for\n                        this. CLAHub is one such example: it checks to see if all of the authors of the\n\n                        contained commits have signed the CLA, and marks the latest commit as “er-\n                        ror” if not.\n\n                           So now we know what the feature is, and what its intended use is. Let’s take\n                        a look at how a program can interact with it.\n\n\n\n                        The API\n\n\n                        First, let’s talk about access control. The commit status API exposes the need\n                        for OAuth as few others do. Making a repository private means you want com-\n                        plete control of what people or applications can access it. Naturally you trust\n\n                        GitHub’s internal code to do the right thing with your data, but what about\n                        some random application from the Internet? OAuth gives you a way to grant\n\n                        private-repository access to an application with limits – the use of OAuth scopes\n                        allows an application to ask for a specific set of permissions, but it won’t be\n\n                        able to do just any old thing with your data. Plus, these you’re always in control\n                        of these permissions; you can revoke an application’s access at any time.\n                           The OAuth system includes the concept of scopes, which can be requested\n\n                        by and granted to an application, each of which allows a certain set of actions.\n                        The commit-status API requires the   repo:status    OAuth scope, which allows\n\n                        an application read and write access to just commit statuses; there is no access\n                        granted to the actual contents of the repository. This might seem strange: how\n\n                        can you judge the status of a commit without being able to inspect its contents?\n                        Just remember that this feature has use cases beyond continuous integration,\n                        and an application may not need full access to make a decision. For services\n\n\n\n\n\n        102","initials":"ew"},"\"oauth content is repeated from earlier in the book\"":{"page":102,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                           The most obvious application for this feature is a continuous-integration ser-\n                        vice. A program like Jenkins will get a notification when new commits are push-\n                        ed to a branch, run a build/test cycle using the new code, and post the results\n\n                        through the commit-status API. An application like this can even include a link\n                        back to the build results, so the user can find out which tests failed. This is a\n\n                        great way to bring together everything needed to make a decision about a pro-\n                        posal: what code has changed, what do people think about it, and does this\n\n                        change break anything? The answer to all of these questions is available on the\n                        same page: the pull-request conversation view.\n                           Building and testing is only the beginning, though; the status of a commit\n\n                        can be used for other purposes as well. For example, open-source projects\n                        often have a license agreement that you must sign in order to submit a contri-\n\n                        bution. These are called “contributor license agreements,” and usually contain\n                        language about licensing the contribution to the maintainers of the project. But\n                        it’s tedious to manually check every incoming pull request to see if the author\n\n                        has signed the CLA, so a continuous-integration-style service can be used for\n                        this. CLAHub is one such example: it checks to see if all of the authors of the\n\n                        contained commits have signed the CLA, and marks the latest commit as “er-\n                        ror” if not.\n\n                           So now we know what the feature is, and what its intended use is. Let’s take\n                        a look at how a program can interact with it.\n\n\n\n                        The API\n\n\n                        First, let’s talk about access control. The commit status API exposes the need\n                        for OAuth as few others do. Making a repository private means you want com-\n                        plete control of what people or applications can access it. Naturally you trust\n\n                        GitHub’s internal code to do the right thing with your data, but what about\n                        some random application from the Internet? OAuth gives you a way to grant\n\n                        private-repository access to an application with limits – the use of OAuth scopes\n                        allows an application to ask for a specific set of permissions, but it won’t be\n\n                        able to do just any old thing with your data. Plus, these you’re always in control\n                        of these permissions; you can revoke an application’s access at any time.\n                           The OAuth system includes the concept of scopes, which can be requested\n\n                        by and granted to an application, each of which allows a certain set of actions.\n                        The commit-status API requires the   repo:status    OAuth scope, which allows\n\n                        an application read and write access to just commit statuses; there is no access\n                        granted to the actual contents of the repository. This might seem strange: how\n\n                        can you judge the status of a commit without being able to inspect its contents?\n                        Just remember that this feature has use cases beyond continuous integration,\n                        and an application may not need full access to make a decision. For services\n\n\n\n\n\n        102","initials":"ew"},"\"General minor gripe: the layout of the pages don't line up vertically. When you scroll down the book, you keep needing to adjust the view left and right.\\nI know this is done to make the book look better in print, across two pages. Nevertheless it is aggr\\navating.\"":{"page":103,"text":"                                                                                             The API\n\n\nthat do need to be able to look at the repository contents, you can request the\n\nrepo  scope, which grants read and write access to the entire contents of a\nrepository, including commit statuses. As of this writing, there’s no way to re-\n\nquest read-only access to repositories, so if a service needs access to your data,\nyou have to trust it with write access also.\n\n\nRaw statuses\n\n\nNow that we know how we get access to commit statuses, let’s see what they\n\nlook like. Commit statuses exist as atomic entities, and each commit can have a\npractically unlimited number of them (the actual number is in the thousands).\nYou can query for existing statuses by doing a GET request to the API serve/ at\n\nrepos/<user>/<repo>/<ref>/statuses        , and it will return a list of them that\nlooks like this:\n\n\n   [\n     {\n       \"url\": \"https://api.github.com/repos/…\",\n\n       \"id\": 224503786,\n       \"state\": \"success\",\n       \"description\": \"The Travis CI build passed\",\n       \"target_url\": \"https://travis-ci.org/libgit2/libgit2/builds/63428108\",\n       \"context\": \"continuous-integration/travis-ci/push\",\n\n       \"created_at\": \"2015-05-21T03:11:02Z\",\n       \"updated_at\": \"2015-05-21T03:11:02Z\"\n     },\n     …\n   ]\n\n\n   Most of this is self-explanatory, but a couple of fields need explaining. The\nstate  field can be “success,” “failure,” “error,” or “pending,” depending on the\n\nstate of the service’s decision. Thtarget_url   is a URL for the specific deci-\nsion made for this commit (in this case a build/test log), which helps the user\n\nfigure out why a particular decision was reached. And thecontext   parameter\nis used for correlating multiple status updates to a single service; each applica-\n\ntion sets this according to its own rules, but any process that creates statuses\nshould post the pending   status and the result status using the same context\nvalue.\n\n   This API is useful for getting the raw data involved, but it gets complicated\nquickly. How do you decide if a given commit is “good?” What if there are 3\n\npending statuses, one success, another pending, two failures, and another suc-\ncess, in that order? Thecontext  field can help you correlate a single service’s\n\n\n\n\n\n\n                                                                                         103","initials":"ew"},"\"This flows quite nicely\"":{"page":104,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                       updates, and you can order them by  created_at   to see how each one turned\n\n                       out, but that’s a lot of work. Fortunately, the API server can do it for you.\n\n\n                       Combined status\n\n\n                       If you instead do a GET t/repos/<user>/<repo>/<ref>/status        (note that\n                       the last word is singular), you’ll instead get a response that looks like this:\n\n\n                          {\n                            \"state\": \"success\",\n                            \"statuses\": [\n\n                              {\n                                \"url\": \"https://api.github.com/repos/…\",\n                                …\n                              },\n                              { … }\n\n                            ],\n                            \"sha\": \"6675aaba883952a1c1b28390866301ee5c281d37\",\n                            \"total_count\": 2,\n                            \"repository\": { … },\n                            \"commit_url\": \"https://api.github.com/repos/…\",\n\n                            \"url\": \"https://api.github.com/repos/…\"\n                          }\n\n                          The statuses  array is the result of the logic you’d probably write if you had\n\n                       to: it collapses the statuses by context, keeping only the last one. Tstate\n                       field contains an overall status that takes into account all of the contexts, pro-\n\n                       viding a final value based on these rules:\n\n                           •failure  if any of the contexts postedfailure   orerror  state\n\n                           •pending   if any of the contexts’ latest statpending  (or if there are no\n                            statuses)\n\n                           •success  if the latest status for every contesuccess\n\n                          This is probably exactly what you want, but if you find that your use case\n\n                       calls for different rules, you can always use statuses   endpoint to get the\n                       raw data and calculate your own combined status.\n\n\n                       Creating a status\n\n\n                       Now obviously these statuses have to come from somewhere. This API also in-\n\n                       cludes a facility for creating them. To do this, you simply make a POST request\n                       to/repos/<user>/<repo>/statuses/<sha>        , and supply a JSON object for\n                       the fields you want to include with your status:\n\n\n\n\n       104","initials":"ew"},"\"This feels a lot more like Hacking GitHub. This is the sort of level you would expect.\"":{"page":105,"text":"                                                                                          Let’s write an app\n\n\n    • state   is required, and must be one of    pending  , success  , error , or\n\n      failure  .\n\n    • target_url    is a link to detailed information on the process of deciding\n      what the state is or will be.\n\n    • description    is a short string describing what the service is doing to\n      make a decision.\n\n    • context   is an application-specific string to allow the API to manage mul-\n\n      tiple services contributing to a single commit’s status.\n\n   Notice how the last component in that URL is  <sha>  . While you can query for\nstatuses or a combined status using a ref name (like  master  ), creating a status\n\nrequires you to know the full SHA-1 hash of the commit you want to annotate.\nThis is to avoid race conditions: if you were targeting a ref, it may have moved\n\nbetween when your process started and when it finishes, but the SHA of a com-\nmit will never change.\n\n\n\nLet’s write an app\n\n\nAlright, now that we know how to read and write statuses, let’s put this API to\nwork. In this chapter, we’ll build a simple HTTP service that lets you create com-\nmit statuses for repositories you have access to, using the OAuth web flow for\n\nauthorization. The system we’ll build will be fairly limited in scope, but it’s a\ngreat starting point to customize for your specific needs.\n\n   The language this time is C#, running on the CLR (Common Language Run-\ntime). At one point in the history of computing this wouldn’t have been a good\nchoice for a book like this, since it was only available on Windows, the develop-\n\nment tools cost quite a bit of money, and the language and libraries were fairly\nlimited. However, with the advent of Mono (an open-source implementation of\n\nthe .NET runtime), the open-sourcing of the CLR core, and the availability of\nfree tools, C# is now a completely valid and rather nice option for open-source\n\nor hobby developers. Plus, it has a vibrant ecosystem of packages we can lever-\nage to make our jobs easier.\n\n\nLibraries\n\n\nYou’ll be happy to know that we won’t be writing an entire HTTP server from\nscratch in this chapter. There are a number of open-source packages that do\n\nthis work for us, and in this project we’ll be using Nancy. Nancy is a project that\nstarted as a CLR port of the Sinatra framework for Ruby (it takes its name from\n\n\n\n\n\n\n                                                                                              105","initials":"ew"},"\"Whilst this is a perfectly valid justification of why C# is a useful tool, it still feels a bit at odds with how this book started.\\nI.e. there was no mention of C# and having so many different languages is a bit distracting\"":{"page":105,"text":"                                                                                          Let’s write an app\n\n\n    • state   is required, and must be one of    pending  , success  , error , or\n\n      failure  .\n\n    • target_url    is a link to detailed information on the process of deciding\n      what the state is or will be.\n\n    • description    is a short string describing what the service is doing to\n      make a decision.\n\n    • context   is an application-specific string to allow the API to manage mul-\n\n      tiple services contributing to a single commit’s status.\n\n   Notice how the last component in that URL is  <sha>  . While you can query for\nstatuses or a combined status using a ref name (like  master  ), creating a status\n\nrequires you to know the full SHA-1 hash of the commit you want to annotate.\nThis is to avoid race conditions: if you were targeting a ref, it may have moved\n\nbetween when your process started and when it finishes, but the SHA of a com-\nmit will never change.\n\n\n\nLet’s write an app\n\n\nAlright, now that we know how to read and write statuses, let’s put this API to\nwork. In this chapter, we’ll build a simple HTTP service that lets you create com-\nmit statuses for repositories you have access to, using the OAuth web flow for\n\nauthorization. The system we’ll build will be fairly limited in scope, but it’s a\ngreat starting point to customize for your specific needs.\n\n   The language this time is C#, running on the CLR (Common Language Run-\ntime). At one point in the history of computing this wouldn’t have been a good\nchoice for a book like this, since it was only available on Windows, the develop-\n\nment tools cost quite a bit of money, and the language and libraries were fairly\nlimited. However, with the advent of Mono (an open-source implementation of\n\nthe .NET runtime), the open-sourcing of the CLR core, and the availability of\nfree tools, C# is now a completely valid and rather nice option for open-source\n\nor hobby developers. Plus, it has a vibrant ecosystem of packages we can lever-\nage to make our jobs easier.\n\n\nLibraries\n\n\nYou’ll be happy to know that we won’t be writing an entire HTTP server from\nscratch in this chapter. There are a number of open-source packages that do\n\nthis work for us, and in this project we’ll be using Nancy. Nancy is a project that\nstarted as a CLR port of the Sinatra framework for Ruby (it takes its name from\n\n\n\n\n\n\n                                                                                              105","initials":"ew"},"\"Good to have some consistency with the use of Sinatra/Nancy\"":{"page":106,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                        Frank Sinatra’s daughter, Nancy). It’s very capable, but also very succinct, as\n                        you’ll see.\n                           We also won’t be directly implementing access to the GitHub API, because\n\n                        GitHub provides a CLR library for that. It’s called octokit.net, and it does all the\n                        right things with regard to asynchrony and type safety. This is the same library\n\n                        used by the GitHub client for Windows, so it’ll definitely do the job for our little\n                        application. It is, however, the source of a constraint on how we set up our ex-\n\n                        ample project: it requires a rather new version of the CLR (4.5) in order to func-\n                        tion. If you want some guidance on how to avoid this pitfall and follow along,\n                        continue reading the next section. If you’ve worked with Nancy before, and\n\n                        have installed NuGet packages in the past, you might be able to skip to the sec-\n                        tion labeled “First steps”.\n\n\n\n                        Following along\n\n                        If you’d like to follow along with the code examples, here’s how to set up a de-\n\n                        velopment environment with all the necessary elements. The process is differ-\n                        ent on Windows (using Visual Studio) and any other platforms (using Xamarin\n\n                        tools).\n\n\n                        VISUAL STUDIO\n\n                        If you’re running Windows, you’ll want to visit https://www.visualstudio.com/\n\n                        and download the Community edition of Visual Studio. The installer will\n                        present you with lots of options; for this example, we’ll only need the “web de-\n                        veloper” components, but feel free to check all the boxes that look interesting\n\n                        to you. (If you have access to a higher tier of Visual Studio, or already have it\n                        installed with the web-development packages, you’re all set.)\n\n                           In order to make things just a little smoother, you’ll want to install a plugin:\n                        the Nancy project templates. Visit https://visualstudiogallery.msdn.micro-\n\n                        soft.com/ and search for “nancy.templates”. As of this writing, there appears to\n                        be some difficulty with file formats, so when you download it, it comes as a ZIP\n                        file. If this has been resolved by the time you’re reading this, simply double-\n\n                        click the file to install the templates; if not, you’ll have to rename it to have\n                        a .vsix  extension first.\n\n                           The next step is to create a new project using one of the newly-installed tem-\n                        plates. Go to “File>New Project…” and select “Visual C#>Web>Nancy Applica-\n\n                        tion with ASP.NET Hosting” from the template list (as shown in Figure 6-2.\n                        Make sure the path and name settings at the bottom are to your liking, and\n                        click OK.\n\n\n\n\n\n\n\n        106","initials":"ew"},"\"All this setup is very dry... not much you can do with instructions I suppose though\"":{"page":108,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                        this is a newer version of MonoDevelop with more capabilities, and will work\n\n                        just as well for these examples.\n                           There are no Nancy-specific project templates for these IDEs, so you’ll just\n\n                        start with an empty web project. Go to “File>New>Solution…”, and choose\n                        “ASP.NET>Empty ASP.NET Project” from the template chooser, as shown in\n\n                        Figure 6-3.\n\n\n\n FIGURE 6-3\n\n Creating an empty\n\n ASP.NET application\n in Xamarin Studio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           The rest of the wizard steps are about the project name and location; feel\n                        free to name and locate this project however you like.\n\n                           Next, update the target framework setting. Control- or right-click on the\n                        node in the solution explorer that corresponds with your project (not your solu-\n\n                        tion), and select “Options” from the menu. Under “Build>General,” set the Tar-\n                        get Framework to “Mono / .NET 4.5” (or later) and click OK.\n\n                           Lastly, install the Nancy and Octokit NuGet packages. Go to “Project>Add\n                        NuGet Packages…” in the menu to open the package manager. Search for Nan-\n\n                        cy, check the box next to it, search for Octokit, check its box, and click “Add\n                        Packages” at the bottom right. Once the process is complete, your project is\n\n                        ready for our example code. To run it under the debugger, go to “Run>Start De-\n                        bugging…,” or type ⌘-Enter. Xamarin will start the server and open a browser\n\n                        window to http://127.0.0.1:80080 (possibly with a different port), which at this\n                        point will just show the default “404 Not Found” page.\n\n\n\n\n        108","initials":"ew"},"\"Line 4 in the code and the explanation feel a little complex\"":{"page":109,"text":"                                                                                  Let’s write an app\n\n\nFirst steps\n\n\nAlright, now that we have a project ready for some code, let’s get our Nancy ap-\nplication up and running. Here’s what it looks like to do perform a simple re-\n\nquest using Nancy and Octokit.\n\n\n   using Nancy;\n   using Octokit;\n   using System;\n   using System.Collections.Generic;\n   using System.Linq;\n\n\n   namespace NancyApp\n   {\n       public class Handler : NancyModule\n       {\n           private readonly GitHubClient client =\n\n                new GitHubClient(new ProductHeaderValue(\"MyHello\"));\n\n           public Handler()\n           {\n\n                Get[\"/{user}\", true] = async (parms, ct) =>\n                    {\n                        var user = await client.User.Get(parms.user.ToString());\n                        return String.Format(\"{0} people love {1}!\",\n                                              user.Followers, user.Name);\n\n                    };\n           }\n       }\n   }\n\n\n   Here we derive a class from NancyModule  , which is all you have to do to\n\n   start receiving and processing HTTP requests in Nancy.\n\n\n   The GitHubClient   class is the entry point for Octokit. Here we create an in-\n   stance which we’ll use later on, using a placeholder product name – this\n   name will not be used for the APIs we’ll be accessing.\n\n\n   The module’s constructor needs to set up route mappings. We map/{user}\n\n   to a lambda function using theGet dictionary that comes withNancyMod-\n   ule. The second parameter to the index operator says that the handler will\n\n   be asynchronous.\n\n   Here we see how to get the {user}  part of the request URL (it comes as a\n\n   property on the parms parameter), and how to query the GitHub User API\n\n\n\n\n                                                                                      109","initials":"ew"},"\"Is the added complexity here really worth it when we are trying to learn about GitHub rather than the intricacies of C#?\"":{"page":110,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                          using Octokit. Note that we have to  await  the result of the network query,\n                          since it may take some time.\n\n\n                          Nancy request handlers can simply return a text string, which will be marked\n                          as HTML for the viewing browser. Here we return a simple string with the\n\n                          user’s name and number of followers.\n\n                       EXAMPLE 6-1.\n\n                          The  async  and  await  keywords bear special mention. These comprise a\n\n                       syntactic nicety that encapsulates a series of functions that are running on an\n                       event loop. The code looks like it runs in order, but really when thawait key-\n                       word is reached, the system starts an asynchronous request, and returns con-\n\n                       trol back to the main event loop. Once the request has finished, and the\n                       promise is fulfilled, the event loop will then call back into the code that’s ex-\n\n                       pecting the return value of theawait  keyword, with all the scope variables in-\n                       tact. This feature was introduced in .NET 4.0 (which was released in 2012), and\n\n                       it lets you write asynchronous code almost as though it were synchronous. This\n                       is but one of the features that make C# the favorite of many developers.\n\n                          This example is a bit more complicated than “hello, world,” but it’s still fairly\n                       succinct and clear. This bodes well, because we’re about to introduce some\n                       complexity, in the form of OAuth.\n\n\n\n                       OAuth flow\n\n\n                       In order to post a status update for a commit, we’re going to have to ask the\n                       user for permission. Apart from asking for their username and password (which\n                       gives way too much control, and if two-factor authentication is enabled may\n\n                       not even be enough), the only way to do this is OAuth, which isn’t entirely\n                       straightforward.\n\n                          Here’s a simple outline of the OAuth process, from our little server’s point of\n                       view:\n\n                          1. We need an authorization token, either because we don’t have one, or be-\n\n                             cause the one we have is expired. This is just a string of characters, but we\n                             can’t generate it ourselves, so we ask GitHub for one. This involves redi-\n                             recting the user’s browser to a GitHub API endpoint, with the kind of per-\n\n                             mission we’re asking for and some other details as query parameters.\n\n                          2. GitHub tells the user (through their browser) that an application is re-\n                             questing some permissions, and they can either allow or deny them.\n\n\n\n\n\n\n\n\n        110","initials":"ew"},"\"The oauth parts need to be merged together and consolidated with the parts in the rest of the book\"":{"page":111,"text":"                                                                                       Let’s write an app\n\n\n   3. If the user allows this access, their browser is redirected to a URL we\n      specified in step 1. A “code” is passed as a query parameter; this is not the\n\n      access token we want, but a time-limited key to get one.\n   4. From inside the handler for this request, we can use a REST API to get the\n\n      actual OAuth access token, which we can store somewhere safe. We do\n      this because if we already have a token, we can skip all the way to the last\n\n      step of this process.\n   5. Now we have permission, and we can use the GitHub API in authenticated\n\n      mode.\n\n   This might seem overly complicated, but its design achieves several goals.\n\nFirst, permission can be scoped – an application is almost never given full ac-\ncess to the user’s account and data. Second, the whole exchange is secure; at\nleast one part of this has to go through the user, and cannot be automated.\n\nThird, the access token is never transmitted to the user’s browser, which avoids\nan entire class of security vulnerabilities.\n\n   Let’s walk through the code for our tiny little server’s implementation of this\nflow. First, once we have a token, we should store it so we’re not going through\n\nthe entire redirect cycle for every user request. We’re going to store it in a cook-\nie (though since this goes back and forth to the user’s browser, a production ap-\n\nplication would probably use a database). Nancy can help us with this, but first\nwe have to enable it, and the way this is accomplished is by using a bootstrap-\nper. We’re going to add this class to our application:\n\n\n   using Nancy;\n   using Nancy.Bootstrapper;\n\n   using Nancy.Session;\n   using Nancy.TinyIoc;\n\n   namespace NancyApp\n   {\n\n       public class Bootstrapper : DefaultNancyBootstrapper\n       {\n            protected override void ApplicationStartup(TinyIoCContainer container,\n                                                            IPipelines pipelines)\n\n            {\n                CookieBasedSessions.Enable(pipelines);\n            }\n       }\n   }\n\n\n   Nancy will automatically detect a bootstrapper class, and use it to initialize\n\nour server. Now, from within aNancyModule   , we can use theSession   property\nto store and retrieve values that are transmitted as cookies.\n\n\n\n\n\n                                                                                           111","initials":"ew"},"\"It really feels like covering oauth in multiple different languages is repetitive and possible a waste of time\"":{"page":112,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                          Next, we have to include our application’s ID and secret in some of the re-\n                       quests, so we embed them in the code by adding these fields to the   Handler\n\n                       class. If you don’t have an application, visit https://github.com/settings/devel-\n                       opers to create one and use  http://localhost:8080/authorize         (depend-\n                       ing in your environment, the port number might be slightly different) for the\n\n                       callback URL – we’ll see why in a bit.\n\n\n                                   private const string clientId = \"<clientId>\";\n                                   private const string clientSecret = \"<clientSecret>\";\n\n                          Obviously, you should use values from your own API application if you’re fol-\n\n                       lowing along.\n                          After that, we’ll need a helper method that kicks off the OAuth process:\n\n\n                          private Response RedirectToOAuth()\n                          {\n                               var csrf = Guid.NewGuid().ToString();\n\n                               Session[\"CSRF:State\"] = csrf;\n                               Session[\"OrigUrl\"] = this.Request.Path;\n\n                               var request = new OauthLoginRequest(clientId)\n                                   {\n\n                                       Scopes = { \"repo:status\" },\n                                       State = csrf,\n                                   };\n                               var oauthLoginUrl = client.Oauth.GetGitHubLoginUrl(request);\n\n                               return Response.AsRedirect(oauthLoginUrl.ToString());\n                          }\n\n\n                          CSRF stands for “cross-site request forgery.” This is a mechanism by which\n                          we can be sure the OAuth request process really did originate from our site.\n\n                          The GitHub OAuth API will pass this value back to us when the user author-\n                          izes access, so we store it in the cookie for later reference.\n\n\n                          Storing the original URL in the session cookie is a UX feature; once the OAuth\n                          process has completed, we want to send the user back to what they were\n\n                          trying to do in the first place.\n\n\n                          repo:status   is the permission set we’re asking for. Note that we’re also in-\n                          cluding our CSRF token in this object; this is so GitHub can give it back to us\n                          later for verification.\n\n\n                          Here we use Octokit to generate the redirect URL, and send the user’s brows-\n\n                          er there.\n\n\n\n\n        112","initials":"ew"},"\"At this point you have thrown a lot of code at the reader and it is somewhat overwhelming\"":{"page":114,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                         Here we redirect the user back to what they were originally trying to do, with\n\n                         as little disruption as possible.\n\n                         Once all that is done, we’ve got our token and are able to continue on our\n                      merry way. All our handlers have to do to trigger an OAuth sequence is to call\n\n                      RedirectToOAuth()     if it’s necessary, and we’ll automatically return the user\n                      to where they were when the process completes.\n\n\n                      Status handler\n\n\n                      Having gone through all that OAuth business, we should now have a token that\n\n                      grants us permission to create commit statuses, so let’s see what it takes to do\n                      that. We’re going to add this handler to our Nancy module constructor:\n\n\n                         Get[\"/{user}/{repo}/{sha}/{status}\", true] = async (parms, ct) =>\n                             {\n                                  var accessToken = Session[\"accessToken\"] as string;\n                                  if (string.IsNullOrEmpty(accessToken))\n\n                                      return RedirectToOAuth();\n                                  client.Credentials = new Credentials(accessToken);\n\n                                  CommitState newState = Enum.Parse(typeof(CommitState),\n\n                                                                      parms.status,\n                                                                      true);\n                                  try\n                                  {\n                                      var newStatus = new NewCommitStatus\n\n                                      {\n                                          State = newState,\n                                          Context = \"example-api-app\",\n                                          TargetUrl = new Uri(Request.Url.SiteBase),\n                                      };\n\n                                      await client.Repository.CommitStatus.Create(parms.user,\n                                                                                     parms.repo,\n                                                                                     parms.sha,\n                                                                                     newStatus);\n                                  }\n\n                                  catch (NotFoundException)\n                                  {\n                                      return HttpStatusCode.NotFound;\n                                  }\n\n\n                                  var template = @\"Done! Go to <a href=\"\"https://\"\n                                  + @\"api.github.com/repos/{0}/{1}/commits/{2}/status\"\n                                  + @\"\"\">this API endpiont</a>\";\n                                  return String.Format(template,\n\n\n\n\n\n       114","initials":"ew"},"\"It feels like a missed opportunity for us to have walked through a prepared example that allowed the reader to mess around with the commit status API without having to worry about all the extra boilerplate.\"":{"page":116,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                        Summary\n\n\n                        Even if you haven’t written a lot of code during this chapter, you’ve learned a lot\n                        of concepts.\n\n                           You’ve seen the commit status API, and you’ve seen how it’s used by contin-\n                        uous integration software, but you know that it can be used for much more. You\n\n                        can read and write statuses, and you know how the API server coalesces many\n                        statuses into a single pass/fail value, and you also know how to write your own\n                        multi-status calculation if the default one doesn’t meet your needs. You also\n\n                        know what’s behind the green checkmarks and red X’s you see in your pull re-\n                        quests.\n\n                           You’ve learned how the OAuth web flow works, and why it’s designed the\n                        way it is. OAuth is the key to many other capabilities of the GitHub API, and it’s\n                        the right thing to do with regards to trust and permissions. This will allow you\n\n                        to write truly world-class GitHub-interfacing applications, whether running on\n                        the web or on a user’s device.\n\n                           You’ve gained a passing knowledge of C#, including its package system, at\n                        least one IDE, lambda functions, object initializers, and more. C# really is a nice\n\n                        language, and if you use it for a while, you’ll probably miss some of its features\n                        if you write in anything else.\n                           You’ve seen NuGet, the .NET package manager, and had a peek at the multi-\n\n                        tudes of packages in this ecosystem. The capability you have here is astound-\n                        ing; libraries exist for many common activities, and lots of uncommon ones too,\n\n                        so no matter what you need to do, you’re likely to find a NuGet package to help\n                        you do it.\n\n                           You’ve learned about Nancy, with which you can quickly build any HTTP ser-\n                        vice, from a REST API to an HTML-based interface, and all with a compact syn-\n                        tax and intuitive object model. If you’ve never been exposed to the Sinatra view\n\n                        of the world, this probably makes you think about web servers a bit differently,\n                        and if you have, you’ll have a new appreciation for how this model can be idio-\n\n                        matically implemented.\n                           And you’ve had an introduction to Octokit, a type-safe implementation of a\n                        REST API, with built-in asynchrony and OAuth helpers. This toolkit really does\n\n                        make working with the GitHub API as simple and straightforward as using\n                        any .NET library, including the ability to explore it using Intellisense.\n\n                           In the next chapter we will look at using Ruby to create and build Jekyll\n                        blogs.\n\n\n\n\n\n\n\n\n\n\n\n        116","initials":"ew"},"\"This sort of conclusion is worrying as it almost implies the author is trying to sell C# to the reader. None of this is relevant to GitHub, let alone the commit status API that this chapter is supposed to be about.\"":{"page":116,"text":"CHAPTER 6: DotNet and the Commit Status API\n\n\n                        Summary\n\n\n                        Even if you haven’t written a lot of code during this chapter, you’ve learned a lot\n                        of concepts.\n\n                           You’ve seen the commit status API, and you’ve seen how it’s used by contin-\n                        uous integration software, but you know that it can be used for much more. You\n\n                        can read and write statuses, and you know how the API server coalesces many\n                        statuses into a single pass/fail value, and you also know how to write your own\n                        multi-status calculation if the default one doesn’t meet your needs. You also\n\n                        know what’s behind the green checkmarks and red X’s you see in your pull re-\n                        quests.\n\n                           You’ve learned how the OAuth web flow works, and why it’s designed the\n                        way it is. OAuth is the key to many other capabilities of the GitHub API, and it’s\n                        the right thing to do with regards to trust and permissions. This will allow you\n\n                        to write truly world-class GitHub-interfacing applications, whether running on\n                        the web or on a user’s device.\n\n                           You’ve gained a passing knowledge of C#, including its package system, at\n                        least one IDE, lambda functions, object initializers, and more. C# really is a nice\n\n                        language, and if you use it for a while, you’ll probably miss some of its features\n                        if you write in anything else.\n                           You’ve seen NuGet, the .NET package manager, and had a peek at the multi-\n\n                        tudes of packages in this ecosystem. The capability you have here is astound-\n                        ing; libraries exist for many common activities, and lots of uncommon ones too,\n\n                        so no matter what you need to do, you’re likely to find a NuGet package to help\n                        you do it.\n\n                           You’ve learned about Nancy, with which you can quickly build any HTTP ser-\n                        vice, from a REST API to an HTML-based interface, and all with a compact syn-\n                        tax and intuitive object model. If you’ve never been exposed to the Sinatra view\n\n                        of the world, this probably makes you think about web servers a bit differently,\n                        and if you have, you’ll have a new appreciation for how this model can be idio-\n\n                        matically implemented.\n                           And you’ve had an introduction to Octokit, a type-safe implementation of a\n                        REST API, with built-in asynchrony and OAuth helpers. This toolkit really does\n\n                        make working with the GitHub API as simple and straightforward as using\n                        any .NET library, including the ability to explore it using Intellisense.\n\n                           In the next chapter we will look at using Ruby to create and build Jekyll\n                        blogs.\n\n\n\n\n\n\n\n\n\n\n\n        116","initials":"ew"},"\"These statements are all very general and very opinionated.\"":{"page":118,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n                        that recognizes we only need to be dynamic when new content is created and\n                        not when a new visitor requests the same page as the previous visitor. This is\n                        unfortunately how many sites work right now, resulting in the same page being\n\n                        regenerated over and over, and has led to overwrought solutions like page\n                        caching which has layered more lipstick on the pig. By acknowledging these\n\n                        facts, there is a massive reduction in complexity of Jekyll sites and with it, im-\n                        mense freedom. It may feel like you have traveled back in time, because when\n\n                        you look at a site built by Jekyll, you are looking at a set of static files, backed\n                        by nothing more complex than the same web server technologies available be-\n                        fore the dot com bust of 2000.\n\n\n\n                        The Harmless Brew that Spawned Jekyll\n\n\n                        Though you won’t see it in the documentation, the popularity of Jekyll is due in\n                        large part to the desire of bloggers, especially very technical ones, wanting a\n\n                        blogging tool option other than Wordpress. For many bloggers, Wordpress is a\n                        poor choice of blogging tools. Of course, there is always language snobbery in\n\n                        any programming community, but Wordpress is built on a language, PHP, which\n                        is widely reviled by developers from various language communities. Because\n\n                        PHP is more accessible language than other languages, the choice of PHP has\n                        fostered a large community of plugin developers. Unfortunately, this communi-\n                        ty is fractured, and the plugins which arise from it are at best poorly document-\n\n                        ed and poorly integrated, and at worst, poorly designed and buggy. While\n                        Wordpress does often have a plugin for anything you need, the problem it sol-\n\n                        ves often creates more problems in the long term when scalability or database\n                        optimization or security, for example, become concerns.\n                           Jekyll is interesting because of the components which are not present when\n\n                        compared against Wordpress. Jekyll does not require a database. Jekyll does\n                        not require you know how to write in HTML. While Wordpress ostensibly adver-\n\n                        tises itself as a tool which can be used without knowledge of these technolo-\n                        gies, talk to those of us who have struggled with recovery of a mangled data-\n\n                        base after installation of a new Wordpress plugin, or resolving scalability issues\n                        on a large Wordpress site, or analyzing and fixing broken HTML produced by the\n                        Wordpress editor. We’ll tell you that, when using Wordpress, you don’t need to\n\n                        know about MySQL or HTML at all, because it is already too late by then to fix\n                        whatever problem you are facing.\n\n                           Jekyll responds to these concerns in a really elegant way. Instead of author-\n                        ing in HTML, you author in a simple and readable language called Markdown,\n\n                        which the Jekyll engine converts automatically to HTML for you. Instead of stor-\n                        ing posts and other data inside a MySQL database, you use the filesystem to\n                        store posts and layouts and then regenerate the entire site when infrequent\n\n\n\n\n\n        118","initials":"ew"},"\"I would avoid having so many inflammatory opinions and simply focus on what you are trying to teach the reader.\\nWhether or not X/Y is a good technology choice is neither here nor there.\\nThe aggressive/informal language is slightly inappropriate as well.\\n\"":{"page":118,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n                        that recognizes we only need to be dynamic when new content is created and\n                        not when a new visitor requests the same page as the previous visitor. This is\n                        unfortunately how many sites work right now, resulting in the same page being\n\n                        regenerated over and over, and has led to overwrought solutions like page\n                        caching which has layered more lipstick on the pig. By acknowledging these\n\n                        facts, there is a massive reduction in complexity of Jekyll sites and with it, im-\n                        mense freedom. It may feel like you have traveled back in time, because when\n\n                        you look at a site built by Jekyll, you are looking at a set of static files, backed\n                        by nothing more complex than the same web server technologies available be-\n                        fore the dot com bust of 2000.\n\n\n\n                        The Harmless Brew that Spawned Jekyll\n\n\n                        Though you won’t see it in the documentation, the popularity of Jekyll is due in\n                        large part to the desire of bloggers, especially very technical ones, wanting a\n\n                        blogging tool option other than Wordpress. For many bloggers, Wordpress is a\n                        poor choice of blogging tools. Of course, there is always language snobbery in\n\n                        any programming community, but Wordpress is built on a language, PHP, which\n                        is widely reviled by developers from various language communities. Because\n\n                        PHP is more accessible language than other languages, the choice of PHP has\n                        fostered a large community of plugin developers. Unfortunately, this communi-\n                        ty is fractured, and the plugins which arise from it are at best poorly document-\n\n                        ed and poorly integrated, and at worst, poorly designed and buggy. While\n                        Wordpress does often have a plugin for anything you need, the problem it sol-\n\n                        ves often creates more problems in the long term when scalability or database\n                        optimization or security, for example, become concerns.\n                           Jekyll is interesting because of the components which are not present when\n\n                        compared against Wordpress. Jekyll does not require a database. Jekyll does\n                        not require you know how to write in HTML. While Wordpress ostensibly adver-\n\n                        tises itself as a tool which can be used without knowledge of these technolo-\n                        gies, talk to those of us who have struggled with recovery of a mangled data-\n\n                        base after installation of a new Wordpress plugin, or resolving scalability issues\n                        on a large Wordpress site, or analyzing and fixing broken HTML produced by the\n                        Wordpress editor. We’ll tell you that, when using Wordpress, you don’t need to\n\n                        know about MySQL or HTML at all, because it is already too late by then to fix\n                        whatever problem you are facing.\n\n                           Jekyll responds to these concerns in a really elegant way. Instead of author-\n                        ing in HTML, you author in a simple and readable language called Markdown,\n\n                        which the Jekyll engine converts automatically to HTML for you. Instead of stor-\n                        ing posts and other data inside a MySQL database, you use the filesystem to\n                        store posts and layouts and then regenerate the entire site when infrequent\n\n\n\n\n\n        118","initials":"ew"},"\"Seems odd to teach the reader an \\\"incorrect\\\" way of doing things\"":{"page":119,"text":"                                                                    The Harmless Brew that Spawned Jekyll\n\n\nchanges are made. And, to faciliate interaction, you use client side JavaScript\nplugins instead of pushing that interaction into your database. Jekyll has found\na sweet spot with a simple technology set that makes your life easier and\n\nmakes beautiful blogs and simple web sites. And, the biggest benefit of doing\nthings the “Jekyll” way is that all dependencies can sit within a repository, and\n\nthis means your entire site can be hosted on GitHub.\n\n\n(Less Than) Evil Scientist Parents\n\n\nLike many of the open source technologies in heavy usage at GitHub, jekyll was\noriginally developed by Tom Preson Warner, one of the co-founders of GitHub,\n\nand Nick Quaranto, of 37 Signals, though there are now thousands of contribu-\ntors to the Jekyll codebase. Like many open source projects, the strength of the\ntool comes not from the brilliance of the original developers or the brilliance of\n\nthe idea, but the way that those original developers cultivated community and\ninvolvement among the users of the tool.\n\n\n\nOperating Jekyll Locally\n\n\nTo really use jekyll, you’ll need thjekyll  gem. As we explain in the ???, we\ncould install a ruby gem using this command:\n\n\n   $ gem install jekyll\n\n   There are two issues with doing installation this way. The first is that any\n\ncommands we run inside the command line are lost to us and the world (other\nthan in our private shell history file). The second is that if we are going to pub-\nlish any of our sites to GitHub, we will want to make sure we are matching the\n\nexact versions of Jekyll and its dependencies so that a site that works on our\nlocal laptop also works when published into GitHub. If you don’t take care of\n\nthis, you’ll occasionally get an email like this from GitHub:\n\n    The page build failed with the following error:\n\n\n    page build failed\n\n    For information on troubleshooting Jekyll see\n    https://help.github.com/articles/using-jekyll-with-pages#troubleshooting\n\n    If you have any questions please contact GitHub Support.\n\n   The fix for these two issues is a simple one. You’ve probably seen other\n\nchapters using a Gemfile   to install ruby libraries. Instead of using a manual\ncommand like   bundle  to install from the command line, let’s put this depend-\n\n\n\n\n                                                                                          119","initials":"ew"},"\"Do you need to drill down to this level of detail?\"":{"page":121,"text":"                                                                                A Jekyll Blog in 15 Minutes\n\n\n     The jekyll new command does not create or initialize a new git repository\n\n     for you with your files. If you want to do this, you will need togite the\n     init command. The jekyll initialization command does create the proper\n     structure for you to easily add all files to a git repositorygitust use\n\n     add .; git commit and your gitignore file will be added and configure\n     your repository to ignore unnecessary files like_sitedirectory.\n\n\n   All your blog posts are stored in th_posts  directory. Jekyll sites are not re-\n\nquired to have a  _posts  directory (you can use jekyll with any kind of static\nsite) but if you do include files in this directory jekyll handles them in a special\n\nway. If you look in the_posts  directory now, you see that the jekyll initializa-\ntion command has created your first post for you, something like      _posts/\n\n2014-03-03-welcome-to-jekyll.Markdown          . These posts have a special\nnaming format: the title of the post (with any whitespace replaced with hy-\n\nphens) trailed by the date and then an extension (either .Markdown   or .md for\nMarkdown files, or .textile  for Textile)\n\n   Your new jekyll blog also comes with a few HTML files: an  index.html    file\nwhich is the starting point for your blog, and several layout files which are used\n\nas wrappers when generating your content. If you look in the_layouts   directo-\n\nry, notice there is a file namedefault.html   and another named    post.html  .\nThese files are the layout files, files which are wrapped around all generated\ncontent, like those from your Markdown formatted blog posts. For example, the\n\npost.html   file is wrapped around the generated content of each file stored in-\n\nside the _posts  directory. First the markup content is turned into HTML and\nthen the layout wrapper is applied. If you look inside each of the files inside the\n\n_layouts    directory, you will see that each contains a placeholder with\n{{ content }}    . This placeholder is replaced with the generated content from\n\nother files.\n   These placeholders are actually a markup language on their own: “Liquid\n\nTemplating.” Liquid Templating (or Liquid Markup) was developed and open\nsourced by Shopify, and is a safe way to include programmatic constructs (like\nloops and variables) into a template, without exposing the rendering context to\n\na full fledged programming environment. Shopify wanted to build a way for un-\ntrusted users of their public facing systems to upload dynamic content but not\n\nworry that the markup language would permit malicious activity; for example,\ngiven a full fledged embedded programming language, they would open them-\n\nselves to attack if a user wrote code to open network connections to sites on\ntheir internal networks. Templating languages like PHP or ERB (embedded ruby\n\ntemplates, popular with the Ruby on Rails framework) allow fully embedded\ncode snippets and while this is very powerful when you have full control over\n\nyour source documents, it can be dangerous to provide a mechanism where\n\n\n\n\n                                                                                           121","initials":"ew"},"\"this is all very dry, consider changing the style of presentation? Could an annotated file structure be a better way to talk about all of this?\"":{"page":122,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n                        that embedded code could look like    system(\"rm -rf /\")      . Liquid provides\n                        many of the benefits of embedded programming templates, without the dan-\n\n                        gers.\n                           Lastly, your jekyll directory has a special file ca_config.yml   . This is the\n\n                        jekyll configuration file. Peering into it, you’ll see it is very basic:\n\n                           name: Your New Jekyll Site\n\n                           markdown: redcarpet\n                           pygments: true\n\n                           We only have three lines to contend with and they are simple to understand:\n\n                        the name of our site, the Markdown parser used by our jekyll command, and\n                        whether to use pygments to do syntax highlighting.\n\n                           To view this site locally run this command:\n\n\n                           $ jekyll serve\n\n                           This command builds the entirety of your jekyll directory, and then starts a\n\n                        mini web server to serve the files up to you. If you then visihttp://local-\n                        host:4000   in your web browser, you will see something the front page of your\n\n                        site and a single blog post listed in the index.\n\n\n\n FIGURE 7-1\n\n A bare Jekyll site\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Clicking into the link inside the “Blog Posts” section, you will then see your\n\n                        first post.\n\n\n\n\n\n\n\n\n\n        122","initials":"ew"},"\"Good to show the screenshot of the real thing alongside the code that generated it\"":{"page":123,"text":"                                                                             A Jekyll Blog in 15 Minutes\n\n\n\n                                                                                FIGURE 7-2\n\n                                                                                A sample post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Our jekyll initialization command created this new post for us. This page is\n\nbacked by the Markdown file inside the _posts directory which we saw earlier.\n\n\n   ---\n   layout: post\n   title: \"Welcome to Jekyll!\"\n   date:    2014-03-03 12:56:40\n   categories: jekyll update\n\n   ---\n\n   You'll find this post in your `_posts` directory - edit this post and re-build (or run with the `-w` switch) to see your chang▯es!\n   To add new posts, simply add a file in the `_posts` directory that follows the convention: YYYY-MM-DD-name-of-post.ext.\n\n\n   Jekyll also offers powerful support for code snippets:\n\n   {% highlight ruby %}\n\n   def print_hi(name)\n     puts \"Hi, #{name}\"\n   end\n   print_hi('Tom')\n   #=> prints 'Hi, Tom' to STDOUT.\n\n   {% endhighlight %}\n\n\n\n\n\n                                                                                       123","initials":"ew"},"\"Yet Another Markup Language?\"":{"page":124,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n                          Check out the [Jekyll docs][jekyll] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyl▯l's GitHub repo][jekyll-gh].\n\n\n                          [jekyll-gh]: https://github.com/mojombo/jekyll\n                          [jekyll]:      http://jekyllrb.com\n\n                          Hopefully you’ll agree this is a fairly intuitive and readable alternative to raw\n\n                       HTML. This simplicity and readability is one of the major benefits of using Je-\n                       kyll. Your source files maintain a readability that allows you to focus on the con-\n\n                       tent itself, not on the technology that will eventually make them beautiful. Let’s\n                       go over this file and investigate some of the important pieces.\n\n\n                       YFM: YAML Front Matter\n\n\n                       The first thing we see in a Jekyll file is the YAML Front Matter (YFM).\n\n\n                          ---\n                          layout: post\n                          title: \"Welcome to Jekyll!\"\n                          date:    2014-03-03 12:56:40\n                          categories: jekyll update\n\n                          ---\n\n                          YFM is a snippet of YAML (“YAML Aint Markup Language”) delimited by three\n\n                       hyphens on both the top and bottom. YAML is a simple structured data seriali-\n                       zation language used by many open source projects instead of XML. Many peo-\n                       ple find it more readable and editable by humans than XML. The YFM in this file\n\n                       shows a few configuration options: a layout, the title, the date and a list of cate-\n                       gories.\n\n                          The layout specified references one of the files in o_layouts   directory. If\n                       you don’t specify a layout file in the YFM, then Jekyll assumes you want to use a\n\n                       file calledefault.html    to wrap your content. You can easily imagine adding\n                       your own custom layout files to this directory and then overriding them in the\n\n                       YFM. If you look at this file, you see that it manually specifiposthelayout.\n                          The title is used to generate t<title>   tag and can be used anywhere else\n                       you need it inside your template using the double braces syntax from Liquid:\n\n                       {{ page.title }}    . Notice that any variable from th_config.yml   file is pre-\n                       fixed with the site.  namespace, while variables from your YFM are prefixed\n\n                       with page. . Though the title matches the filename (after replacing spaces with\n                       hyphens), changing the title in the YFM does not affect the name of the URL\n\n                       generated by Jekyll. If you want to change the URL, you need to rename the file\n                       itself. This is a nice benefit if you need to slightly modify the title and don’t want\n\n                       to damage preexisting URLs.\n\n\n\n\n\n        124","initials":"ew"},"\"Do we need to go through the YAML in such depth? If this is necessary, would the information be better presented as a table of features?\"":{"page":125,"text":"                                                                                   A Jekyll Blog in 15 Minutes\n\n\n   The date and categories are two other variables included in the YFM. They\nare completely optional and strangely unused by the structure and templates\n\ncreated by default using the Jekyll initializer. They do provide additional con-\ntext to the post, but are only stored in the Markdown file and not included in-\nside the generated content itself. The categories list is often used to generate an\n\nindex file of categories with a list of each post included in a category. If you\ncome from a Wordpress background, you’ll likely have used categories. These\n\nare generated dynamically from the MySQL database each time you request a\nlist of them, but in Jekyll this file is staticly generated. If you wanted something\n\nmore dynamic, you could imagine generating a JSON file with these categories\nand files, and then building a JavaScript widget which requests this file and\n\nthen does something more interactive on the client side. Jekyll can take any\ntemplate file and convert it to JSON (or any other format) — you are not limited\nto just generating HTML files.\n\n   YFM is completely optional. A post or page can be rendered into your Jekyll\nsite without any YFM inside it. Without YFM, your page is rendered using the de-\n\nfaults for those variables, so make sure the default template, at the very least, is\nwhat you expect will wrap around all pages left with unspecified layouts.\n\n   One important default variable for YFM is the published variable. This vari-\nable is set to true by default. This means that if you create a file in your Jekyll\n\nrepository and do not manually specify the published setting, it will be publish-\ned automatically. If you set the variable to false then the post will not be pub-\nlished. With private repositories you can keep the contents of draft posts entire-\n\nly private until writing has completed by making sure published is set to false.\nUnfortunately, not all tools that help you create Jekyll Markdown files remem-\n\nber to set the published variable explicitly inside of YFM, so make sure you\ncheck before committing the file to your repository if there is something you\n\ndon’t yet want published.\n\n\nJekyll markup\n\n\nGoing past the YFM, we can start to see the structure of Markdown files. Mark-\ndown files can be, at their simplest, just textual information without any for-\n\nmatting characters. In fact, if your layout files are well done, you can definitely\ncreate great blog posts without any fancing formatting, just pure textual con-\n\ntent.\n   But, with a few small Markdown additions, you can really make posts shine.\nOne of the first Markdown components we notice is the backtick character,\n\nwhich is used to wrap small spans of code (or code-ish information, like file-\nnames in this case). As you use more and more Markdown, you’ll find Mark-\n\ndown to be insidiously clever in the way it provides formatting characters\n\n\n\n\n\n                                                                                              125","initials":"ew"},"\"It seems a little strange to complain about dynamic webpages only to talk about themes later on...\"":{"page":127,"text":"                                                                                 A Jekyll Blog in 15 Minutes\n\n\n     The jekyll watch switch does reload all HTML and markup files, but does\n     not reload the _config.yml file. If you make changes to it, you will need\n     to stop and restart the server.\n\n\n\n   If you are running multiple Jekyll sites on the same laptop, you’ll quickly\nfind that the second instance of  jekyll serve     fails because it cannot open\n\nport 4000. In this case, usejekyll --port 4010      to open port 4010 (or what-\never port you wish to use instead).\n\n\nPrivacy Levels with Jekyll\n\n\nJekyll repositories on GitHub can be either public or private repositories. If your\n\nrepository is public you can host public content generated from the Jekyll\nsource files without publishing the source files themselves. Remember, as no-\n\nted previously, that any file without publishing: false      inside the YFM will\nbe made public the moment you push it into your repository.\n\n\nThemes\n\n\nJekyll does not support theming internally, but it is trivial to add any CSS files\n\nor entire CSS frameworks. You could do this yourself, or you could just fork an\nexisting jekyll blog which has the theming you like. The most popular themed\nJekyll blog structure is Octopress. We don’t display this here, but you another\n\neasy option is to add the Bootstrap CSS library just as we did in the Chapter 9\nchapter.\n\n\n\nPublishing on GitHub\n\nOnce you have your blog created, you can easily publish it to GitHub. There are\n\ntwo ways which you can publish Jekyll blogs:\n\n    • As a github.io site\n\n    • On a domain you own\n\n   Github offers free personal blogs which are hosted on the github.io domain.\nAnd, you can host any site with your own domain name with a little bit of con-\n\nfiguration.\n\n\n\n\n\n\n\n\n\n\n                                                                                            127","initials":"ew"},"\"This all seems rather standard, is it not already covered elsewhere?\"":{"page":129,"text":"                                                                               A Jekyll Blog in 15 Minutes\n\n\nTHE GH-PAGES BRANCH\n\nTo work on the gh-pages branch, check it out and create the branch inside your\n\nrepository.\n\n\n   $ git checkout -b gh-pages\n   $ rake post title=\"My next big blog post\"\n   $ git add _posts\n   $ git commit -m \"Added my next big blog post\"\n   $ git push -u origin gh-pages\n\n\n   You will need to always remember to work on the gh-pages branch; if this\nrepository is only used as a blog, then this probably is not an issue. Adding the\n\n-u switch will make sure that git always pushes up the gh-pages branch when-\never you do a push.\n\n\nTHE CNAME FILE\n\n\nThe CNAME file is a simple text file with the domain name inside of it.\n\n\n   $ echo 'mydomain.com' > CNAME\n   $ git add CNAME\n   $ git commit -m \"Added CNAME\"\n   $ git push\n\n\n   Once you have pushed the CNAME file to your repository, you can verify that\nGitHub thinks the blog is established correctly by visiting the admin page of\n\nyour repository. An easy way to get there is using thegithub  gem, no longer\nactively maintained but still a useful command line tool.\n\n\n   $ gem install github\n   $ github admin # Opens up https://github.com/username/repo/settings\n\n\n   The github gem is a useful command line tool, but unfortunately it is tied to\nan older version of the GitHub API, which means the documented functionality\n\nis often incorrect.\n   If your blog is correctly setup, you will see something like Figure 3 in the mid-\n\ndle of your settings page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                         129","initials":"ew"},"\"Whilst some personal stories are nice to add character... this one about a travel website you built is a bit unnecessary... plus TripAdvisor?\"":{"page":133,"text":"                                                                                     Scraping Sites into Jekyll\n\n\n   The Tumblr import plugin has a few interesting options.\n\n   Write out HTML; if you prefer to use Markdown use   md .\n\n\n   This importer will grab images if you provide a true value.\n\n\n   Wrap code blocks (indented 4 spaces) in a Liquid “highlight” tag if this is set\n   to true.\n\n\n   Write pages that redirect from the old Tumblr paths to the new Jekyll paths\n   using this configuration option.\n\n\n   Exporting from Tumblr is considerably easier than Wordpress. The Tumblr\nexporter scrapes all public posts from the blog, and then converts to a Jekyll\ncompatible post format.\n\n   We’ve seen how we can use the importers available on import.jekyllrb.com\nto import. What if we have a non-standard site that we need to import?\n\n\n\nScraping Sites into Jekyll\n\n\nIf you are stuck with a site that does not fit any of the standard importers, you\ncould write your own importer by perusing the source of the Jekyll importers\n\non GitHub (http://github.com/jekyll/jekyll-import). This is probably the right\nway to build an importer if you plan on letting others use it, as it will extend\n\nseveral jekyll importer classes already available to make importing standard for\nother contributors. Learning all the existing methods and reading through the\n\ndozens of samples can be a lot of work, however; another option is just to write\nout our files respecting the very simple format required by Jekyll. As we are pro-\ngrammers in the true sense of the word we embrace and accept our laziness\n\nand choose the second route. Let’s write some code to scrape and generate a\nJekyll site.\n\n   Almost fifteen years ago while traveling in Brazil I grew increasingly frustra-\nted with the guide books I used. It seemed like every time I went to a restaurant\nrecommended by a guidebook I left the restaurant thinking “well, either they\n\npaid for that review or the review was written several years ago when this res-\ntaurant had a different owner.” To address this discrepancy between reviews\n\nand realities, I built a site called ByTravelers.com. The idea was that travelers\ncould use ByTravelers to record their experiences and easily share their experi-\n\nences with their friends and families (replacing the long emails they used to\nsend) and that that information would then become an authentication source\nof information about good and bad travel experiences.\n\n\n\n\n\n\n                                                                                               133","initials":"ew"},"\"Kind of strange to be promoting your own defunct website\"":{"page":136,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n\n FIGURE 7-5\n\n Archive of\n ByTravelers.com on\n Archive.org\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Taking the URL from Chrome, we can use this as our starting point for scrap-\n\n                        ing. Clicking around throughout the site, it becomes evident that each URLs to\n                        a journal entry uses a standard format; in other words,   http://www.bytrav-\n\n                        elers.com/journal/entry/56         indicates the 56th journal item stored on the\n                        site. With this knowledge in hand, we can iterate over the first hundred or so\n\n                        URLs easily.\n                           Going to one of these pages through the archived site, it is useful to view the\n\n                        source of the page and start to understand the structure of a page which we can\n                        then use when pointing our mechanize scraper at the page to pull out content.\n\n                        Any modern web browser supports a debug mode, and Chrome (my browser of\n                        choice) supports this as well. If we hold down the control key and click (at least\n\n                        on Mac OSX; righting-click on Windows or Linux works in the same way) into the\n                        “body” of a journal entry on its page, we will see a context menu that gives us\n\n                        the option to “Inspect Element”. Chosing this option brings up the Chrome De-\n                        veloper Tools and shows us the HTML code of the page pretty printed for us.\n\n\n\n\n\n        136","initials":"ew"},"\"This seems like an awful lot of code to obtain information you could just give to the reader... It could even be stored in a Git repository\"":{"page":142,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n                              # puts \"Title: #{title_for_filename}\"\n                              filename = \"_posts/#{creation_date}-#{title_for_filename}.md\"\n                              File.open( filename, \"w+\" ) do |f|\n                                f.write template\n                              end\n\n                            end\n\n                            def run\n                              100.times do |i|\n                                get_ith_page( i )\n\n                              end\n                              100.times do |i|\n                                if pages[i]\n                                  write_post( pages[i] )\n                                end\n\n                              end\n                            end\n\n                            def process_creation_date( i, row )\n                              location, creation_date = row.text().split /last updated on:/\n\n                              creation_date.strip()\n                            end\n\n                            def process_location( i, row )\n                              location, creation_date = row.text().split /last updated on:/\n\n\n                              location.gsub!( /Concerning: /, \"\" )\n\n\n                          We’ve modified theget_ith_page    method to save each page as a tuple (the\n                       title and body) and then print them outfter processing inside trun method.\n\n\n                          Our process body might look a little excessive. Why not just return the result\n                          ofrow.text()  ? The reason is that markdown is very specific about the for-\n\n                          mat it requires for text formatting. Each block of text separated by two new-\n                          lines will be formatted with<p> tags. Unfortunately, Mechanize and Noko-\n\n                          giri don’t return text formatted that way, so this function retrieves<p>ch\n                          tag we scraped, strips whitespace from the ends, and then adds it back to a\n\n                          body variable. If you scrape text from a site like we are doing here, you might\n                          need to normalize the text in a similar way.\n\n\n                          Do the same type of processing with the title. With this site there are occa-\n                          sionally titles which include the word “Title:” in the title itself (authors have\n\n                          to be forgiven for their own formatting quirks) so strip that out if we see it\n                          there.\n\n\n\n\n\n\n\n       142","initials":"ew"},"\"Really don't understand the point of this. What is the reader learning? \"":{"page":148,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n                           ...\n\n                                 Regenerating: 1 files at 2014-06-20 12:54:52 ...done.\n                                 Regenerating: 2 files at 2014-06-20 12:55:03 ...done.\n                                 Regenerating: 1 files at 2014-06-20 12:55:15 ...done.\n                                 Regenerating: 4 files at 2014-06-20 12:55:15 ...done.\n\n                           ...\n\n                           And, if we reload our page we’ll see them listed inside our index.\n\n\n\n\n FIGURE 7-8\n\n Publishing a single\n post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           If we trust that all these posts are correct, we can change thescraper.rb\n\n                        script to make them all public (inside the heredoc template, just change the\n                        published flag), or we could change files individually by hand as we did here.\n\n                           Taking a look at the blog post itself, we see this after clicking on the first link.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        148","initials":"ew"},"\"Teaching the reader about CSS and formatting a webpage is again very far removed from GitHub\"":{"page":149,"text":"                                                                               Scraping Sites into Jekyll\n\n\n\n                                                                                FIGURE 7-9\n\n                                                                                Not the best\n                                                                                formatting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Not very pretty at all. We can beautiful this by adding some styling to the\npage. We’ll use Bootstrap, the most popular CSS framework on GitHub. To start,\n\nedit the layout to include Bootstrap from the Bootstrap CDN. And, add acon-\ntainer  class around the content.\n\n\n   <html>\n   <head>\n\n   <title>ByTravelers.com</title>\n\n   <link href=\"//maxcdn.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css\" rel=\"stylesheet\">\n\n   </head>\n\n\n   <body>\n\n   {{ content }}\n\n\n   </body>\n   </html>\n\n\n   Notice that our files are regenerated in the terminal window with jekyll\nserve -w  . Refreshing the page shows some improvement, but we can do bet-\n\nter. Let’s make a front page which shows just the ten most recent post, and an\narchive page which shows all the posts in reverse chronological order.\n\n\n\n\n                                                                                        149","initials":"ew"},"\"Firstly - my general point on \\\"why are we covering this?\\\" still stands.\\n\\nSecondly - again you could just provide the images.\\n\\nThirdly - generally taking images from other websites is frowned upon... in some cases it maybe copyright theft. I'm not sure\\n you want to be encouraging or educating people on how to do it\"":{"page":153,"text":"                                                                             Scraping Sites into Jekyll\n\n\n   <link href=\"/assets/css/site.css\" rel=\"stylesheet\">\n\n\n   </head>\n   ...\n\n\nGRABBING ORIGINAL IMAGES\n\nOur site is bare beyond text and the original colors; adding the images would\n\nadd some pop. We can easily modify our scraper.rb  script and pull down the\noriginal images from our site and then republish them into our new Jekyll blog.\n\nTaking a look at the archived site, note that each title has an image to the left of\nit. If we customize ouprocess_title   method we can retrieve these images\nand then publish them into our blog.\n\n   Finding the image is easyimg = ( title/ \"img\" )     will retrieve an “img”\ntag from the title as passed to us. Printing out the element putsgis simple\n\nway to view the contents of the elements, one of which looks like this<img\nsrc=\"/web/20030502075943im_/http://www.bytravelers.com/\n\nimages/pro/book.gif\">     We can then dig into the element using syntax like\n\nimg.attr('src')    and get to the actual source of the image. We’ll need to ap-\npend the base site URL to this and can then retrieve the image from archive.org.\n   Unfortunately, the VCR gem does not easily allow us to make requests which\n\nare not captured. There are methods in VCR to ignore requests, but without\nheavily refactoring ourget_ith_method   . Instead we cheat by using a com-\n\nmand line tool called wget to download the image. The VCR gem works by\nhooking into ruby libraries which make HTTP calls; by using thwget tool we\n\ncan avoid using Ruby for a moment and download the file manually.\n\n   ...\n   def process_title( i, title )\n\n     img = ( title / \"img\" )\n     src = img.attr('src').text()\n     filename = src.split( \"/\" ).pop\n\n     output = \"assets/images/\"\n\n     full = File.join( output, filename )\n\n     unless File.exists? full\n       root = \"https://web.archive.org\"\n       remote = root + src\n\n       contents = `wget --quiet -O #{full} #{remote}`\n     end\n\n     title = title.text()\n     if title\n\n       title.gsub!( /Title:/, \"\" )\n\n\n\n                                                                                      153","initials":"ew"},"\"I think we can safely say that this book has gone completely off on a tangent now\"":{"page":156,"text":"CHAPTER 7: Ruby and Jekyll\n\n\n                         <br/>\n                         <em>Posted on {{ post.date | date_to_string }}</em>\n                         <br/>\n\n                         {% endfor %}\n                         </div>\n\n\n                         Note that in this case the variable is presentedpost.image   as compared\n                      to page.image   when we are inside a post. Jekyll and Liquid are not consistent\n\n                      here, so caveat emptor.\n                         Our blog definitely has more life when we add in the original colors and im-\n                      ages. It still looks like a blog from the last millenium, but it is an improvement.\n\n\n\n\n FIGURE 7-11\n\n Restoring the\n original colors and\n images\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                      MAPPING INTEGRATION\n\n                      This is a site about travel information, so it makes sense to add maps as well.\n\n                      We can process the location information we retrieved from the archived site\n                      and add a map image.\n\n                         Add a new function called process_location    and place it underneath the\n                      process_creation_date      method inside theget_ith_page   method.\n\n\n\n\n       156","initials":"ew"},"\"The fact that the blog/site is hosted on GitHub is the only tentative link this chapter has to GitHub.\"":{"page":159,"text":"                                                                                          Summary\n\n\nPublishing our blog to GitHub\n\n\nLike any other GitHub repository, we can then publish our blog using the same\ncommands we saw with earlier repositories. Obviously you should change the\n\nusername and blog name to suit your own needs.\n\n\n   $ export BLOG_NAME=xrd/bytravelers.com\n   $ gem install hub\n   $ hub create $BLOG_NAME # You might need to login here\n   $ sleep $((10*60)) && open $BLOG_NAME\n\n\n   And, don’t forget to setup DNS records and give yourself appropriate time to\nlet those records propagate out.\n\n\n\nSummary\n\n\nWe’ve shown that we can quickly setup a blog on GitHub that has version con-\ntrol built in. We’ve shown how to import blogs like Wordpress into Jekyll. And,\nwe’ve taken a site available only as an archive on the Internet Archive and scra-\n\nped the content, images and even the colors, and then converted it to a Jekyll\nblog. Jekyll is a simple tool for managing websites, but a hidden benefit is that\n\nbecause Jekyll is so simple, you can easily write your own tools to interact and\nbuild on top of Jekyll. Once our site is a repository on GitHub, making changes\nyourself, or accepting them from other contributors is as easy as clicking\n\n“Merge” on a pull request.\n   In the next chapter we will continue looking at Jekyll by building an Android\n\napplication that uses the Java GitHub API bindings and allows you to create Je-\nkyll blog posts with the Git Data API.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                       159","initials":"ew"},"\"Nice short introduction that lays out what you plan to cover nicely\"":{"page":161,"text":"     Android and the Git Data API                                 8\n\n\n\n\n\n\n\n\nAndroid is currently the most popular operating system for mobile devices,\novertaking Symbian OS in 2010, experiencing incredible growth since its release\nin 2008. If it is not the case already, mobile OSes like Android and iOS will soon\nbe more prevalent than desktop OSes. Though it may not occur to many people\nyet, as more and more people shift to using mobile devices, we will see demand\nfor developer tools on mobile devices. The GitHub API has a good set of bind-\n\nings for developing Java and Android applications. We’ll use the egit libraries to\ndevelop a small Android application which posts to our blog hosted on GitHub.\n  Our blogging application will allow us to login to GitHub, and then ask us for\na quick note describing how we are feeling. The application will then compose\na Jekyll blog post for us and push the post into our blog on GitHub.\n\n\nPrerequisites\n\n\nSetting up a Jekyll blog\n\nWe are writing an application which adds jekyll blog entries, and we are writing\ntests to verify our application works as advertisted, so we need a sandbox blog\nagainst which we can run commands. There are various ways to create a new\nJekyll blog. The simplest is to run a series of Ruby commands documented\nhere; if you want to know more about Jekyll, it is covered in more depth in the\nJekyll chapter. There are a few items of note when establishing a Jekyll blog\n\nthat have some complexity, things like mapping a hostname properly and using\nthe correct branch inside git. For our purposes here, however, we won’t need to\nmake sure all that is established. All we need is to make sure that we have a\nsandbox repository that has the structure of a jekyll blog.\n\n  $ printf \"source 'https://rubygems.org'\\n\\ngem 'github-pages'\\ngem 'hub'\" >> Gemfile\n  $ export BLOG_NAME=mytestblog\n  $ bundle\n  $ jekyll new $BLOG_NAME\n\n\n                                                                        161","initials":"ew"},"\"You probably don't need to tell the reader anything other than to Google instructions on how to install the necessary Java and Android SDKs\"":{"page":162,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                           $ cd $BLOG_NAME\n                           $ hub create\n                           $ git push -u origin master\n\n\n                           These commands install the correct libraries for using Jekyll (and one for our\n                        tests as well), generate a new blog using the jekyll command line tool, and then\n\n                        create a blog on GitHub with those files. On the second line we specify the\n                        name of the blog; you are welcome to change this to any name you’d like, just\n\n                        make sure the tests match the name.\n\n\n                             When you have finished running these commands, you should close the\n                             terminal window. There are other commands later in this chapter which\n                             should occur in a fresh directory and as such it is best not to run those\n\n                             commands from within the same directory where you created your jekyll\n                             blog. You’ve pushed all those files into GitHub, so you could safely delete\n                             the local repository in this directory.\n\n\n\n                        Android Development Tools\n\n\n                        If you don’t have a physical Android device, don’t fret. You can follow along\n                        with this chapter without having an actual Android device by doing develop-\n\n                        ment and testing on a virtual device.\n\n\n                        Installing the Java SDK\n\n\n                        Unfortunately there is no simple shell command to install Java in the same way\n\n                        as there is for Ruby and NodeJS using RVM or NVM. Oracle controls the Java\n                        language and distribution of official SDKs, and they restrict access to down-\n                        loads other than from java.oracle.com. Java is freely available, but you need to\n\n                        visit java.oracle.com and find the correct download for your needs. Android\n                        works with either the 1.6 or 1.7 versions of Java.\n\n\n\n                        Installing the Android SDK\n\n\n                        Creating AVDs for Development\n\n\n                        Once you have installed the SDK, you can create a virtual device called an AVD\n                        (Android Virtual Device). AVDs are very useful because you can see any screen\n\n                        resolution, any SDK version, and can quickly delete and reset the devices. In ad-\n                        dition, AVDs are created “rooted” by default, which allows you to view the con-\n\n                        tents of all files, a capability which is not available on non-rooted devices. This\n                        makes debugging AVDs much easier because, for example, you can inspect\n\n\n\n\n        162","initials":"ew"},"\"Good to walkthrough with screenshots\"":{"page":164,"text":"CHAPTER 8: Android and the Git Data API\n\n\n\n FIGURE 8-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                            You are generally free to choose whatever settings you like. Google produces\n                         a real device called the Nexus 5. This is the Android reference device, and is a\n\n                         good option for a generic device with good support across all features. You can\n                         choose this one if you are confused about which to use.\n\n                            Once you have created an AVD, start it up. It will take a few minutes to boot;\n                         AVDs emulate the chipset in software and booting up can take a few minutes,\n                         unfortunately. There are alternative tools that speed up AVD boot time (Geny-\n\n                         motion is one of those), but usage of these tools is are outside the scope of this\n                         book. After your AVD has completed booting, you can test to see if it is correctly\n\n                         started and available using the adb command line tool.\n\n\n\n\n        164","initials":"ew"},"\"Gradle is already on major version 2 and has been in active development for many years now, so I'm not sure it is still new?\"":{"page":165,"text":"                                                                                       Prerequisites\n\n\n   $ adb devices -l\n\n   * daemon not running. starting it now on port 5037 *\n   * daemon started successfully *\n   List of devices attached\n   0222fdc60910aede         device usb:1D110000 product:hammerhead model:Nexus_5 device:hammerhead\n\n\n   Running the adb command starts a daemon service which communicates\nwith your device. In this case the daemon reported that I have a single device\nattached via USB which is the emulated Nexus 5 device.\n\n\n\nCreating a New Project\n\n   $ mkdir ghru # GitHub R U?\n\n   $ cd ghru\n   $ android create project --target 13 --name GHRU --path . --activity GitHubRu --package com.githubru\n\n   These three commands create a new directory ghru , enter the directory, and\n\nthen build a simple Android directory structure with the proper files.\n   If you have the ant tool installed, you can build this new project want the\n\ndebug  command. This will create an APK in the bin directory called ./bin/\nGhru-debug-unaligned.apk     . To install it on your device run the command\n\nant debug install    . Then, you can launch the application by double clicking\non the application titled “GitHubRu”.\n\n\nADDING GRADLE SUPPORT\n\n\nGradle is a new build system for Java and has become theoffical build system\nfor the Android platform. Using a simpbuild.gradle   file we can build an en-\n\ntire Android application from the command line. Gradle is well supported with\nmore advanced editors, so you can always import an Android project using Gra-\ndle and use it with editors like Eclipse or Android Studio. Ant used to be the de-\n\nfault and preferred method of building Java projects from the command line,\nbut gradle syntax is so simple and powerful and lightweight that it is now the\n\npreferred build system by the Android team.\n\n   buildscript {\n\n       repositories {\n           mavenCentral()\n       }\n       dependencies {\n           classpath 'com.android.tools.build:gradle:0.6.+'\n       }\n\n   }\n\n\n\n\n\n                                                                                       165","initials":"ew"},"\"Again you don't have to justify the technology choice here. Just focus on what is important.\"":{"page":165,"text":"                                                                                       Prerequisites\n\n\n   $ adb devices -l\n\n   * daemon not running. starting it now on port 5037 *\n   * daemon started successfully *\n   List of devices attached\n   0222fdc60910aede         device usb:1D110000 product:hammerhead model:Nexus_5 device:hammerhead\n\n\n   Running the adb command starts a daemon service which communicates\nwith your device. In this case the daemon reported that I have a single device\nattached via USB which is the emulated Nexus 5 device.\n\n\n\nCreating a New Project\n\n   $ mkdir ghru # GitHub R U?\n\n   $ cd ghru\n   $ android create project --target 13 --name GHRU --path . --activity GitHubRu --package com.githubru\n\n   These three commands create a new directory ghru , enter the directory, and\n\nthen build a simple Android directory structure with the proper files.\n   If you have the ant tool installed, you can build this new project want the\n\ndebug  command. This will create an APK in the bin directory called ./bin/\nGhru-debug-unaligned.apk     . To install it on your device run the command\n\nant debug install    . Then, you can launch the application by double clicking\non the application titled “GitHubRu”.\n\n\nADDING GRADLE SUPPORT\n\n\nGradle is a new build system for Java and has become theoffical build system\nfor the Android platform. Using a simpbuild.gradle   file we can build an en-\n\ntire Android application from the command line. Gradle is well supported with\nmore advanced editors, so you can always import an Android project using Gra-\ndle and use it with editors like Eclipse or Android Studio. Ant used to be the de-\n\nfault and preferred method of building Java projects from the command line,\nbut gradle syntax is so simple and powerful and lightweight that it is now the\n\npreferred build system by the Android team.\n\n   buildscript {\n\n       repositories {\n           mavenCentral()\n       }\n       dependencies {\n           classpath 'com.android.tools.build:gradle:0.6.+'\n       }\n\n   }\n\n\n\n\n\n                                                                                       165","initials":"ew"},"\"Again just point the reader at the Gradle site, they can figure the rest out.\"":{"page":166,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                          repositories {\n                              mavenCentral()\n                          }\n\n\n                          apply plugin: 'android'\n\n                          android {\n                              compileSdkVersion 19\n                              buildToolsVersion \"19.0.1\"\n\n\n                              sourceSets {\n                                main {\n                                  manifest.srcFile 'AndroidManifest.xml'\n\n                                  java.srcDirs = ['src']\n                                  resources.srcDirs = ['src']\n                                  res.srcDirs = ['res']\n                                  assets.srcDirs = ['assets']\n                                }\n\n                              }\n\n                          }\n\n\n                          dependencies {\n                            compile 'org.eclipse.mylyn.github:org.eclipse.egit.github.core:2.1.5'\n                          }\n\n\n                          Gradle build files use some standard boilerplate which you can ignore here,\n                       but there are two items which are worth noting.\n\n\n                          Gradle was not designed for Android; it started as a generic java build tool.\n                          We need to specify where the files to compile reside for an android project\n\n                          using thesourceSets   variable.\n\n\n                          We can install the egit library, our interface to the GitHub API from within\n                          Java, using this simple declaration. Gradle will download the proper JAR\n                          files from the Maven repository and build them into our application using\n\n                          this dependency declaration.\n\n\n                       INSTALLING GRADLE\n\n\n                       To use gradle, you need to install it manually. Gradle does not come packaged\n                       with the Java SDK nor with the Android SDK. There are various ways to install\n                       gradle: either using a built in package manager, or downloading from the web-\n\n                       site gradle.org. Gradle is in constant flux as new versions are released, and I\n                       found the easiest way for me to install it was to download into a directory\n\n                       called “bin” and unzip the files there. Then, I could specify exactly the version\n\n\n\n\n       166","initials":"ew"},"\"There is a lot here that will be a mystery to a novice Android developer and it maybe worth explaining.\"":{"page":168,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                          You may have complicated feelings about XML files (I know I do), but the An-\n                       droid layout XML files are a straightforward way to design layouts declaratively,\n                       and many GUI tools provide sophisticated ways to manage them. We’ll manage\n\n                       ours by hand as they are exceedingly simple.\n\n\n                       Preparing our application for Calabash testing\n\n\n                       Calabash requires the internet permission added to your AndroidManifest.xml\n                       file. Calabash is a set of technologies combined together to permit testing. One\n\n                       of these pieces is a wrapper around your application (built on Robotium) that\n                       communicates with Ruby over HTTP calls, and as such, your application must\n\n                       permit network communication. To enable this, edit the        AndroidMani-\n                       fest.xml  file to have the internet permission (look for the line labled uses-\n\n                       permission):\n\n                          <?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n                          <manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n                                package=\"com.whereimat\"\n                                android:versionCode=\"1\"\n                                android:versionName=\"1.0\">\n                              <application android:label=\"@string/app_name\"\n\n                                            android:icon=\"@drawable/ic_launcher\">\n                                <uses-permission android:name=\"android.permission.INTERNET\" />\n                                  <activity android:name=\".MainActivity\"\n                                             android:label=\"@string/app_name\">\n                                       <intent-filter>\n\n                                           <action android:name=\"android.intent.action.MAIN\" />\n                                           <category android:name=\"android.intent.category.LAUNCHER\" />\n                                       </intent-filter>\n                                  </activity>\n                              </application>\n\n                          </manifest>\n\n\n                       Writing tests\n\n\n                       Practicing test driven development, we write tests for our application before\n                       writing the code. There are many options for writing tests on Java and Android.\n                       JUnit is a popular testing tool which permits writing unit tests. Robotium is an-\n\n                       other testing tool which focuses on adifferent aspect of testing, user interface\n                       tests. We’ll use a wrapper around Robotium called Calabash for Android which\n\n                       allows us to write in a high level domain specific language. I find that writing\n                       Calabash tests is a simpler way to write tests using APIs because Calabash tests\n\n                       interact with the entire application, rather than only the internals like unit test-\n                       ing. With unit testing you can be required to mock out network interactions,\n\n\n\n\n       168","initials":"ew"},"\"Seems odd that TDD is mentioned here but there are no tests from earlier in this book\"":{"page":168,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                          You may have complicated feelings about XML files (I know I do), but the An-\n                       droid layout XML files are a straightforward way to design layouts declaratively,\n                       and many GUI tools provide sophisticated ways to manage them. We’ll manage\n\n                       ours by hand as they are exceedingly simple.\n\n\n                       Preparing our application for Calabash testing\n\n\n                       Calabash requires the internet permission added to your AndroidManifest.xml\n                       file. Calabash is a set of technologies combined together to permit testing. One\n\n                       of these pieces is a wrapper around your application (built on Robotium) that\n                       communicates with Ruby over HTTP calls, and as such, your application must\n\n                       permit network communication. To enable this, edit the        AndroidMani-\n                       fest.xml  file to have the internet permission (look for the line labled uses-\n\n                       permission):\n\n                          <?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n                          <manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\n                                package=\"com.whereimat\"\n                                android:versionCode=\"1\"\n                                android:versionName=\"1.0\">\n                              <application android:label=\"@string/app_name\"\n\n                                            android:icon=\"@drawable/ic_launcher\">\n                                <uses-permission android:name=\"android.permission.INTERNET\" />\n                                  <activity android:name=\".MainActivity\"\n                                             android:label=\"@string/app_name\">\n                                       <intent-filter>\n\n                                           <action android:name=\"android.intent.action.MAIN\" />\n                                           <category android:name=\"android.intent.category.LAUNCHER\" />\n                                       </intent-filter>\n                                  </activity>\n                              </application>\n\n                          </manifest>\n\n\n                       Writing tests\n\n\n                       Practicing test driven development, we write tests for our application before\n                       writing the code. There are many options for writing tests on Java and Android.\n                       JUnit is a popular testing tool which permits writing unit tests. Robotium is an-\n\n                       other testing tool which focuses on adifferent aspect of testing, user interface\n                       tests. We’ll use a wrapper around Robotium called Calabash for Android which\n\n                       allows us to write in a high level domain specific language. I find that writing\n                       Calabash tests is a simpler way to write tests using APIs because Calabash tests\n\n                       interact with the entire application, rather than only the internals like unit test-\n                       ing. With unit testing you can be required to mock out network interactions,\n\n\n\n\n       168","initials":"ew"},"\"Could all this information be better presented as a table of features? Or a bullet point list perhaps?\"":{"page":169,"text":"                                                                                         Prerequisites\n\n\nand as such, often miss subtle changes in APIs if your mocks are not synchron-\nized with the API itself, a cumbersome and error prone process. Calabash uses a\nsimple DSL for writing tests which is readable and elegant. Most importantly,\n\nCalabash scripts are not compiled, so refactoring and changing tests does not\nrequire the code and compile loop involved in writing tests using pure Java\n\nwith JUnit.\n   Calabash also has a console mode which allows you to interactively refine\n\nyour tests. You jump into a console and query a running application using sim-\nple ruby commands. This is a powerful way to experiement with the calabash\nruby API and allows you to build tests quickly once you have determined the\n\ncorrect code to use.\n   Calabash makes testing easy; your code can be complicated, but tests\n\nshould not be an onerous task. Calabash test scripts do require more overhead\nand take longer to run because they are instantiating and running a new app\nfor each test (unlike unit tests which can isolate a test to a small piece of code),\n\nbut you can mitigate the impact of this on your development flow by using con-\ntinuous integration tools or using a service like AppThwack.com to run tests in\n\nthe cloud.\n   Calabash runs using ruby. You already have ruby installed, so to install cala-\n\nbash, run these commands:\n\n   $ printf \"source 'https://rubygems.org'\\n\\ngem 'calabash-android', '0.4.20'\\ngem 'httparty'\" >> Gemfile\n   $ bundle install\n\n   $ calabash-android gen\n\n   The Gemfile  you just created should now look like this:\n\n\n   source 'https://rubygems.org'\n\n   gem 'calabash-android', '0.4.20'\n\n   gem \"httparty\"\n\n   We’ve also now installed calabash and created the folder structure to hold\n\nour tests along with some helper scripts. The calabash-android gen      com-\nmand will write out a default calabash feature file. This is boilerplate which we\n\nshould change, so make the file named    features/my_first.feature       look\nlike this:\n\n\n   Feature: Login and post\n\n     Scenario: As a valid user I can log into my app and post to my blog\n       When I enter the username\n\n       And I enter the password\n       And I press button number 1\n\n\n\n\n                                                                                         169","initials":"ew"},"\"This code is very confusing, it feels overcomplicated. Why can't the mood be chosen first and then the title string be constructed directly? Rather than hacking it together with unnecessary regex\"":{"page":172,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                      lishes the correct filename format for a Jekyll blog post (we’ll do this same for-\n                      matting in our Java code later). Then we write a method which uses the Cala-\n\n                      bash Ruby API to verify a UI element exists by the ID (these are the IDs created\n                      inside our XML layout files), and if so, sets the field to the text provided. After\n                      this our steps are very basic and uniform except for the last item. Our last item\n\n                      verifies that we successfully stored the data inside our GitHub repository by\n                      making a basic HTTP call using the Httparty ruby gem and then looking inside\n\n                      the retrieved content to make sure it matches the mood we saved earlier.\n\n\n                         require 'calabash-android/calabash_steps'\n                         require 'httparty'\n\n                         def set_title_and_mood\n                           moods = %w{ happy sad angry blue energized }\n\n                           @mood = \"Feeling #{moods[(rand()*moods.length).to_i]} today\"\n                           @title = @mood.downcase.strip.gsub(' ', '-').gsub(/[^\\w-]/, '')\n                           date = (ENV['date'] ? Time.parse(ENV['date']) : Time.now).strftime('%Y-%m-%d')\n                           @filename = \"_posts/#{date}-#{@title}.md\"\n                         end\n\n\n                         def check_and_set( id, text )\n                           check_element_exists \"edittext id:'#{id}'\"\n                           query \"edittext id:'#{id}'\", :setText => text\n\n                         end\n\n                         When(/^I enter the username$/) do\n                           check_and_set( \"username\", ENV['GH_USERNAME'] )\n                         end\n\n\n                         When(/^I enter the password$/) do\n                           check_and_set( \"password\", ENV['GH_PASSWORD'] )\n                         end\n\n\n                         Then(/^I choose my blog$/) do\n                           check_and_set( \"repository\", ENV['GH_REPO'] )\n                         end\n\n\n                         Then(/^I enter my current mood status$/) do\n                           set_title_and_mood()\n                           check_and_set( \"post\", @mood )\n                         end\n\n\n                         And(/^I have a new jekyll post with my mood status$/) do\n                           url = \"https://raw.githubusercontent.com/#{ENV['GH_USERNAME']}/#{ENV['GH_REPO']}/#{ENV['GH_BRANCH']||'master'}/#{@filename}\"\n                           puts \"Checking #{url} for content...\"\n                           response = HTTParty.get( url )\n\n                           assert( response.body.include?( @mood ), \"Post unsuccessful\" )\n                         end\n\n\n\n\n       172","initials":"ew"},"\"The code here looks cut off\"":{"page":172,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                      lishes the correct filename format for a Jekyll blog post (we’ll do this same for-\n                      matting in our Java code later). Then we write a method which uses the Cala-\n\n                      bash Ruby API to verify a UI element exists by the ID (these are the IDs created\n                      inside our XML layout files), and if so, sets the field to the text provided. After\n                      this our steps are very basic and uniform except for the last item. Our last item\n\n                      verifies that we successfully stored the data inside our GitHub repository by\n                      making a basic HTTP call using the Httparty ruby gem and then looking inside\n\n                      the retrieved content to make sure it matches the mood we saved earlier.\n\n\n                         require 'calabash-android/calabash_steps'\n                         require 'httparty'\n\n                         def set_title_and_mood\n                           moods = %w{ happy sad angry blue energized }\n\n                           @mood = \"Feeling #{moods[(rand()*moods.length).to_i]} today\"\n                           @title = @mood.downcase.strip.gsub(' ', '-').gsub(/[^\\w-]/, '')\n                           date = (ENV['date'] ? Time.parse(ENV['date']) : Time.now).strftime('%Y-%m-%d')\n                           @filename = \"_posts/#{date}-#{@title}.md\"\n                         end\n\n\n                         def check_and_set( id, text )\n                           check_element_exists \"edittext id:'#{id}'\"\n                           query \"edittext id:'#{id}'\", :setText => text\n\n                         end\n\n                         When(/^I enter the username$/) do\n                           check_and_set( \"username\", ENV['GH_USERNAME'] )\n                         end\n\n\n                         When(/^I enter the password$/) do\n                           check_and_set( \"password\", ENV['GH_PASSWORD'] )\n                         end\n\n\n                         Then(/^I choose my blog$/) do\n                           check_and_set( \"repository\", ENV['GH_REPO'] )\n                         end\n\n\n                         Then(/^I enter my current mood status$/) do\n                           set_title_and_mood()\n                           check_and_set( \"post\", @mood )\n                         end\n\n\n                         And(/^I have a new jekyll post with my mood status$/) do\n                           url = \"https://raw.githubusercontent.com/#{ENV['GH_USERNAME']}/#{ENV['GH_REPO']}/#{ENV['GH_BRANCH']||'master'}/#{@filename}\"\n                           puts \"Checking #{url} for content...\"\n                           response = HTTParty.get( url )\n\n                           assert( response.body.include?( @mood ), \"Post unsuccessful\" )\n                         end\n\n\n\n\n       172","initials":"ew"},"\"Again the font here is very small\"":{"page":173,"text":"                                                                                      Prerequisites\n\n\n   Then we run from the command line using this command  GH_USER=foobar\n\nGH_PASS=barfoo       GH_REPO=mytestblog        calabash-android       run\nbuild/apk/ghru-release-unsigned.apk       . Our tests will still fail to pass, but\n\nnow we are establishing a baseline success story for the real functionality of our\nfuture app.\n\n\n\n\n                                                                              FIGURE 8-4\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Login Screen\n\n\nSo, let’s start building our application. Obviously we need to put a username\nand password field into our application. Jumping into our XML layout files and\n\nediting gives us this file:\n\n   <?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n   <LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n       android:orientation=\"vertical\"\n       android:layout_width=\"fill_parent\"\n       android:layout_height=\"fill_parent\"\n       >\n\n   <TextView\n       android:layout_width=\"fill_parent\"\n       android:layout_height=\"wrap_content\"\n       android:text=\"GitHub Username:\"\n       />\n\n   <EditText\n       android:layout_width=\"fill_parent\"\n       android:layout_height=\"wrap_content\"\n       android:id=\"@+id/username\"\n       />\n\n\n   <TextView\n       android:layout_width=\"fill_parent\"\n       android:layout_height=\"wrap_content\"\n       android:text=\"GitHub Password:\"\n       />\n\n\n\n\n\n                                                                                      173","initials":"ew"},"\"Possibly be nice to show the reader what layout we are aiming for before showing this wall of XML\"":{"page":175,"text":"                                                                                     Prerequisites\n\n\n         />\n\n     <EditText\n         android:gravity=\"top\"\n\n         android:layout_width=\"fill_parent\"\n         android:layout_height=\"fill_parent\"\n         android:hint=\"Enter your blog post\"\n         android:id=\"@+id/post\"\n         android:layout_weight=\"1\"\n\n         />\n\n     <Button\n         android:layout_width=\"fill_parent\"\n\n         android:layout_height=\"wrap_content\"\n         android:layout_weight=\"0\"\n         android:id=\"@+id/submit\"\n         android:text=\"Send blog post\"/>\n\n\n     <TextView\n         android:layout_width=\"fill_parent\"\n         android:layout_height=\"wrap_content\"\n         android:id=\"@+id/post_status\"\n         android:layout_weight=\"0\"\n\n         android:text=\"\"/>\n\n   </LinearLayout>\n\n\n   Our MainActivity   now can implement the functionality to use these two\nlayouts.\n\n\n   package com.githubru;\n\n   import android.app.Activity;\n\n   import android.os.Bundle;\n   import android.widget.Button;\n   import android.widget.LinearLayout;\n   import android.widget.EditText;\n\n   import android.widget.TextView;\n   import android.view.View;\n\n   public class MainActivity extends Activity\n   {\n\n       /** Called when the activity is first created. */\n       @Override\n       public void onCreate(Bundle savedInstanceState)\n       {\n           super.onCreate(savedInstanceState);\n\n           setContentView(R.layout.main);\n\n\n\n\n\n                                                                                     175","initials":"ew"},"\"Don't need to cast the result of toString to a String\"":{"page":178,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                          Our tests will pass completely right now except for the final test which\n                       checks GitHub to verify a file was correctly posted. We can now proceed to writ-\n                       ing code to login to GitHub and write a file into our Jekyll repository.\n\n\n\n                       Code to Login to GitHub\n\n\n                       Let’s first work on thlogin()  method. Poking into the Egit libary reference\n                       (https://github.com/eclipse/egit-github/tree/master/\n                       org.eclipse.egit.github.core:), we can write GitHub login code that is as simple\n\n                       as the following.\n\n\n                          //Basic authentication\n                          GitHubClient client = new GitHubClient();\n                          client.setCredentials(\"user\", \"passw0rd\");\n\n\n                          The context in which the code runs makes as much a difference as the code.\n                       Android requires that any code which makes network connections run inside a\n                       background thread. Android applications, in order to maintain responsive UI\n\n                       behavior, disallow any long running processes (or indeterminate processes, like\n                       network activity) from running on the main UI thread. If your eyes are starting\n\n                       to spin at the thought of learning about threading using Java, dispell your wor-\n                       ries. The Android SDK provides a great class for managing background thread\n\n                       code calledAsyncTask   . We implement a class which supports this interface by\n                       overriding at least one method which runs our background thread code (called\n\n                       doInBackground()    ).\n\n                          ...\n                          public class MainActivity extends Activity\n\n                          {\n                              /** Called when the activity is first created. */\n                              @Override\n                              public void onCreate(Bundle savedInstanceState)\n                              {\n\n                                  super.onCreate(savedInstanceState);\n                                  setContentView(R.layout.main);\n\n                                  Button login = (Button)findViewById( R.id.login );\n                                  login.setOnClickListener(new View.OnClickListener() {\n\n                                           public void onClick(View v) {\n                                                EditText utv = (EditText)findViewById( R.id.username );\n                                                EditText ptv = (EditText)findViewById( R.id.password );\n                                                String username = (String)utv.getText().toString();\n\n                                                String password = (String)ptv.getText().toString();\n                                                TextView status = (TextView)findViewById( R.id.login_status );\n                                                status.setText( \"Logging in, please wait...\" );\n\n\n\n\n       178","initials":"ew"},"\"Swallowing the exception is a bad idea, consider logging?\"":{"page":179,"text":"                                                                                  Prerequisites\n\n\n                     new LoginTask().execute( username, password );\n                 }\n             });\n    }\n\n\n    private void loggedIn() {\n\n        setContentView(R.layout.logged_in);\n\n\n        Button submit = (Button)findViewById( R.id.submit );\n        submit.setOnClickListener(new View.OnClickListener() {\n                 public void onClick(View v) {\n                     doPost();\n\n                 }\n             });\n    }\n\n\n    class LoginTask extends AsyncTask<String, Void, Boolean> {\n        @Override\n             protected Boolean doInBackground(String... credentials) {\n             boolean rv = false;\n             UserService us = new UserService();\n\n             us.getClient().setCredentials( credentials[0], credentials[1] );\n             try {\n                 User user = us.getUser( credentials[0] );\n                 rv = null != user;\n\n             }\n             catch( IOException ioe ) {}\n             return rv;\n        }\n\n\n        @Override\n             protected void onPostExecute(Boolean result) {\n             if( result ) {\n                 loggedIn();\n\n             }\n             else {\n                 TextView status = (TextView)findViewById( R.id.login_status );\n                 status.setText( \"Invalid login, please check credentials\" );\n\n             }\n        }\n    }\n\n    private void doPost() {\n\n        TextView tv = (TextView)findViewById( R.id.post_status );\n        tv.setText( \"Successful jekyll post\" );\n    }\n\n...\n\n\n\n\n\n                                                                                  179","initials":"ew"},"\"This is a lot to cover, consider breaking it up?\"":{"page":180,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                           We’ve now implemented the login functionality.\n\n                           We retrieve the username and password from our UI elements.\n\n\n                           Our UI should notify the user that a login is occurring in a background task,\n                           so we grab the status text element and update the text in it.\n\n\n                           We then start the background thread process to do our login. This syntax\n\n                           creates a new thread for us with the username and password as parameters.\n                           Android will manage the lifecycle of this thread for us, meaning starting a\n                           new thread, separate from the main UI thread.\n\n\n                           Here we define the derived AsyncTask class. The three types in the generics\n\n                           signature provide a way to parameterize our instantiated task; we need to\n                           provide a username and password to the background task, and the first type\n                           in the signature allows us to pass an array of Strings. You can see in the ac-\n\n                           tual method definition that the ellipsis notation provides a way to parame-\n                           terize a method with a variable number of arguments (called varargs). Inside\n\n                           our defined method we expect we will send two Strings in, and we make\n                           sure to do that in our call.\n\n\n                           Once inside the  doInBackground()      function, we instantiate a UserSer-\n                           vice  class, a wrapper around the GitHub API which interacts with the user\n\n                           service API call. In order to access this information, we have to retrieve the\n                           client for this service call and provide the client with the username and pass-\n\n                           word credentials. This is the syntax to do that.\n\n\n                           We wrap the call to getUser()    in a try block as the function signature can\n                           throw an error (if the network were down, for example). We don’t really need\n                           to retrieve information about the user using the User object, but this call\n\n                           verifies that our username and password are correct and we store the result\n                           of the call in our return value. GitHub will not use the credentials you set un-\n\n                           til you make an API call, so we need to use our credentials to access some-\n                           thing in order to verify those credentials work.\n\n\n                           We renamed the    login()  function to more accurately reflect the fact that\n                           when we call this, we are already logged into GitHub.\n\n\n                           If our login was a failure, either because of network failure, or because our\n\n                           credentials were incorrect, we indicate this in the status message. A user can\n                           retry if they wish.\n\n\n\n\n\n\n        180","initials":"ew"},"\"Seems odd to reference another Git book from within a Git book!\"":{"page":181,"text":"                                                                                           Prerequisites\n\n\n   This code will not compile yet, because we need to import the support\nclasses. The JARs and classes for Egit have already been added to our project\n\nautomatically using gradle. Make sure you add these  import  statements to the\ntop of the file, under the other imports.\n\n\n   ...\n   import android.view.View;\n   import android.os.AsyncTask;\n   import org.eclipse.egit.github.core.service.UserService;\n   import org.eclipse.egit.github.core.User;\n\n   import java.io.IOException;\n   ...\n\n\nCode to talk to GitHub\n\n\nOur last step is to write the code which handles putting content into GitHub.\nThis is not a simple function, because the GitHub API requires you build out the\n\nstructure used internally by Git. A great reference for learning more about this\nstructure is the free and open source book called “Pro Git” and specifically the\nlast chapter called “Git Internals”:http://git-scm.com/book/en/Git-Internals. In\n\na nutshell, the GitHub API expects you to create a git “tree” and then place a\n“blob” object into that tree. You then wrap the tree in a “commit” object and\n\nthen create that commit on GitHub using a data service wrapper. In addition,\nwriting a tree into GitHub requires knowing the base SHA identifier, so you’ll see\n\ncode which retrieves the last SHA in the tree associated with our current\nbranch. This code will work regardless of whether we are pushing code into the\nmaster branch, or into the gh-pages branch, so this utility class works with real\n\nJekyll blogs. It would be lovely if the GitHub API provided more “porcelain” (the\nGit term for user friendly verbs that insulate you from knowing the internals of\n\nGit) instead of only this “plumbing” API, but having the API work like this does\ngive you full control over manipulating your repository and data programmati-\n\ncally in any way that you could possibly need to as it maps exactly to the capa-\nbilities you would have writing to a file stored in a local git repository on your\nhard drive.\n\n   We’ll write a helper class called GitHubHelper    and add a single method\nwhich writes a file to our repository.\n\n   The GitHub API requires that files written into repositories be Base64 enco-\nded. The Apache Foundation provides a suite of tools published to Maven (the\n\nsame  software repository where we grabbed the egit libraries) which can do\nthis encoding for us. To add this library to our project, we need to add to our\n\ndependencies inside our  build.gradle   file:\n\n\n\n\n\n\n                                                                                           181","initials":"ew"},"\"The error handling here is a bit obtuse\"":{"page":183,"text":"                                                                                 Writing the blog content\n\n\n                rv = true;\n            }\n            catch( IOException ieo ) {\n                ieo.printStackTrace();\n            }\n\n\n            return rv;\n       }\n\n\n\n       String blobSha;\n       Tree newTree;\n   ...\n\n   This class hides the details of the GitHub API, and the specifics of writing\n\nfiles to Jekyll repositories. We start by providing a constructor with our login\nand password. Then, we implement a method called SaveFile which takes the\n\nrepository name and the post contents. From here, we work to build the proper\nstructure for creating a new Jekyll post.\n\n\n\nWriting the blog content\n\n\nThe following code snippet shows functions defined to generate the content\nwhich we will place into our remote git repository stored on GitHub.\n\n   We define several instance variables which store data we will use later in\nmethod calls; data like the SHA hash for our blob, the tree into which we will\n\nplace our commit, and strings which are used when creating the commit.\nThough not typical of most Java class definitions which place all member vari-\nables at the top of the class, placing them right above the methods which load\n\ndata into them makes it easier to explain their relevance, so we do that for all\nvariables used in the following methods.\n\n   Our method generateContent sets a commit message and then creates the\nYAML Front Matter (see the Jekyll chapter for more details on YFM if you need a\nrefresher). We then base64 encode the contents of the blog post itself using a\n\nutility class found inside the Apache Commons library. Contents inside a git\nrepository are stored either as UTF-8 content or base64; we could have used\n\nUTF-8 since this is text content but base64 works losslessly and you can always\nsafely use base64 without concerning yourself about the content.\n\n   Filename generation is a bit more complex because Jekyll repositories re-\nquire a specific format: the date as yyyy-MM-dd, with the title of the post, re-\nplacing all whitespace with hyphens.\n\n\n   ...\n\n\n\n\n\n                                                                                          183","initials":"ew"},"\"Surely we want to see the details of the GitHub API?! This is what we are here to learn?\"":{"page":183,"text":"                                                                                 Writing the blog content\n\n\n                rv = true;\n            }\n            catch( IOException ieo ) {\n                ieo.printStackTrace();\n            }\n\n\n            return rv;\n       }\n\n\n\n       String blobSha;\n       Tree newTree;\n   ...\n\n   This class hides the details of the GitHub API, and the specifics of writing\n\nfiles to Jekyll repositories. We start by providing a constructor with our login\nand password. Then, we implement a method called SaveFile which takes the\n\nrepository name and the post contents. From here, we work to build the proper\nstructure for creating a new Jekyll post.\n\n\n\nWriting the blog content\n\n\nThe following code snippet shows functions defined to generate the content\nwhich we will place into our remote git repository stored on GitHub.\n\n   We define several instance variables which store data we will use later in\nmethod calls; data like the SHA hash for our blob, the tree into which we will\n\nplace our commit, and strings which are used when creating the commit.\nThough not typical of most Java class definitions which place all member vari-\nables at the top of the class, placing them right above the methods which load\n\ndata into them makes it easier to explain their relevance, so we do that for all\nvariables used in the following methods.\n\n   Our method generateContent sets a commit message and then creates the\nYAML Front Matter (see the Jekyll chapter for more details on YFM if you need a\nrefresher). We then base64 encode the contents of the blog post itself using a\n\nutility class found inside the Apache Commons library. Contents inside a git\nrepository are stored either as UTF-8 content or base64; we could have used\n\nUTF-8 since this is text content but base64 works losslessly and you can always\nsafely use base64 without concerning yourself about the content.\n\n   Filename generation is a bit more complex because Jekyll repositories re-\nquire a specific format: the date as yyyy-MM-dd, with the title of the post, re-\nplacing all whitespace with hyphens.\n\n\n   ...\n\n\n\n\n\n                                                                                          183","initials":"ew"},"\"This all looks a bit nonsensical... What exactly are you trying to create as content?\"":{"page":184,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                         String blobSha;\n                         Tree newTree;\n                         String commitMessage;\n                         String postContentsWithYfm;\n                         String contentsBase64;\n\n                         String filename;\n                         String post;\n                         String repoName;\n\n                         private void generateContent() {\n\n                             commitMessage = \"GitHubRu Update\";\n                             postContentsWithYfm = \"---\\nlayout: post\\npublished: true\\n---\\n\\n\" + post;\n                             contentsBase64 = new String( Base64.encodeBase64( postContentsWithYfm.getBytes() ) );\n                             getFilename( post );\n                         }\n\n\n                         private void getFilename( String post ) {\n                             String title = post.substring( 0, post.length() > 30 ? 30 : post.length() );\n                             String jekyllfied = title.toLowerCase().replaceAll( \"\\W+\", \"-\").replaceAll( \"\\W+$\", \"\" );\n                             SimpleDateFormat sdf = new SimpleDateFormat( \"yyyy-MM-dd-\" );\n\n                             String prefix = sdf.format( new Date() );\n                             filename = \"_posts/\" + prefix + jekyllfied + \".md\";\n                         }\n                         ...\n\n\n\n                      Services\n\n\n                      There are several services (wrappers around git protocols) which we need to in-\n                      stantiate. We don’t use them all immediately, but we will need them at various\n\n                      steps during the file save process.createServices   call manages these for\n                      us. Once we have created the three services (repositories, commit, and data)\n\n\n                         ...\n\n                         RepositoryService repositoryService;\n                         CommitService commitService;\n\n                         DataService dataService;\n\n                         private void createServices() throws IOException {\n                             repositoryService = new RepositoryService();\n                             repositoryService.getClient().setCredentials( login, password );\n\n                             commitService = new CommitService();\n                             commitService.getClient().setCredentials( login, password );\n                             dataService = new DataService();\n                             dataService.getClient().setCredentials( login, password );\n                         }\n\n\n                         Repository repository;\n\n\n\n       184","initials":"ew"},"\"The narrative seems to have disappeared, why are we jumping between all these topics?\"":{"page":185,"text":"                                                            The Base SHA from the Repository and Branch\n\n\n   RepositoryBranch theBranch;\n   String baseCommitSha;\n   private String retrieveBaseSha() throws IOException {\n       // get some sha's from current state in git\n       repository = repositoryService.getRepository(login, repoName);\n\n       theBranch = getBranch();\n       return theBranch.getCommit().getSha();\n   }\n\n   ...\n\n\n\nThe Base SHA from the Repository and Branch\n\n\nA git repository is a directed acrylic graph (DAG) and as such, each node in the\ngraph must have a starting point. When we append content to our graph, we\n\nneed to determine the starting point on that graph.retrieveBaseSha   does\nthis: it finds the SHA hash for our starting point, a SHA hash which is functional-\n\nly an address inside our tree. To determine this address, our applications needs\nto have a reference to the repository, and we use the repository service we in-\nstantiated earlier to get this reference. Once we have the repository, we need to\n\nlook inside the correct brancgetBranch  does this for us.\n\n\n   ...\n\n       Repository repository;\n       RepositoryBranch theBranch;\n       String baseCommitSha;\n\n       private String retrieveBaseSha() throws IOException {\n           // get some sha's from current state in git\n           repository = repositoryService.getRepository(login, repoName);\n           theBranch = getBranch();\n\n           return theBranch.getCommit().getSha();\n       }\n\n       public RepositoryBranch getBranch() throws IOException {\n   List<RepositoryBranch> branches = repositoryService.getBranches(repository);\n\n   RepositoryBranch master = null;\n   // Iterate over the branches and find gh-pages or master\n   for( RepositoryBranch i : branches ) {\n       String theName = i.getName().toString();\n       if( theName.equalsIgnoreCase(\"gh-pages\") ) {\n\n           theBranch = i;\n       }\n       else if( theName.equalsIgnoreCase(\"master\") ) {\n           master = i;\n       }\n\n   }\n\n\n\n                                                                                      185","initials":"ew"},"\"Code formatting is a little strange here\"":{"page":185,"text":"                                                            The Base SHA from the Repository and Branch\n\n\n   RepositoryBranch theBranch;\n   String baseCommitSha;\n   private String retrieveBaseSha() throws IOException {\n       // get some sha's from current state in git\n       repository = repositoryService.getRepository(login, repoName);\n\n       theBranch = getBranch();\n       return theBranch.getCommit().getSha();\n   }\n\n   ...\n\n\n\nThe Base SHA from the Repository and Branch\n\n\nA git repository is a directed acrylic graph (DAG) and as such, each node in the\ngraph must have a starting point. When we append content to our graph, we\n\nneed to determine the starting point on that graph.retrieveBaseSha   does\nthis: it finds the SHA hash for our starting point, a SHA hash which is functional-\n\nly an address inside our tree. To determine this address, our applications needs\nto have a reference to the repository, and we use the repository service we in-\nstantiated earlier to get this reference. Once we have the repository, we need to\n\nlook inside the correct brancgetBranch  does this for us.\n\n\n   ...\n\n       Repository repository;\n       RepositoryBranch theBranch;\n       String baseCommitSha;\n\n       private String retrieveBaseSha() throws IOException {\n           // get some sha's from current state in git\n           repository = repositoryService.getRepository(login, repoName);\n           theBranch = getBranch();\n\n           return theBranch.getCommit().getSha();\n       }\n\n       public RepositoryBranch getBranch() throws IOException {\n   List<RepositoryBranch> branches = repositoryService.getBranches(repository);\n\n   RepositoryBranch master = null;\n   // Iterate over the branches and find gh-pages or master\n   for( RepositoryBranch i : branches ) {\n       String theName = i.getName().toString();\n       if( theName.equalsIgnoreCase(\"gh-pages\") ) {\n\n           theBranch = i;\n       }\n       else if( theName.equalsIgnoreCase(\"master\") ) {\n           master = i;\n       }\n\n   }\n\n\n\n                                                                                      185","initials":"ew"},"\"Doesn't getName return a String already?\"":{"page":185,"text":"                                                            The Base SHA from the Repository and Branch\n\n\n   RepositoryBranch theBranch;\n   String baseCommitSha;\n   private String retrieveBaseSha() throws IOException {\n       // get some sha's from current state in git\n       repository = repositoryService.getRepository(login, repoName);\n\n       theBranch = getBranch();\n       return theBranch.getCommit().getSha();\n   }\n\n   ...\n\n\n\nThe Base SHA from the Repository and Branch\n\n\nA git repository is a directed acrylic graph (DAG) and as such, each node in the\ngraph must have a starting point. When we append content to our graph, we\n\nneed to determine the starting point on that graph.retrieveBaseSha   does\nthis: it finds the SHA hash for our starting point, a SHA hash which is functional-\n\nly an address inside our tree. To determine this address, our applications needs\nto have a reference to the repository, and we use the repository service we in-\nstantiated earlier to get this reference. Once we have the repository, we need to\n\nlook inside the correct brancgetBranch  does this for us.\n\n\n   ...\n\n       Repository repository;\n       RepositoryBranch theBranch;\n       String baseCommitSha;\n\n       private String retrieveBaseSha() throws IOException {\n           // get some sha's from current state in git\n           repository = repositoryService.getRepository(login, repoName);\n           theBranch = getBranch();\n\n           return theBranch.getCommit().getSha();\n       }\n\n       public RepositoryBranch getBranch() throws IOException {\n   List<RepositoryBranch> branches = repositoryService.getBranches(repository);\n\n   RepositoryBranch master = null;\n   // Iterate over the branches and find gh-pages or master\n   for( RepositoryBranch i : branches ) {\n       String theName = i.getName().toString();\n       if( theName.equalsIgnoreCase(\"gh-pages\") ) {\n\n           theBranch = i;\n       }\n       else if( theName.equalsIgnoreCase(\"master\") ) {\n           master = i;\n       }\n\n   }\n\n\n\n                                                                                      185","initials":"ew"},"\"More strange formatting\"":{"page":186,"text":"CHAPTER 8: Android and the Git Data API\n\n\n                         if( null == theBranch ) {\n\n                             theBranch = master;\n                         }\n                         return theBranch;\n                             }\n\n                         ...\n\n\n\n                      Creating the Blob\n\n\n                      Contents inside a git repository are stored as blcreateBlob  manages stor-\n                      ing our content as a blob object, and then uses the dataService to store this\n\n                      blob into a repository. Until we have calleddataService.createBlob    , we\n                      have not actually placed the object inside GitHub. Also, remember that blobs\n\n                      are not linked into our DAG by themselves; they need to be associated with our\n                      DAG vis-a-vis a tree and commit object, which we do next.\n\n\n                         ...\n\n                             Blob blob;\n                             Tree baseTree;\n                             private void createBlob() throws IOException {\n\n                         Random random = new Random();\n                         blob = new Blob();\n                         blob.setContent(contentsBase64);\n                         blob.setEncoding(Blob.ENCODING_BASE64);\n                         dataService.createBlob(repository, blob);\n\n                             }\n\n                         ...\n\n\n                      Generating a Tree\n\n\n                      Next, we generate a tree. A tree wraps a blob object and provides basically a\n\n                      path to our object: if you know UNIX file system concepts, you can think of a\n                      tree as the filename path and the blob as an inode object. Our data service\n\n                      manager uses a repository name and a base SHA address, one that we retrieved\n                      earlier, to validate that this is a valid starting point inside our repository. Once\n                      we have a tree, we fill out the necessary tree attributes, like tree type (blob) and\n\n                      and tree mode (blob), and set the SHA from the previously created blob object\n                      along with the size. Then we store the tree into our GitHub account using the\n\n                      data service object.\n\n\n\n\n\n\n       186","initials":"ew"},"\"All of this is very heavy, the steps are somewhat mundane\"":{"page":187,"text":"                                                                                Creating the Commit\n\n\n   ...\n       Tree baseTree;\n       private void generateTree() throws IOException {\n           baseTree = dataService.getTree(repository, baseCommitSha);\n   TreeEntry treeEntry = new TreeEntry();\n\n   treeEntry.setPath( filename );\n   treeEntry.setMode( TreeEntry.MODE_BLOB );\n   treeEntry.setType( TreeEntry.TYPE_BLOB );\n   treeEntry.setSha(blobSha);\n   treeEntry.setSize(blob.getContent().length());\n\n   Collection<TreeEntry> entries = new ArrayList<TreeEntry>();\n   entries.add(treeEntry);\n   newTree = dataService.createTree( repository, entries, baseTree.getSha() );\n       }\n\n\n   ...\n\n\nCreating the Commit\n\n\nWe are getting close to actually finalizing our save. We have created a blob\nwhich stores the actual content, and created a tree which stores the path to the\n\ncontent (more or less), but since git is a version control system, we also need to\nstore information about who wrote this object and why. A commit object stores\n\nthis information. The process should look familiar coming from the previous\nsteps: we create the commit and then add relevant metadata, in this case the\n\ncommit message. The “who” of this commit is inferred from our login: GitHub\nknows that we authenticated and assigns this commit to us on the server side.\nWe then use the data service to create the commit inside our repository in Git-\n\nHub at the correct SHA address.\n\n\n   ...\n   Commit newCommit;\n   private void createCommit() throws IOException {\n       // create commit\n       Commit commit = new Commit();\n\n       commit.setMessage( commitMessage );\n       commit.setTree( newTree );\n       List<Commit> listOfCommits = new ArrayList<Commit>();\n       listOfCommits.add(new Commit().setSha(baseCommitSha));\n       commit.setParents(listOfCommits);\n\n       newCommit = dataService.createCommit(repository, commit);\n   }\n\n   ...\n\n\n\n\n\n\n\n                                                                                      187","initials":"ew"},"\"Why start with these random brackets\"":{"page":188,"text":"CHAPTER 8: Android and the Git Data API\n\n\n\n                      Creating the Resource and Updating the Master\n\n\n                      Finally, we need to adjust “master” or “gh-pages”, the branch from which Git-\n                      Hub will generate your Jekyll blog. Previously, we determined the correct\n                      branch against which to apply our additions. GitHub follows this convention\n\n                      when generating your Jekyll blog, using either master or gh-pages as the check-\n                      out point for retrieving your content and then doing a site rebuild from a work-\n\n                      ing copy there. In our code, we use the commit we created and stored in the\n                      previous code to generate a commit resource, set the URL, and then use our da-\n                      ta service to update the reference inside the repository inside GitHub.\n\n\n                        ...\n                        TypedResource commitResource;\n                        private void createResource() {\n\n                             commitResource = new TypedResource();\n                             commitResource.setSha(newCommit.getSha());\n                             commitResource.setType(TypedResource.TYPE_COMMIT);\n                             commitResource.setUrl(newCommit.getUrl());\n                        }\n\n\n                        private void updateMasterResource() throws IOException {\n                             // get master reference and update it\n                             Reference reference = dataService.getReference(repository, \"heads/\" + theBranch.getName() );\n                             reference.setObject(commitResource);\n\n                             Reference response = dataService.editReference(repository, reference, true) ;\n                        }\n\n                        ...\n\n\n\n                      Implementing Our Final doPost\n\n\n                      Finally, we can now implement thdoPost()  method inside our MainActivi-\n                      tyclass.\n\n\n                        ...\n                             }\n                        }\n\n\n                        private void doPost() {\n                             new PostTask().execute( username, password );\n                        }\n\n                        class PostTask extends AsyncTask<String, Void, Boolean> {\n\n\n                             @Override\n                                 protected Boolean doInBackground(String... credentials) {\n\n\n\n       188","initials":"ew"},"\"You could argue that you're not really doing TDD here as you are writing loads of methods off the back of a single test\"":{"page":189,"text":"                                                                        Implementing Our Final doPost\n\n\n           String login = credentials[0];\n           String password = credentials[1];\n\n           EditText post = (EditText)findViewById( R.id.post );\n           String postContents = post.getText().toString();\n\n\n           EditText repo = (EditText)findViewById( R.id.repository );\n           String repoName = repo.getText().toString();\n\n           GitHubHelper ghh = new GitHubHelper( login, password );\n\n           return ghh.SaveFile( repoName, postContents );\n       }\n\n       @Override\n           protected void onPostExecute(Boolean result) {\n\n           TextView status = (TextView)findViewById( R.id.post_status );\n           if( result ) {\n                status.setText( \"Successful jekyll post\" );\n           }\n           else {\n\n                status.setText( \"Post failed.\" );\n           }\n       }\n   }\n\n\n   Our doPost()  command now does one thing: instantiates a new PostTask.\nAs we are performing network operations, we again create a subclass oAsyn-\n\ncTask  which handles these operations automatically on a background thread.\nWe pass in the username and password which we retrieved earlier along with\n\nthe post contents and the repository name we specified. We’ve isolated our Git-\nHub code into our helper class; our MainActivity class does only the necessary\n\nsteps to retrieve items from UI elements and pass them on to our helper class.\n\n\nPassing our Tests\n\n\nNow that we have fully implemented our Android application, we can run our\ntests.\n\n\n   $ GH_REPO=mytestblog \\\n   GH_USERNAME=myusername \\\n   GH_PASSWORD=mypassword \\\n   bundle exec \\\n\n   calabash-android run build/apk/ghru-release-unsigned.apk\n\n   You’ll see them pass with flying colors this time:\n\n\n\n\n\n\n                                                                                      189","initials":"ew"},"\"This introduction is poor. Too much opinion and unsubstantiated claims. Stick to facts and what exactly you plan to cover.\"":{"page":191,"text":"  JavaScript and the Git Data API                                     9\n\n\n\n\n\n\n\n\n\nGitHub offers libraries which work within other programming languages and\nframeworks, and more often than not, these tools run on a server. Runnig a\nserver means finding and managing a server, often an onerous and expensive\ntask. The GitHub API is accessible purely on the client side, however, if you use\n\nthe JavaScript client library. To demonstrate this library, we will build a single\npage app, hosting everything on GitHub and using the GitHub API through a\npure JavaScript client-side library.\n  JavaScript is the lingua franca of the web, invented in 1995 as a scripting\nlanguage embedded inside the first web browsers. JavaScript has recently\n\nfound its way to the server side as well with NodeJS, an innovative toolkit for\nbuilding high performance network servers. JavaScript is a useful language to\nknow, whether on the server side or strictly client side web development. There\nis a separate GitHub API client for NodeJS which supports a different interface\nand has different considerations because of browser sandbox security issues.\n\nThe client side JavaScript library is, in my humble opinion, a bit more ambi-\ntious because it is effective on the browser, and it opens up some fun integra-\ntions with Firebase as we will discuss later in the chapter.\n\n\n    The main weakness of JavaScript has always been testability. Mainly due\n    to the asynchronous nature of JavaScript, writing tests has never been\n    easy; polling for changes when a callback returns was until recently the\n    best way to test nonlinear code. But, recent toolkits like AngularJS and\n    promise-based libraries have made testing not only easy but elegant as\n    well. Building applications on top of third party services makes testing\n    even more important than it already was, and we’ll make sure to add\n    testing to our application to verify the functionality works as we expect.\n\n  A single page application should fulfill a single simple purpose. It has to be\n\nimportant and direct. It should help us do something important. Logically, this\napplication should help us find good coffee.\n\n\n\n\n\n                                                                            191","initials":"ew"},"\"Valid advice but very random to have it here\"":{"page":193,"text":"                                                                  Building a Coffee Shop Database on GitHub\n\n\n   $ printf \"<html>\\n<body>Hello from CoffeeTe.ch</body>\\n</html>\\n\" > index.html\n   $ git commit -m \"Add starting point index.html\" -a\n   $ git config push.default gh-pages\n\n\n   Notice that we created a new repository, and then created and entered the\ngh-pages branch. We’ll do all our work there. And, by using the git config com-\n\nmand, we specified that we want the default push branch to be gh-pages. This\nallows us to use git push    to push our branch up instead of the longer   git\n\npush origin gh-pages     .\n\n\nMAPPING HOSTNAMES\n\n\nOnce we publish these files into GitHub inside a repository we can connect the\nrepository to a real hostname. There are two steps to take to do this:\n\n    • Add a CNAME file which tells GitHub under which server name this service\n\n      should resolve.\n\n    • Setup DNS records so that the hostname maps to the correct IP address\n      at GitHub.\n\n   Imagine you have the hostname myspecialhostname.com. If you map this\n\nrepository to a subdomain called coffeetech, then you would do something like\nthis.\n\n\n   $ echo 'coffeetech.myspecialhostname.com' > CNAME\n   $ git commit -m \"Added CNAME mapping\" -a\n\n   $ git push\n\n   Remember that you need to wait about ten minutes before GitHub regener-\n\nates their database to establish the connection between your gh-pages site and\nthe mapping on their front end servers. This is only the first time you connect a\n\nrepository to a hostname; you will see subsequent changes almost instantane-\nously.\n\n\n     Generally it takes several hours to even a few days to propagate DNS set-\n     tings out into the wild, so make sure you choose and setup a hostname\n\n     far in advance if your site has to be live by a certain point.\n\n\n   Now we can install the libraries needed for this application.\n\n\n\n\n\n\n\n\n\n\n\n                                                                                           193","initials":"ew"},"\"Seems a bit odd to number these from bottom to top\"":{"page":195,"text":"                                                                Building a Coffee Shop Database on GitHub\n\n\nINSERTING GITHUB DATA USING GITHUB.JS\n\nLet’s add some GitHub data to our HTML file. Change the  index.html   file as\n\nfollows.\n\n   <html>\n\n   <head>\n   <title>CoffeeTe.ch</title>\n   <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n   <link rel=\"stylesheet\" type=\"text/css\" href=\"bootstrap.min.css\"></link>\n   </head>\n\n   <body ng-app=\"coffeetech\">\n   <div class=\"container\" ng-controller=\"GithubCtrl\">\n   {{ repo | json }}\n   </div>\n\n   <script src=\"angular.js\"></script>\n   <script src=\"github.js\"></script>\n   <script src=\"coffeetech.js\"></script>\n   </body>\n   </html>\n\n\n   We added or changed just three lines.\n\n\n   We added a reference to ourcoffeetech.js    file beneath our other JS refer-\n   ences.\n\n\n   We removed our databinding to the   Welcome to CoffeeTech      string and\n\n   replaced it with a binding to the variarepo filtered by the JSON filter.\n\n   Finally, we changed the ng-app  reference to use the module we defined in\n\n   our coffeetech.js   file.\n\n   If you have never used AngularJS before, you are probably thoroughly con-\n\nfused about the  coffeetech.js   file. Before we dive into the syntax, under-\nstand the following features of AngularJS, and then you’ll understand the signif-\n\nicant problems solved by those same features:\n\n    • AngularJS utilizes something called two-way databinding. AngularJS sol-\n      ves the problem you have with building JS apps: marshalling data from\n\n      your JS code into your HTML templates, marshalling data from your AJAX\n      calls into your JS code and then marshalling that into your HTML tem-\n      plates. Marcia, Marcia, Marcia! Enough already: allow AngularJS to do this\n\n      heavy lifting. To use it, we just define a variable on the AngularJS scope,\n      and then place a reference to the scope in our HTML using th{{ }}  da-\n\n      tabinding directives. In this case we set a variable callerepo on our\n\n\n\n\n                                                                                        195","initials":"ew"},"\"Feels odd to start introducing AngularJS concepts now. These subjects aren't trivial and it is questionable whether this book should try to cover them\"":{"page":195,"text":"                                                                Building a Coffee Shop Database on GitHub\n\n\nINSERTING GITHUB DATA USING GITHUB.JS\n\nLet’s add some GitHub data to our HTML file. Change the  index.html   file as\n\nfollows.\n\n   <html>\n\n   <head>\n   <title>CoffeeTe.ch</title>\n   <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n   <link rel=\"stylesheet\" type=\"text/css\" href=\"bootstrap.min.css\"></link>\n   </head>\n\n   <body ng-app=\"coffeetech\">\n   <div class=\"container\" ng-controller=\"GithubCtrl\">\n   {{ repo | json }}\n   </div>\n\n   <script src=\"angular.js\"></script>\n   <script src=\"github.js\"></script>\n   <script src=\"coffeetech.js\"></script>\n   </body>\n   </html>\n\n\n   We added or changed just three lines.\n\n\n   We added a reference to ourcoffeetech.js    file beneath our other JS refer-\n   ences.\n\n\n   We removed our databinding to the   Welcome to CoffeeTech      string and\n\n   replaced it with a binding to the variarepo filtered by the JSON filter.\n\n   Finally, we changed the ng-app  reference to use the module we defined in\n\n   our coffeetech.js   file.\n\n   If you have never used AngularJS before, you are probably thoroughly con-\n\nfused about the  coffeetech.js   file. Before we dive into the syntax, under-\nstand the following features of AngularJS, and then you’ll understand the signif-\n\nicant problems solved by those same features:\n\n    • AngularJS utilizes something called two-way databinding. AngularJS sol-\n      ves the problem you have with building JS apps: marshalling data from\n\n      your JS code into your HTML templates, marshalling data from your AJAX\n      calls into your JS code and then marshalling that into your HTML tem-\n      plates. Marcia, Marcia, Marcia! Enough already: allow AngularJS to do this\n\n      heavy lifting. To use it, we just define a variable on the AngularJS scope,\n      and then place a reference to the scope in our HTML using th{{ }}  da-\n\n      tabinding directives. In this case we set a variable callerepo on our\n\n\n\n\n                                                                                        195","initials":"ew"},"\"If you bound the view to a function that returned the scope variable repo, you could just assign this directly to the scope. No need to call apply/show.\"":{"page":196,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                            scope once we return from the show() method callback in the Github.js\n\n                            API call. Notice we don’t have to do anything to place data inside the\n                            HTML once the   repo.show()   callback has completed other than notify-\n\n                            ing AngularJS that data has changed using the$apply()   method. We on-\n                            ly need to call$apply()   if we are using a third party library that uses\n\n                            callbacks, anything defined within AngularJS is wrapped inside the$ap-\n                            ply()  block.\n\n                           • Inspecting a JS object inside your webpage can be complicated; do you\n                            extract information from the object, put them into <div>s, doing all the\n\n                            marshalling we just realized is a royal pain in the lives of most modern\n                            JavaScript developers? If we are using AngularJS it does not have to be.\n\n                            AngularJS provides a filter which you can apply (using the pipe character)\n                            that produces a pretty printed object in your webpage. You see that with\n\n                            the repo | json    code. json  is a filter AngularJS provides by default.\n                            We’ll use filters later in a powerful way.\n\n                           • Many people see this kind of two way databinding and assume it cannot\n                            be performant, arguing that AngularJS must be polling the JavaScript ob-\n\n                            jects to see changes. Not true! AngularJS is written in a smart way and\n                            only processes and changes the DOM when changes are noticed inside a\n                            digest cycle. If you put all your code properly into your scope, AngularJS\n\n                            will handle tracking changes for you. As we mentioned briefly above, us-\n                            ing a third party library that works with callbacks (like the Github.js li-\n\n                            brary) means you need to notify AngularJS that there has been a change\n                            by manually calling the $apply  function on the$scope  object once you\n                            have completed adding data to the scope inside the callback. Without\n\n                            this, your application would not respond to the change until a digest was\n                            triggered by some other incidental change in the scope.\n\n                           •AngularJS allows you to break application functionality into isolated\n                            components which makes your application more testable. When we call\n\n                            angular.controller     we are creating a controller which allows us to\n                            isolate and encapsulate functionality.\n\n\n                          Now let’s implement the coffeetech.js   file. Create a new file callcof-\n                       feetech.js  in the root of your repository.\n\n\n                          var mod = angular.module( 'coffeetech', [] )\n                          mod.controller( 'GithubCtrl', function( $scope ) {\n                            var github = new Github({} );\n                            var repo = github.getRepo( \"gollum\", \"gollum\" );\n\n                            repo.show( function(err, repo) {\n                              $scope.repo = repo;\n                              $scope.$apply();\n\n\n\n\n       196","initials":"ew"},"\"There is no benefit to showing all of this\"":{"page":197,"text":"                                                                 Building a Coffee Shop Database on GitHub\n\n\n     });\n   })\n\n\n   We create a new Github() object using the constructor. This constructor can\n   take user credentials, but for now, we can just create it without those since\n\n   we are accessing a public repository.\n\n   Once we have our   github   object, we call the method  getRepo()   with a\n\n   owner and a name. This returns our repository object.\n\n\n   To actually load the data for this repository object, we call the show method\n   and pass it a callback which uses the two parameters err and  repo to han-\n\n   dle errors or otherwise provide us with details of the repository specified. In\n   this case we are using the Gollum wiki public repository to display some\n   sample data.\n\n\n   Once we have loaded the repository data, we need to call $apply  to tell An-\n\n   gular a change has occurred to the scope variable.\n\n   So, Github.js handles making the proper request to Github for us, and Angu-\nlarJS handles putting the results into our web page.\n\n   If you load this up in your browser, you will see something like this:\n\n\n\n                                                                                  FIGURE 9-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   Yikes, that is a lot of data. AngularJS’s JSON filter pretty printed it for us, but\n\nthis is a bit too much. Change the HTML to be like this:\n\n   <html>\n\n   <head>\n   <title>CoffeeTe.ch</title>\n\n\n\n\n                                                                                         197","initials":"ew"},"\"Using Angular mocks before you explained what they are?\"":{"page":199,"text":"                                                                  Building a Coffee Shop Database on GitHub\n\n\nmajor difference between accessing data stored inside a Git repository and a\ntraditional database: searchability. Git repositories are great for storing data,\nand GitHub exposes storing data through their API. The GitHub API does sup-\n\nport searching of code, but the GitHub.js library does not expose access to this\npart of the API. So, let’s make sure to design and store the data in a structured\n\nway so that we can search it on the client side.\n   This application allows us to search coffee shops. These coffee shops will be,\n\nfor the most part, in larger cities. If we keep all the data stored as JSON files\nnamed after the city, we can keep data located in a file named after the city,\nand then either use geolocation on the client side to retrieve a set of the data,\n\nor ask the user to choose their city manually.\n   If we look at the Github.js javascript documentation on Github (https://\n\ngithub.com/michael/github:) we can see that there are some options for us to\npull content from a repository. We’ll store a data file in JSON named after the\ncity inside our repository and retrieve this from that repository. It looks like the\n\ncalls we need to use are   github.getRepo( username, reponame )           and\n\nonce we have retrieved the repository,    repo.contents( branch, path,\ncallback )   .\n   Now that we have a bare bones application let’s pause and make sure we are\n\nbuilding something that is future proofed. This means adding tests to our\nproject.\n\n\n\nMaking our App Testable\n\nTesting not only builds better code by making us think clearly about how our\n\ncode will be used from the outside, but makes it easier for an outsider (meaning\nother team members) to use our code. Testing facilitates “social coding.”\n\n   We’ll use a tool callekarma  . Karma simplifies writing JavaScript unit tests.\nWe need to first install the tool, then write a test or two. Karma can easily be\n\ninstalled using NPM (installation of which is documented in the ???).\n\n   $ npm install karma -g\n\n   $ karma init karma.config.js\n   $ wget https://ajax.googleapis.com/ajax/libs/angularjs/1.2.7/angular-mocks.js\n\n   Then, create a file calledkarma.config.js     and enter the following con-\n\ntents:\n\n\n   module.exports = function(config) {\n     config.set({\n       basePath: '',\n       frameworks: ['jasmine'],\n       files: [\n\n\n\n\n                                                                                           199","initials":"ew"},"\"The rest of this config isn't explained, what are all these libraries you plan to use?\"":{"page":200,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                                    'angular.js',\n                                    'fixtures-*.js',\n                                    'angular-mocks.js',\n                                    'firebase-mock.js',\n                                    'github.js',\n\n                                    '*.js'\n                               ],\n                               reporters: ['progress'],\n                               port: 9876,\n                               colors: true,\n\n                               logLevel: config.LOG_INFO,\n                               autoWatch: true,\n                               browsers: ['Chrome'],\n                               captureTimeout: 60000,\n                               singleRun: false\n\n                             });\n                           };\n\n                           This is more or less a default Karma configuration file.\n\n\n                           The files  section specifying the load order of our JavaScript implementa-\n\n                           tions and the test scripts. You can see a few of the files we’ve added above\n                           specified directly and wildcards to cover the remaining files.\n\n\n                           Note also that we’ve specified Chrome as our test browser (so you should\n                           have it installed), which is a safe bet because it works on just about any\n\n                           desktop platform you might be running. Know that you can always choose\n                           Safari or Firefox if you want Karma to test inside those as well. Karma will\n                           start a new instance of each browser specified and run your tests inside a\n\n                           test harness in those browsers.\n\n                           To write the test, let’s clarify what we want our code to do:\n\n                            • When a user first visits the application, we should use the geolocation\n\n                              features of their browser to determine their location.\n                            • Pull a file from our repository which contains general latitude and longi-\n\n                              tude locations of different cities.\n\n                            • Iterate over the list of cities and see if we are within 25 miles of any of the\n                              cities. If so, set the current city to the first match.\n\n                            • If we found a city, load the JSON data file from GitHub\n\n                           We’ll use a ng-init  directive which is the mechanism to tell AngularJS we\n                        want to call the function specified when the controller has finished loading.\n\n                        We’ll call this functiinit  so let’s test it below.\n\n\n\n\n\n\n        200","initials":"ew"},"\"You should introduce the rspec style notation before you dive into a test\"":{"page":201,"text":"                                                            Building a Coffee Shop Database on GitHub\n\n\ndescribe( \"GithubCtrl\", function() {\n    var scope = undefined;\n    var ctrl = undefined;\n    var gh = undefined;\n    var repo = undefined;\n\n    var geo = undefined;\n\n    beforeEach( module( \"coffeetech\" ) );\n\n    beforeEach( inject( function ($controller, $rootScope ) {\n\n             generateMockGeolocationSupport();\n             generateMockRepositorySupport();\n             scope = $rootScope.$new();\n             ctrl = $controller( \"GithubCtrl\",\n\n                { $scope: scope, Github: gh, Geo: geo } );\n         } )\n    );\n\n\n    describe( \"#init\", function() {\n         it( \"should initialize, grabbing current city\", function() {\n             scope.init();\n             expect( geo.getCurrentPosition ).toHaveBeenCalled();\n             expect( gh.getRepo ).toHaveBeenCalled();\n\n             expect( repo.read ).toHaveBeenCalled();\n             expect( scope.cities.length ).toEqual( 2 );\n             expect( scope.city.name ).toEqual( \"portland\" );\n             expect( scope.shops.length ).toEqual( 3 );\n\n         });\n    });\n});\n\n\nFirst we declare our variables. If we did not do this, JavaScript would silently\ndefine them inside the functions the first time the variable is used and we\n\nwould have some strange behavior.\n\n\nWe load our coffeetech   module into our tests using themodule  method\ninside abeforeEach  call, code which is executed before our tests run.\n\n\nWe will be creating two functions which generate the mock objects required\nfor our tests. We’ll discuss these two functions in a bit.\n\n\nOur scope is the object where all our functionality is held, and by creating a\n\nnew scope using the AngularJS utility function$rootScope.$new()    func-\ntion we can then use the scope later to test against the functionality we’ve\n\nimplemented in our actual code.\n\n\n\n\n\n                                                                                   201","initials":"ew"},"\"Having this here is confusing to the reader, deferring explainable to later does not help\"":{"page":201,"text":"                                                            Building a Coffee Shop Database on GitHub\n\n\ndescribe( \"GithubCtrl\", function() {\n    var scope = undefined;\n    var ctrl = undefined;\n    var gh = undefined;\n    var repo = undefined;\n\n    var geo = undefined;\n\n    beforeEach( module( \"coffeetech\" ) );\n\n    beforeEach( inject( function ($controller, $rootScope ) {\n\n             generateMockGeolocationSupport();\n             generateMockRepositorySupport();\n             scope = $rootScope.$new();\n             ctrl = $controller( \"GithubCtrl\",\n\n                { $scope: scope, Github: gh, Geo: geo } );\n         } )\n    );\n\n\n    describe( \"#init\", function() {\n         it( \"should initialize, grabbing current city\", function() {\n             scope.init();\n             expect( geo.getCurrentPosition ).toHaveBeenCalled();\n             expect( gh.getRepo ).toHaveBeenCalled();\n\n             expect( repo.read ).toHaveBeenCalled();\n             expect( scope.cities.length ).toEqual( 2 );\n             expect( scope.city.name ).toEqual( \"portland\" );\n             expect( scope.shops.length ).toEqual( 3 );\n\n         });\n    });\n});\n\n\nFirst we declare our variables. If we did not do this, JavaScript would silently\ndefine them inside the functions the first time the variable is used and we\n\nwould have some strange behavior.\n\n\nWe load our coffeetech   module into our tests using themodule  method\ninside abeforeEach  call, code which is executed before our tests run.\n\n\nWe will be creating two functions which generate the mock objects required\nfor our tests. We’ll discuss these two functions in a bit.\n\n\nOur scope is the object where all our functionality is held, and by creating a\n\nnew scope using the AngularJS utility function$rootScope.$new()    func-\ntion we can then use the scope later to test against the functionality we’ve\n\nimplemented in our actual code.\n\n\n\n\n\n                                                                                   201","initials":"ew"},"\"One could argue that this is not a very good test as it breaks encapsulation: it checks for implementation details.\"":{"page":201,"text":"                                                            Building a Coffee Shop Database on GitHub\n\n\ndescribe( \"GithubCtrl\", function() {\n    var scope = undefined;\n    var ctrl = undefined;\n    var gh = undefined;\n    var repo = undefined;\n\n    var geo = undefined;\n\n    beforeEach( module( \"coffeetech\" ) );\n\n    beforeEach( inject( function ($controller, $rootScope ) {\n\n             generateMockGeolocationSupport();\n             generateMockRepositorySupport();\n             scope = $rootScope.$new();\n             ctrl = $controller( \"GithubCtrl\",\n\n                { $scope: scope, Github: gh, Geo: geo } );\n         } )\n    );\n\n\n    describe( \"#init\", function() {\n         it( \"should initialize, grabbing current city\", function() {\n             scope.init();\n             expect( geo.getCurrentPosition ).toHaveBeenCalled();\n             expect( gh.getRepo ).toHaveBeenCalled();\n\n             expect( repo.read ).toHaveBeenCalled();\n             expect( scope.cities.length ).toEqual( 2 );\n             expect( scope.city.name ).toEqual( \"portland\" );\n             expect( scope.shops.length ).toEqual( 3 );\n\n         });\n    });\n});\n\n\nFirst we declare our variables. If we did not do this, JavaScript would silently\ndefine them inside the functions the first time the variable is used and we\n\nwould have some strange behavior.\n\n\nWe load our coffeetech   module into our tests using themodule  method\ninside abeforeEach  call, code which is executed before our tests run.\n\n\nWe will be creating two functions which generate the mock objects required\nfor our tests. We’ll discuss these two functions in a bit.\n\n\nOur scope is the object where all our functionality is held, and by creating a\n\nnew scope using the AngularJS utility function$rootScope.$new()    func-\ntion we can then use the scope later to test against the functionality we’ve\n\nimplemented in our actual code.\n\n\n\n\n\n                                                                                   201","initials":"ew"},"\"This is a lot to cover for a reader that is potentially new to both AngularJS and Karma\"":{"page":202,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n\n                           We pass in the mocked objects (created by the mocked function calls) as\n                           well as the scope object and instantiate a controller object.\n\n\n                           Now we can write our tests. We create a describe block to group them called\n\n                           #init  as we are testing this function. <7>\n\n                           This JS test file has the boilerplate code used in any AngularJS test. You set-\n                        up the scope and instantiate the controller with that scope, and then can man-\n                        ually call the methods on the scope to simulate interaction with our app. As we\n\n                        are calling into a JavaScript function inside of the Github JS object which uses\n                        an asynchronous callback, we will likely have to wait for an AJAX call to return.\n\n                        Simulating this is difficult in a test, so instead we will create a mock object for\n                        Github and then inject it into ourGithubCtrl   controller. Instead of having our\n\n                        controller make real calls to Github, we can call into our mock object and verify\n                        the correct calls are made. The real meat of of our test is inside thdescribe\n\n                        and it  blocks: we initialize the scope, and then expect that the functions on\n                        our mocked objects will be executed. And, we verify the data is correctly set on\n                        our scope.\n\n                           Specifically, our test does these things:\n\n                            • Calls theinit  function defined in our controller (which will be handled\n\n                              using our ng-init  directive in the HTML).\n                            • Verify that the geolocation service was called.\n\n                            • Verify that we calledgetRepo  on our mocked Github object.\n\n                            • Verify that we called read  on the repo we returned from the     getRepo\n                              call.\n\n                            • Verify that we used the data returned from the read to fill our cities object\n\n                              inside our scope object.\n                            • Verify that we calculated the correct current city as Portland.\n\n                            • Verify that we have loaded the JSON data file for the current city\n\n                           Now we can implement the two mocking functions vital for the test. Put\n\n                        them in between the   beforeEach( module( \"coffeetech\" ) )             line and\n                        the beforeEach( inject( ... ) )         functions to provide proper visibility to\n\n                        Karma.\n\n                           ...\n                           beforeEach( module( \"coffeetech\" ) );\n\n\n                           function generateMockGeolocationSupport( lat, lng ) {\n                               response = ( lat && lng ) ? { coords: { lat: lat, lng: lng } } : { coords: CITIES[0] };\n                               geo = { getCurrentPosition: function( success, failure ) {\n\n\n\n\n        202","initials":"ew"},"\"The code here is very dense and difficult to understand\"":{"page":203,"text":"                                                               Building a Coffee Shop Database on GitHub\n\n\n            success( response );\n       } };\n       spyOn( geo, \"getCurrentPosition\" ).andCallThrough();\n   }\n\n\n   function generateMockRepositorySupport() {\n       repo = { read: function( branch, filename, cb ) {\n            cb( undefined, JSON.stringify( filename == \"cities.json\" ? CITIES : PORTLAND ) );\n       } };\n       spyOn( repo, \"read\" ).andCallThrough();\n\n\n       gh = new Github({});\n       spyOn( gh, \"getRepo\" ).andCallFake( function() {\n            return repo;\n       } );\n\n   }\n\n   beforeEach( inject( function ($controller, $rootScope ) {\n   ...\n\n\n   Now that we have a set of tests, run the test suite from the command line\nand watch them fail.\n\n\n   $ karma start karma.conf.js\n   Chrome 32.0.1700 (Mac OS X 10.9.1) GithubCtrl #init should initialize, grabbing current city FAILED\n            Error: [$injector:modulerr] Failed to instantiate module coffeetech due to:\n            Error: [$injector:nomod] Module 'coffeetech' is not available! You either misspelled the module name or forgot to load it. If re▯gistering a module ensure that you specify the dependencies as the second argument.\n\n   ...\n\n   Once we have failing tests that support the direction of our code, we can\n\nwrite the code to support the tests we have written. First add support fixtures,\ndata files which have test data. Add tfixtures-cities.js    file.\n\n\n   var CITIES = [ { name: \"portland\", latitude: 45, longitude: 45 },\n     { name: \"seattle\", latitude: 47.662613, longitude: -122.323837 } ];\n\n\n   And, thefixtures-portland.js     file.\n\n   var PORTLAND = [ { \"name\" : \"Very Good Coffee Shop\", \"latitude\" : 45.52292, \"longitude\" : -122.643074 },\n\n   { \"name\" : \"Very Bad Coffee Shop\", \"latitude\" : 45.522181, \"longitude\" : -122.63709 },\n   { \"name\" : \"Mediocre Coffee Shop\", \"latitude\" : 45.520437, \"longitude\" : -122.67846 } ]\n\n   Then add the coffeetech.js   file. We’ll focus just on the setup code and the\n\nchanges to the init function for now.\n\n\n   var mod = angular.module( 'coffeetech', [] );\n\n\n\n\n\n                                                                                       203","initials":"ew"},"\"This is a little dry, can you think to present it in another way?\"":{"page":204,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                          mod.factory( 'Github', function() { //\n                              return new Github({});\n                          });\n\n\n                          mod.factory( 'Geo', [ '$window', function( $window ) { //\n                              return $window.navigator.geolocation;\n                          } ] );\n\n\n                          mod.factory( 'Prompt', [ '$window', function( $window ) {\n                              return $window.prompt;\n                          } ] );\n\n                          mod.controller( 'GithubCtrl', [ '$scope', 'Github', 'Geo', 'Prompt', function( $scope, ghs, Geo, Prompt ) {\n\n                              $scope.messages = []\n\n                              $scope.init = function() {\n                                  $scope.getCurrentLocation( function( position ) {\n\n                                       $scope.latitude = position.coords.latitude;\n                                       $scope.longitude = position.coords.longitude;\n                                       $scope.repo = ghs.getRepo( \"xrd\", \"spa.coffeete.ch\" ); //\n                                       $scope.repo.read( \"gh-pages\", \"cities.json\", function(err, data) { //\n\n                                           $scope.cities = JSON.parse( data ); //\n                                           // Determine our current city\n                                           $scope.detectCurrentCity(); //\n\n                                           // If we have a city, get it\n\n                                           if( $scope.city ) {\n                                               $scope.retrieveCity();\n                                           }\n\n\n                                           $scope.$apply(); //\n                                       });\n                                  });\n                              };\n\n                          ...\n\n\n                          We extract the Github library into an AngularJS factory. This allows us to in-\n                          ject our mocked GitHub object inside our tests; if we had placed the GitHub\n\n                          instance creation code inside our controller, we would not have been able to\n                          easily mock it out in our tests.\n\n\n                          We extract the geolocation support into an AngularJS factory. As we did with\n                          the GitHub library mock, we can now inject a fake one into our tests.\n\n\n                          Set the username and repository. If you are putting this into your own repos-\n                          itory, modify this appropriately, but you can use these arguments until you\n\n                          do post this into your own repository.\n\n\n\n\n       204","initials":"ew"},"\"Seems like this should be a map reduce operation\"":{"page":206,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                         $scope.detectCurrentCity = function() {\n                             // Calculate the distance from our current position and use\n                             // this to determine which city we are closest to and within\n                             // 25 miles\n\n                             for( var i = 0; i < $scope.cities.length; i++ ) {\n                                 var dist = $scope.calculateDistance( $scope.latitude,\n                                                                        $scope.longitude,\n                                                                        $scope.cities[i].latitude,\n                                                                        $scope.cities[i].longitude );\n\n                                 if( dist < 25 ) {\n                                     $scope.city = $scope.cities[i];\n                                     break;\n                                 }\n\n                             }\n                         }\n\n                         toRad = function(Value) {\n                             return Value * Math.PI / 180;\n\n                         };\n\n                         $scope.calculateDistance = function( latitude1,\n                                                                longitude1,\n\n                                                                latitude2,\n                                                                longitude2 ) {\n                             R = 6371;\n                             dLatitude = toRad(latitude2 - latitude1);\n                             dLongitude = toRad(longitude2 - longitude1);\n\n                             latitude1 = toRad(latitude1);\n                             latitude2 = toRad(latitude2);\n                             a = Math.sin(dLatitude / 2) * Math.sin(dLatitude / 2) +\n                                 Math.sin(dLongitude / 2) * Math.sin(dLongitude / 2) *\n\n                                 Math.cos(latitude1) * Math.cos(latitude2);\n                             c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));\n                             d = R * c;\n                             return d;\n                         }\n\n\n                         $scope.annotate = function() {\n                             user = Prompt( \"Enter your github username\" )\n                             password = Prompt( \"Enter your github password\" )\n\n                             data = Prompt( \"Enter data to add\" );\n                         };\n\n                         ...\n\n\n                         retrieveCity   retrieves a list of shops in the same way that we retrieved\n\n                         the list of cities.\n\n\n\n\n\n\n       206","initials":"ew"},"\"Feels like this should be explained... the annotations seem to be missing though\"":{"page":206,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                         $scope.detectCurrentCity = function() {\n                             // Calculate the distance from our current position and use\n                             // this to determine which city we are closest to and within\n                             // 25 miles\n\n                             for( var i = 0; i < $scope.cities.length; i++ ) {\n                                 var dist = $scope.calculateDistance( $scope.latitude,\n                                                                        $scope.longitude,\n                                                                        $scope.cities[i].latitude,\n                                                                        $scope.cities[i].longitude );\n\n                                 if( dist < 25 ) {\n                                     $scope.city = $scope.cities[i];\n                                     break;\n                                 }\n\n                             }\n                         }\n\n                         toRad = function(Value) {\n                             return Value * Math.PI / 180;\n\n                         };\n\n                         $scope.calculateDistance = function( latitude1,\n                                                                longitude1,\n\n                                                                latitude2,\n                                                                longitude2 ) {\n                             R = 6371;\n                             dLatitude = toRad(latitude2 - latitude1);\n                             dLongitude = toRad(longitude2 - longitude1);\n\n                             latitude1 = toRad(latitude1);\n                             latitude2 = toRad(latitude2);\n                             a = Math.sin(dLatitude / 2) * Math.sin(dLatitude / 2) +\n                                 Math.sin(dLongitude / 2) * Math.sin(dLongitude / 2) *\n\n                                 Math.cos(latitude1) * Math.cos(latitude2);\n                             c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));\n                             d = R * c;\n                             return d;\n                         }\n\n\n                         $scope.annotate = function() {\n                             user = Prompt( \"Enter your github username\" )\n                             password = Prompt( \"Enter your github password\" )\n\n                             data = Prompt( \"Enter data to add\" );\n                         };\n\n                         ...\n\n\n                         retrieveCity   retrieves a list of shops in the same way that we retrieved\n\n                         the list of cities.\n\n\n\n\n\n\n       206","initials":"ew"},"\"This is all very confusing, we keep jumping to new things\"":{"page":208,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                          ng-repeat   is an AngularJS directive which iterates over an array of items.\n\n                          Here we use it to iterate over the items in oportland.json    file and insert\n                          a snippet of HTML with our data interpolated from each item in the iteration.\n\n\n                          We are now using Bootstrap to establish structure in our HTML. The     col-\n\n                          md-6  class tells Bootstrap to build a column sized at 50% of our 12 column\n                          layout. We setup two adjacent columns this way. And, if we are inside a mo-\n                          bile device, it properly stacks these columns.\n\n\n                          Notice how we bind to data from the JSON file.\n\n\n                       ERRORS ALREADY?\n\n\n                       If you run this in your browser, you will not see the shops for our city displayed.\n                       Something is broken, so let’s investigate. I recommend using the Chrome\n\n                       browser to debug this, but you can use any browser and set of developer tools\n                       you like. For Chrome, right clicking on the page anywhere and selecting “In-\n                       spect Element” at the bottom (or by the keyboard shortcut “F12” or “Ctrl + Shift\n\n                       + I” on Windows or Linux or “Cmd + Opt + I” on Mac ) will bring up the developer\n                       console. Then select the console window. Refresh the browser window, and\n\n                       you’ll see this in the console:\n\n                          Uncaught TypeError: Cannot call method 'select' of undefined\n\n\n                          If you click on the link to the right for github.js, you’ll see this.\n\n\n\n FIGURE 9-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                          You see at the point of error that we are callingselect  on the tree. Select\n                       appears to be a method defined on an underscore character. If you use Java-\n\n\n\n\n        208","initials":"ew"},"\"show the actual website first?\"":{"page":210,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                         [ { \"name\" : \"Very Good Coffee Shop\", \"latitude\" : 45.52292, \"longitude\" : -122.643074, \"information\" : [ \"offers gluten free d▯esserts\", \"free wifi\", \"accepts dogs\" ] },\n                         { \"name\" : \"Very Bad Coffee Shop\", \"latitude\" : 45.522181, \"longitude\" : -122.63709 },\n                         { \"name\" : \"Mediocre Coffee Shop\", \"latitude\" : 45.520437, \"longitude\" : -122.67846 } ]\n\n\n                         Notice that we added an array calleinformation   to our data set. We’ll use\n                      this to allow simple search. Add the search feature toindex.html\n\n\n                         ...\n\n                         <div class=\"container\" ng-controller=\"GithubCtrl\" ng-init=\"init()\">\n\n\n                         <h1>CoffeeTe.ch</h1>\n\n                         <input style=\"width: 20em;\" ng-model=\"search\" placeholder=\"Enter search parameters...\"/>\n\n                         <h3 ng-show=\"city\">Current city: {{city.name}}</h3>\n\n\n                         <div class=\"row=\">\n                         <div class=\"col-md-6\"><h4>Shop Name</h4> </div>\n                         <div class=\"col-md-6\"><h4>Lat/Lng</h4> </div>\n                         </div>\n\n                         <div class=\"row\" ng-repeat=\"shop in shops | filter:search\">\n                         <div class=\"col-md-6\">\n                         {{ shop.name }}\n\n\n                         <div ng-show=\"search\">\n                         <span ng-repeat=\"info in city.information\">\n                         <span class=\"label label-default\">city.data</span>\n                         </span>\n                         </div>\n\n\n                         </div>\n                         <div class=\"col-md-6\">\n                         <a target=\"_map\"\n                            href=\"http://maps.google.com/?q={{shop.latitude}},{{shop.longitude}}\">\n\n                            Open in map ({{shop.latitude}},{{shop.longitude}})\n                         </a>\n                         </div>\n                         ...\n\n\n\n                         We add a search box which binds to thsearch  model in our scope\n\n                         We add a filter on the data to display which searches through all data inside\n\n                         each item in ourshops array.\n\n\n                         If we are searching (the model variabsearch  is defined) then we show the\n                         extra information.\n\n\n\n\n       210","initials":"ew"},"\"It seems very odd to add data via a pull request, especially since we have an Angular site that could write to a database\"":{"page":211,"text":"                                                                Building a Coffee Shop Database on GitHub\n\n\n   We alter our lat/lng information to point to a Google Maps page.\n\n\n   Now if we type in the word gluten  in our search box, we filter out anything\nexcept shops which match that, and we see the information pieces formatted\nas labels underneath the shop name.\n\n\n\n\n                                                                                 FIGURE 9-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nADDING DATA USING PULL REQUESTS\n\n\nNow that we have a functioning application, let’s allow people to add informa-\ntion themselves and help build our database. Just beneath the link to the map\nlink, add a button which will allow us to annotate affeeshop with extra infor-\n\nmation.\n   To add an annotation to our existing data we are going ask users to contrib-\n\nute the “GitHub” way. Users will fork the repository, make a change, and then\nissue a pull-request. We can do all of this from our webapp using the Github.js\n\nlibrary. This requires that we ask the users to login, so we will prompt them for\ntheir username and password, as well as the data they want to annotate.\n\n   The implementation we will use starts with adding an “annotate” button to\nour HTML.\n\n\n   <button ng-click=\"annotate(shop)\">Add factoid</button>\n\n   Let’s add some tests. Add another file called         coffeetech.anno-\n\ntate.spec.js   with these contents:\n\n\n   describe( \"GithubCtrl\", function() {\n\n       var scope = undefined, gh = undefined, repo = undefined, prompter = undefined;\n\n\n       function generateMockPrompt() {\n\n\n\n\n                                                                                         211","initials":"ew"},"\"This code seems unnecessarily confusing, it reads like an abuse of mocks\"":{"page":212,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                                  prompter = { prompt: function() { return \"ABC\" } };\n                                  spyOn( prompter, \"prompt\" ).andCallThrough();\n\n                             }\n\n\n                             var PR_ID = 12345;\n                             function generateMockRepositorySupport() {\n                                  repo = {\n                                      fork: function( cb ) {\n                                          cb( false );\n\n                                      },\n                                      write: function( branch, filename, data, commit_msg, cb ) {\n                                          cb( false );\n                                      },\n                                      createPullRequest: function( pull, cb ) {\n\n                                          cb( false, PR_ID );\n                                      },\n                                      read: function( branch, filename, cb ) {\n                                          cb( undefined, JSON.stringify( filename == \"cities.json\" ? CITIES : PORTLAND ) );\n                                      }\n\n                                  };\n                                  spyOn( repo, \"fork\" ).andCallThrough();\n                                  spyOn( repo, \"write\" ).andCallThrough();\n                                  spyOn( repo, \"createPullRequest\" ).andCallThrough();\n                                  spyOn( repo, \"read\" ).andCallThrough();\n\n\n                                  gh = { getRepo: function() {} };\n                                  spyOn( gh, \"getRepo\" ).andCallFake( function() {\n                                      return repo;\n\n                                  } );\n                                  ghs = { create: function() { return gh; } };\n                             }\n\n                         ...\n\n\n                         It looks similar to our previous tests where we mock out a bunch of items\n                      from the Github.js library.\n\n\n                         We added a mock prompt. We will be prompting the user for username,\n\n                         password and the annotating data, and we will use the native browser\n                         prompt mechanism to do this. If using prompt to gather information from\n                         the user sounds like an ugly way to do it, don’t fret, we’ll find a better way\n\n                         later.\n\n\n                         We added three new methods to our mock Github object:   fork ,write  and\n                         createPullRequest    . We test that these are called.\n\n\n\n\n\n\n       212","initials":"ew"},"\"Code is cut off\"":{"page":276,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n\n                                  it \"should tag the PR on GitHub if the user accepts\", (done) ->\n                                           Handler.accept( responder )\n                                           expect( authenticate ).toHaveBeenCalled()\n\n                                           expect( createComment ).toHaveBeenCalled()\n                                           expect( responder.reply ).toHaveBeenCalled()\n                                           done()\n\n                                  it \"should not tag the PR on GitHub if the user declines\", (done) ->\n\n                                           Handler.decline( responder )\n                                           expect( authenticate ).toHaveBeenCalled()\n                                           expect( createComment ).not.toHaveBeenCalledWith()\n                                           expect( responder.reply ).toHaveBeenCalled()\n\n                                           done()\n\n                                  it \"should decode the URL into a proper message object for the createMessage call\", (done) ->\n                                           url = \"https://github.com/xrd/testing_repository/pull/1\"\n                                           msg = Handler.decodePullRequest( url )\n\n                                           expect( msg.user ).toEqual( \"xrd\" )\n                                           expect( msg.repository ).toEqual( \"testing_repository\" )\n                                           expect( msg.number ).toEqual( \"1\" )\n                                           done()\n\n\n                                  it \"should get the username from the response object\", (done) ->\n                                           res = { username: { name: \"Chris Dawson\" } }\n                                           expect( Handler.getUsernameFromResponse( res ) ).toEqual \"Chris Dawson\"\n                                           done()\n\n\n                          Our tests will fail if we run them now. So, let’s write the code at the end of\n                       our delegator extension. We need code which parses the URL into the appropri-\n\n                       ate structured message object, code to put the reminder into the pull request\n                       comment on GitHub and code which pulls the user out of the response object\n\n                       passed to us. The first two of these are within reach; basic JavaScript and read-\n                       ing the GitHub API binding documentation will get us to these two. The third\n\n                       one requires a little more investigation, so we will leave this as a placeholder\n                       for now.\n\n                          To convert the URL into the object necessary for the createMessage   call,\n                       we just need to split the message into pieces by the slash character, and then\n                       retrieve the correct items by index. We probably could add some additional\n\n                       tests which cover passing in empty strings, or other edge cases, but we’ll leave\n                       it as an exercise to the reader (or you can review the final test cases on the asso-\n\n                       ciated GitHub project page). Our code does not crash in these cases, but it\n                       would be nice to have coverage of our expectations represented in our tests.\n\n\n                          ...\n                          _GITHUB = undefined\n                          _PR_URL = undefined\n\n\n\n\n       276","initials":"ew"},"\"You're explaining a lot here in one go, consider how you might break this down?\"":{"page":214,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                          our app and query for the new repository. If we are using AngularJS, we can\n                          ask it for a mocked and programmatic timeout interface which we can con-\n                          trol inside our tests.\n\n\n                          We generate our mocked GitHub method calls and spies, and we follow that\n\n                          by mocking our prompt calls.\n\n                          As mentioned above, we need to get     $timeout   we can use the injector to\n\n                          retrieve that in this way.\n\n\n                          We create a new describe block to organize our tests, calling it#annotate   .\n                          We then implement one    it function which is the single test we are creating:\n\n                          “annotate a shop.”\n\n                          After setting up the preconditions that our scope object should have a city\n\n                          selected, and creating a shop to annotate, We then call our annotate  meth-\n                          od.\n\n\n                          Once we have called  annotate  , our code should request our our credentials\n\n                          for the GitHub API, and then ask us for the information to use in annotating\n                          the shop. If this were happening in the browser, we would get three prompts.\n                          Our test mocks out prompt, and we should therefore see three calls made to\n\n                          our mocked prompt object. We also validate some state we should see on\n                          the scope object like holding a username and annotation for usage later.\n\n\n                          We should then see the first of our GitHub API calls being made: we should\n                          see GitHub.js issue a requet tofork the repository.\n\n\n                          We should then enter in our waiting state; we will tell the user we are waiting\n\n                          and our UI will use the scopewaiting.state    to notify them of that.\n\n                          Once we have flushed the timeout to simulate completion of the fork, we\n\n                          will then see our code storing the result of the forked repo into the scope.\n                          We’ll also see our other GitHub API calls to actually do the annotation.\n\n\n                          Finally, after everything is done, we should no longer be telling the user they\n                          are in a waiting state.\n\n\n                          If you are still running karma in the background, you’ll see the tests fail with:\n\n                          Chrome 32.0.1700 (Mac OS X 10.9.1) GithubCtrl #annotate should\n\n                          annotate a shop FAILED\n\n\n\n\n\n        214","initials":"ew"},"\"When you run out of annotation numbers, your book is trying to tell you something :)\"":{"page":216,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                                      var newData = JSON.stringify( $scope.shops, stripHashKey, 2 );\n                                      $scope.forkedRepo.write('gh-pages', $scope.city.name + '.json',\n                                                                newData,\n                                                                'Added my quirky information',\n\n                                                                function(err) {\n                                          if( !err ) {\n                                               // Annotate our data using a pull request\n                                               var pull = {\n\n                                                   title: \"Adding quirky information to \" + $scope.shopToAnnotate.name,\n                                                   body: \"Created by :\" + $scope.username,\n                                                   base: \"gh-pages\",\n                                                   head: $scope.username + \":\" + \"gh-pages\"\n                                               };\n\n\n                                               target = gh.getRepo( \"xrd\", \"spa.coffeete.ch\" );\n\n\n                                               target.createPullRequest( pull, function( err, pullRequest ) {\n\n                                                   if( !err ) {\n                                                       $scope.notifyWaiting( \"annotated\", \"Successfully sent annotation request\" );\n\n\n                                                       $timeout( function() { $scope.notifyWaiting( undefined ) }, 5000 );\n\n\n                                                       $scope.$apply();\n                                                   }\n                                               } );\n                                          }\n                                          $scope.$apply();\n\n                                      });\n                                  }\n                                  $scope.$apply();\n                             } );\n\n\n                           ...\n\n\n                         We start by creating our annotation function. As we specified in our tests,\n                         this function takes a shop object, an object into which annotations about\n\n                         the shop are added.\n\n\n                         We prompt the user three times: username and password on GitHub, and\n                         the text they want to annotate. If this seems like a really bad way to do\n                         things, don’t worry, we’ll fix it in a moment.\n\n\n                         We create a new Github object with the username and password provided.\n\n                         We leave it as an exercise of the reader to contend with mistyped or incor-\n                         rect credentials.\n\n\n\n\n\n       216","initials":"ew"},"\"Statements of opinion don't really add much unless you provide some sort of compelling arguments to back if up.\\nHave you researched what other change management tools there are out there for databases?\\nCan you demonstrate how what you have produced is b\\netter than it?\\nCan you justify all the effort required to create and maintain your solution over using any other existing solutions?\"":{"page":219,"text":"                                                                   Building a Coffee Shop Database on GitHub\n\n\nAccepting the user contribution via a pull request\n\n\nWhen someone makes an annotation to a shop, the owner of the original repos-\nitory gets a pull request notification on GitHub.\n\n\n\n                                                                                    FIGURE 9-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   As the owner of this repository and manager of this data, I like managing\ncontributions using pull requests inside GitHub. In my humble opinion, there\n\nare no better tools for managing and reviewing changes of information than\nthose found on GitHub. This is a simple case of adding data and might look like\n\noverkill at this scale. You could imagine, however, that were you to have thou-\nsands of users, making many contributions per day, that all of a sudden you\n\nwould need a complex system for managing, reviewing and accepting changes\nto your data set. GitHub gives you all these tools: diff’ing files, user manage-\nment in case you wanted to delegate review to other people in your organiza-\n\ntion, among many other features GitHub provides. This may not be the most\nobvious way to manage a database of information, but there are compelling\n\nreasons to consider it against a traditional database like Postgresql or Mysql.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                            219","initials":"ew"},"\"Screenshot is hard to read\"":{"page":219,"text":"                                                                   Building a Coffee Shop Database on GitHub\n\n\nAccepting the user contribution via a pull request\n\n\nWhen someone makes an annotation to a shop, the owner of the original repos-\nitory gets a pull request notification on GitHub.\n\n\n\n                                                                                    FIGURE 9-5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   As the owner of this repository and manager of this data, I like managing\ncontributions using pull requests inside GitHub. In my humble opinion, there\n\nare no better tools for managing and reviewing changes of information than\nthose found on GitHub. This is a simple case of adding data and might look like\n\noverkill at this scale. You could imagine, however, that were you to have thou-\nsands of users, making many contributions per day, that all of a sudden you\n\nwould need a complex system for managing, reviewing and accepting changes\nto your data set. GitHub gives you all these tools: diff’ing files, user manage-\nment in case you wanted to delegate review to other people in your organiza-\n\ntion, among many other features GitHub provides. This may not be the most\nobvious way to manage a database of information, but there are compelling\n\nreasons to consider it against a traditional database like Postgresql or Mysql.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                            219","initials":"ew"},"\"Screenshot is again a little hard to read\"":{"page":220,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n\n FIGURE 9-6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           As you can see, we can see a clear diff of the changes our contributor made:\n                        they added an annotation that tells us “no turtles allowed”. We might want to\n\n                        consider a different location the next time we have a date with Morla. The diff is\n                        clear in that the green information is easy to read, which is a benefit we get\n\n                        when we use the    JSON.stringify     function with the third parameter set to\n                        something other than undefined. Unortunately, the first line is only different\n                        because we added a comma to it, but this is still a very readable diff.\n\n\n\n                        A SAFE LOGIN IMPLEMENTATION\n\n                        If I saw this app in the wild and knew nothing about the authors, I would never\n\n                        use it to submit data. The app asks for my GitHub username and password. Ask-\n                        ing for my username and password implicitly asks me to trust the authors of\n                        this application. Trust in this case means that I trust them to not maliciously\n\n                        use my credentials for nefarious purposes, and also asks me to trust that they\n                        are not doing something stupid which would allow an attacker to insert them-\n\n                        selves into the middle of the authentication process and steal my crendentials.\n                        It seems like every day we hear of a break-in at a major internet service; I want\n\n                        to believe that most people are out to do good in the world, so I am less wor-\n                        ried about the provider of such a service maliciously stealing my crendentials,\n\n                        but I am worried about a script kiddie attacking the service for fun and stealing\n                        my credentials. At any rate, I would never use a service which requires me to\n                        give up my username and password to another service, especially one which is\n\n                        as important as GitHub is to me.\n                           So, let’s use oAuth instead and resolve these problems.\n\n                           If we use oAuth, we enter our credentials directly into GitHub. We can take\n                        advantage of 2-factor authentication. Once we have entered our credentials,\n\n\n\n\n        220","initials":"ew"},"\"This detracts from your point about this method being the best\"":{"page":220,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n\n FIGURE 9-6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           As you can see, we can see a clear diff of the changes our contributor made:\n                        they added an annotation that tells us “no turtles allowed”. We might want to\n\n                        consider a different location the next time we have a date with Morla. The diff is\n                        clear in that the green information is easy to read, which is a benefit we get\n\n                        when we use the    JSON.stringify     function with the third parameter set to\n                        something other than undefined. Unortunately, the first line is only different\n                        because we added a comma to it, but this is still a very readable diff.\n\n\n\n                        A SAFE LOGIN IMPLEMENTATION\n\n                        If I saw this app in the wild and knew nothing about the authors, I would never\n\n                        use it to submit data. The app asks for my GitHub username and password. Ask-\n                        ing for my username and password implicitly asks me to trust the authors of\n                        this application. Trust in this case means that I trust them to not maliciously\n\n                        use my credentials for nefarious purposes, and also asks me to trust that they\n                        are not doing something stupid which would allow an attacker to insert them-\n\n                        selves into the middle of the authentication process and steal my crendentials.\n                        It seems like every day we hear of a break-in at a major internet service; I want\n\n                        to believe that most people are out to do good in the world, so I am less wor-\n                        ried about the provider of such a service maliciously stealing my crendentials,\n\n                        but I am worried about a script kiddie attacking the service for fun and stealing\n                        my credentials. At any rate, I would never use a service which requires me to\n                        give up my username and password to another service, especially one which is\n\n                        as important as GitHub is to me.\n                           So, let’s use oAuth instead and resolve these problems.\n\n                           If we use oAuth, we enter our credentials directly into GitHub. We can take\n                        advantage of 2-factor authentication. Once we have entered our credentials,\n\n\n\n\n        220","initials":"ew"},"\"Do we need to cover all of this?\"":{"page":221,"text":"                                                                    Building a Coffee Shop Database on GitHub\n\n\nGitHub decides whether we are who we say we are, and then returns us to the\napplication which requested access.\n\n   GitHub provides the application with what is called an oAuth token that en-\ncapsulates exactly what services on GitHub we have access to, and whether\nthat access is read-only or whether we can add data in a read-write manner.\n\nThis means our requesting service can ask to modify only parts of our data\nwithin GitHub; this provides a much higher level of trust to users as they know\n\nthe application cannot touch the more private parts within GitHub. Specifically,\nthis means we could ask for access only to gists and not request access to our\n\nrepositories. One important point about oAuth tokens is that they can be re-\nvoked. So, once a specific action has been taken, we can destroy the token and\n\nrevoke access. With simple username and password access, the only way to re-\nvoke access is to change the password, which means any place you have saved\nthat password (password managers or other applications which login via user-\n\nname and password) need to update their settings as well. With oAuth we can\nrevoke a single token at any time (and GitHub makes it easy to do this) without\n\naffecting access to other services.\n\n\nAuthentication Requires a Server\n\n\nWe would like to host everything on GitHub, but sadly there is one piece which\nwe cannot host there: the authentication component. Somehow we need to\n\nsafely authenticate our user into GitHub and retrieve an oAuth token. There is\ncurrently no way to do this strictly client side (using only static HTML and Java-\n\nScript running in the browser). Other authentication providers like Facebook do\nprovide pure JavaScript login functionality in their SDKs, but GitHub, citing se-\n\ncurity concerns, has not release anything that does authentication purely on\nthe client side as of yet.\n   Somehow we have to involve a server into our authentication process. The\n\nmost obvious choice we have is to run a small authentication server, delegate\nauthentication to it, and once authentication is completed, jump back in our\n\napplication hosted on GitHub. We provide code (written in NodeJS, JavaScript\nfor the server side) to do this at the end of the xchapter. But, creating even a\n\nsimple authentication system has a baseline of complexity that seems like over-\nkill, and this code is complex and lengthy, and requires figuring out a hosting\n\nprovider. If we could instead delegate this authentication to a third party, we\ncould reduce a massive amount of code and complexity from our system. Let’s\nsee what that looks like.\n\n\n\n\n\n\n\n\n\n\n                                                                                             221","initials":"ew"},"\"Perhaps we just need a list here?\"":{"page":222,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                       FIXING AUTHENTICATION WITH FIREBASE\n\n                       Instead of writing our own server to manage authentication and talk to the Git-\n\n                       Hub API, we will delegate that authentication to Firebase. Firebase is a real time\n                       communication toolset which integrates well with our choice of AngularJS. By\n\n                       far the simplest and safest option, Firebase offers AngularJS bindings (called\n                       “AngularFire”) and an integrated GitHub authentication component (called\n\n                       “Simple Login”). Together they resolve the authentication issue for us, and keep\n                       all our code hosted on GitHub. Delegation of our authentication component is\n                       easy with Firebase: we just modify our existing GitHub application, provide the\n\n                       credentials and GitHub oAuth scope to Firebase, and then our application off-\n                       loads user management to Firebase.\n\n                          First, we need to create a new GitHub application. In the top right corner on\n                       GitHub.com, click on the “Account settings” link, and then navigate to the “Ap-\n\n                       plications” link towards the bottom. Click on the “Developer Applications” tab\n                       in the right center column and then click on the “Register new application” but-\n\n                       ton. Make sure “Authorization callback URL” is set to   https://auth.fire-\n                       base.com/auth/github/callback        . Then save the application by clicking on\n\n                       the “Register application” button.\n\n\n\n FIGURE 9-7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        222","initials":"ew"},"\"Do you need to give an overview of how we are going to test Firebase before we jump into it?\"":{"page":224,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                         var Firebase = function (url) {\n                         }\n\n                         angular.module( 'firebase', [] );\n\n\n                         To test our code, we make the following changes to our    coffeetech-\n\n                      annotate.spec.js   :\n\n                         beforeEach( module( \"coffeetech\" ) );\n\n\n                         var mockFirebase = mockSimpleLogin = undefined;\n                         function generateMockFirebaseSupport() {\n                             mockFirebase = function() {};\n                             mockSimpleLogin = function() {\n\n                                 return {\n                                     '$login': function() {\n                                          return { then: function( cb ) {\n                                              cb( { name: \"someUser\",\n\n                                                    accessToken: \"abcdefghi\" } );\n                                          } };\n                                     }\n                                 }\n                             };\n\n                         }\n\n                         var $timeout;\n                         beforeEach( inject( function ($controller, $rootScope, $injector ) {\n\n                             generateMockRepositorySupport();\n                             generateMockPrompt();\n                             generateMockFirebaseSupport();\n                             $timeout = $injector.get( '$timeout' );\n                             scope = $rootScope.$new();\n\n                             ctrl = $controller( \"GithubCtrl\", { $scope: scope, Github: ghs, '$timeout': $timeout, '$window': prompter, '$firebase': moc▯kFirebase, '$firebaseSimpleLogin': mockSimpleLogin } );\n                         } ) );\n\n\n\n                         describe( \"#annotate\", function() {\n                             it( \"should annotate a shop\", function() {\n                                 scope.auth = mockSimpleLogin( mockFirebase() );\n                                 scope.city = PORTLAND\n\n                                 var shop = { name: \"A coffeeshop\" }\n                                 scope.annotate( shop );\n                                 expect( prompter.prompt.calls.length ).toEqual( 1 );\n                                 expect( scope.shopToAnnotate ).toBeTruthy();\n                                 expect( scope.username ).not.toBeFalsy();\n\n                                 expect( scope.annotation ).not.toBeFalsy();\n\n\n\n\n\n\n\n       224","initials":"ew"},"\"Code is cut off at the edge and it looks incomplete at the bottom\"":{"page":226,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                          mod.controller( 'GithubCtrl', [ '$scope', 'Github', 'Geo', '$window', '$timeout', '$firebase', '$firebaseSimpleLogin', function▯( $scope, ghs, Geo, $window, $timeout, $firebase, $firebaseSimpleLogin ) {\n\n                              $scope.init = function() {\n\n                                  var ref = new Firebase( 'https://coffeetech.firebaseio.com' );\n\n                                  $scope.auth = $firebaseSimpleLogin( ref );\n\n                                  $scope.getCurrentLocation( function( position ) {\n                                       $scope.latitude = position.coords.latitude;\n\n\n                          Then, when we annotate, we need to provide the auth token returned from\n                       Firebase. But, it is gratifying to see that little else needs to change in our flow.\n\n\n                          $scope.annotate = function( shop ) {\n                              $scope.shopToAnnotate = shop;\n\n                              $scope.auth.$login( 'github', { scope: 'repo' } ).then( function( user ) {\n\n\n                                  $scope.me = user;\n                                  $scope.username = user.name;\n\n\n                                  $scope.annotation = $window.prompt( \"Enter data to add\" );\n\n                                  if( $scope.annotation ) {\n                                       gh = ghs.create( $scope.me.accessToken );\n                                       toFork = gh.getRepo( \"xrd\", \"spa.coffeete.ch\" );\n\n                                       toFork.fork( function( err ) {\n\n\n                          We call the$login  method on our auth object created using the Firebase\n                          SimpleLogin service. It returns a “promise” which is an interface that has a\n\n                          then() method, which will be called if th$login()  succeeds. then() calls\n                          our callback function, giving us a user object.\n\n\n                          We still need to prompt the user for one piece of information, the data to an-\n                          notate. You can imagine other ways to get this information, using modal\n\n                          HTML5 dialogs, but this will work for us for right now. At least we are only\n                          prompting once instead of three times!\n\n\n                          Once we are ready to fork we need to create our user object using the token.\n\n                          After we make these changes, we can click the “Add factoid” button and\n\n                       we’ll get a dialog like this one indicating we are logging into GitHub (via the\n                       Firebase SimpleLogin).\n\n\n\n\n\n\n\n\n       226","initials":"ew"},"\"Code looks incomplete here as well\"":{"page":226,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                          mod.controller( 'GithubCtrl', [ '$scope', 'Github', 'Geo', '$window', '$timeout', '$firebase', '$firebaseSimpleLogin', function▯( $scope, ghs, Geo, $window, $timeout, $firebase, $firebaseSimpleLogin ) {\n\n                              $scope.init = function() {\n\n                                  var ref = new Firebase( 'https://coffeetech.firebaseio.com' );\n\n                                  $scope.auth = $firebaseSimpleLogin( ref );\n\n                                  $scope.getCurrentLocation( function( position ) {\n                                       $scope.latitude = position.coords.latitude;\n\n\n                          Then, when we annotate, we need to provide the auth token returned from\n                       Firebase. But, it is gratifying to see that little else needs to change in our flow.\n\n\n                          $scope.annotate = function( shop ) {\n                              $scope.shopToAnnotate = shop;\n\n                              $scope.auth.$login( 'github', { scope: 'repo' } ).then( function( user ) {\n\n\n                                  $scope.me = user;\n                                  $scope.username = user.name;\n\n\n                                  $scope.annotation = $window.prompt( \"Enter data to add\" );\n\n                                  if( $scope.annotation ) {\n                                       gh = ghs.create( $scope.me.accessToken );\n                                       toFork = gh.getRepo( \"xrd\", \"spa.coffeete.ch\" );\n\n                                       toFork.fork( function( err ) {\n\n\n                          We call the$login  method on our auth object created using the Firebase\n                          SimpleLogin service. It returns a “promise” which is an interface that has a\n\n                          then() method, which will be called if th$login()  succeeds. then() calls\n                          our callback function, giving us a user object.\n\n\n                          We still need to prompt the user for one piece of information, the data to an-\n                          notate. You can imagine other ways to get this information, using modal\n\n                          HTML5 dialogs, but this will work for us for right now. At least we are only\n                          prompting once instead of three times!\n\n\n                          Once we are ready to fork we need to create our user object using the token.\n\n                          After we make these changes, we can click the “Add factoid” button and\n\n                       we’ll get a dialog like this one indicating we are logging into GitHub (via the\n                       Firebase SimpleLogin).\n\n\n\n\n\n\n\n\n       226","initials":"ew"},"\"This overview seems a bit quick and feels like it is lacking proper details\"":{"page":227,"text":"                                                                                           Summary\n\n\n                                                                                FIGURE 9-9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   After you authorize the application, the flow is identical to the manually\nusername and password authentication flow. As an optimization we could\n\ncheck for previous logins before callin$login()   again but we don’t do that\nhere, meaning the login dialog is momentarily popped up each time we click\n\nthe button.\n   Once users have logged in, they will be redirected to the application, and\nwe’ll notify them they have submitted a pull request with their contribution.\n\nSince their contribution is associated with their GitHub account, they will re-\nceive standard pull request notifications when their contribution is accepted, so\n\nwe don’t need to implement that ourselves.\n\n\nSummary\n\n\nWe’ve built an application in JavaScript that requires no server and provides\n\nusers with a searchable coffeeshop database that accepts contributions in a\nvery safe and secure way using the Pull Request API. We were able to complete-\n\nly ignore all the administrative features of a data entry system, delegating all\nthese to GitHub. Our single page app permits us to focus on one thing: making a\npowerful and useful application.\n\n   In our final chapter we will use CoffeeScript to create our own chat robot\nthat requests pull request reviews from chat room members using the Activities\n\nAPI.\n\n\n\n\n\n\n                                                                                        227","initials":"ew"},"\"This chapter feels incredibly dense and very forced.\"":{"page":227,"text":"                                                                                           Summary\n\n\n                                                                                FIGURE 9-9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   After you authorize the application, the flow is identical to the manually\nusername and password authentication flow. As an optimization we could\n\ncheck for previous logins before callin$login()   again but we don’t do that\nhere, meaning the login dialog is momentarily popped up each time we click\n\nthe button.\n   Once users have logged in, they will be redirected to the application, and\nwe’ll notify them they have submitted a pull request with their contribution.\n\nSince their contribution is associated with their GitHub account, they will re-\nceive standard pull request notifications when their contribution is accepted, so\n\nwe don’t need to implement that ourselves.\n\n\nSummary\n\n\nWe’ve built an application in JavaScript that requires no server and provides\n\nusers with a searchable coffeeshop database that accepts contributions in a\nvery safe and secure way using the Pull Request API. We were able to complete-\n\nly ignore all the administrative features of a data entry system, delegating all\nthese to GitHub. Our single page app permits us to focus on one thing: making a\npowerful and useful application.\n\n   In our final chapter we will use CoffeeScript to create our own chat robot\nthat requests pull request reviews from chat room members using the Activities\n\nAPI.\n\n\n\n\n\n\n                                                                                        227","initials":"ew"},"\"Very random to have more content after the conclusion\"":{"page":228,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n\n                      Addendum: a NodeJS GitHub Authentication\n                      Service\n\n\n                      If you feel more comfortable using your own authentication server and not us-\n\n                      ing the Firebase option presented above, this addendum to the chapter shows\n                      you how to do this.\n\n                         There are several libraries which offer support for oAuth authentication on\n                      GitHub, but the consistency of NodeJS modules often leaves something to be\n                      desired. As I was building this chapter I experimented with several authentica-\n\n                      tion modules and quickly discovered even though that module might have\n                      been the sanctioned and approved module last year, that it has already been\n\n                      abandoned this year. There is a tendency to build software using the latest li-\n                      braries as you might assume the newest library would have the fewest bugs. In\n                      my experience, NodeJS libraries come with less test coverage than other lan-\n\n                      guage libraries, and for this reason, often have more breaking changes than the\n                      authors would care to admit. Another reason to build testable code.\n\n                         The library I finally settled upon is called Passport, written by Stuart P.\n                      Benchley, which supports a strategy calledpassport-github   written by Jared\n                      Hanson. Both are open source and, of course, hosted on GitHub. We’ll write a\n\n                      simple NodeJS server which allows login via GitHub, and then provides our sin-\n                      gle page application with a token to use when talking to the GitHub API using\n\n                      Github.js.\n\n\n                      Our own NodeJS Application\n\n\n                      Let’s build our app inside a directory calnode . Doing this will keep our code\n                      separate from our client side code and prevent our karma test runner from\n\n                      loading these files as part of test runs.\n                         To build out NodeJS application we will first create the application package\n\n                      manifest (node/package.json    ) which specifies the required pieces and allows\n                      us to runnpm install   to download them all.\n\n\n                         {\n                              \"name\": \"coffeetech-localauth\",\n                              \"description\": \"Sample NodeJS for GitHub Auth\",\n                              \"version\": \"0.0.1\",\n                              \"homepage\": \"http://spa.coffeete.ch\",\n\n                              \"repository\": {\n                                  \"type\": \"git\",\n                                  \"url\": \"git://github.com/xrd/spa.coffeete.ch.git\"\n                              },\n                              \"author\": \"Chris Dawson <xrdawson@gmail.com> (https://github.com/xrd/)\",\n\n                              \"keywords\": [\n\n\n\n       228","initials":"ew"},"\"This seems incomplete and looks like it is supposed to be in the Appendix\"":{"page":232,"text":"CHAPTER 9: JavaScript and the Git Data API\n\n\n                           We do need to alter the GitHub application settings to use this, but fortu-\n                        nately it is only in one place. The “Authorization callback URL” should be set to\n\n                        http://localhost:3000/auth/github/callback             . This is the URL which\n                        our application will tell the GitHub API to redirect us back to after authentica-\n\n                        tion has successfully completed (the passport library automatically provides\n                        this when connecting to GitHub).\n                           We run this app by specifying the github client ID and secret on the com-\n\n                        mand line as environment variables (to avoid checking them into our source\n                        code repository). Your client ID and secret will be different, of course.\n\n\n                           $ GITHUB_CLIENT_ID=1234567890abcdefghijk \\\n                           GITHUB_CLIENT_SECRET=0987654321kmnopqrstuv \\\n\n                           node node/github-local-login.js\n\n                           Though we have something working, there are some significant problems\n\n                        with this approach:\n\n                            • We can run this locally as we are developing and testing, but eventually\n                              we will require a hosting provider, like Heroku or Nodejitsu, to host our\n\n                              application, or we will need to setup a full stack server, virtual or other-\n                              wise, ourselves.\n\n                            • If you did spend time reviewing this code, you’ll see a GET request at\n                              the /token.js    mount point. Once login has completed inside our ser-\n\n                              vice, we return to our JS application, and the web page in which it is hos-\n                              ted adds another script tag which loads the auth token using this access\n\n                              point. This is a consequence of browser security: since the application is\n                              hosted on the GitHub.io domain we cannot make requests using a more\n\n                              secure method like POST to another domain (the domain where our au-\n                              thentication service will be hosted). Unfortunately, this means that we\n                              have opened a security hole, since any other application running inside\n\n                              our browser could do the make the same GET request, hijacking the auth\n                              token.\n\n                            • We could migrate our entire app into this NodeJS application, using\n\n                              something like EJS templates, but then we are losing the fact that our ap-\n                              plication is simply our repository, and the point of this chapter becomes\n                              sadly moot.\n\n                            • This app is not very testable. One of the reasons AngularJS is such a pop-\n\n                              ular framework is that it makes you write JavaScript code which is test-\n                              able. Most NodeJS frameworks don’t make that easy, and I found it very\n\n                              difficult to wrap this code inside of any of the current test frameworks.\n\n\n\n\n\n\n\n        232","initials":"ew"},"\"What? Everyone loves WALL-E!!!!!!!\"":{"page":235,"text":"          CoffeScript, Hubot and the\n\n                                      Activities API                10\n\n\n\n\n\n\n\n\nThough the phrase has now been removed from their marketing materials, Git-\nHub used to call itself a tool for “social coding.” This idea is still central to the\nservices GitHub provides, intimate access to the social layer inside of GitHub\nthrough the Activity API.\n   The Activities API includes:\n\n   • notifications (comments issued to users through various events)\n   • stargazing tools (Facebook has “likes” while GitHub has “stars” to indi-\n\n     cate approval or interest)\n   • watching (a way to track GitHub data)\n   • events (a higher level activity stream useful for following actions of\n\n     users).\n\n    The Activity API section also includes “feeds.” While feeds are grouped\n    within the Activity API, they are not programmatic in the same way that\n    an API is, and we won’t cover them in depth here. Feeds are actually\n    Atom feeds and not interactive beyond that. Atom feeds are similar to\n    RSS feeds: a static feed you can subscribe to with an Atom client.\n\n   In this chapter we’ll investigate the Activity API by extending a chat robot.\n\nYou might find it odd that a robot, generally considered an anti-social invention\ndespite all best attempts, would play nicely with a social API, but this is a social\nrobot. GitHubbers use an extensible chat robot called Hubot to record and au-\ntomate their tasks, and to have fun on the Internet. If there were any robot suit-\ned for interacting with the GitHub Activity API, it’s Hubot, described on the site\nhubot.github.io as “a customizable, kegerator-powered life embetterment ro-\n\nbot.”\n   Hubot is a framework for developing chat robots. For many people, chat is at\nbest a way to waste time and at worst an annoyance or distraction. For GitHub-\n\n\n\n                                                                             235","initials":"ew"},"\"Really random statements\"":{"page":235,"text":"          CoffeScript, Hubot and the\n\n                                      Activities API                10\n\n\n\n\n\n\n\n\nThough the phrase has now been removed from their marketing materials, Git-\nHub used to call itself a tool for “social coding.” This idea is still central to the\nservices GitHub provides, intimate access to the social layer inside of GitHub\nthrough the Activity API.\n   The Activities API includes:\n\n   • notifications (comments issued to users through various events)\n   • stargazing tools (Facebook has “likes” while GitHub has “stars” to indi-\n\n     cate approval or interest)\n   • watching (a way to track GitHub data)\n   • events (a higher level activity stream useful for following actions of\n\n     users).\n\n    The Activity API section also includes “feeds.” While feeds are grouped\n    within the Activity API, they are not programmatic in the same way that\n    an API is, and we won’t cover them in depth here. Feeds are actually\n    Atom feeds and not interactive beyond that. Atom feeds are similar to\n    RSS feeds: a static feed you can subscribe to with an Atom client.\n\n   In this chapter we’ll investigate the Activity API by extending a chat robot.\n\nYou might find it odd that a robot, generally considered an anti-social invention\ndespite all best attempts, would play nicely with a social API, but this is a social\nrobot. GitHubbers use an extensible chat robot called Hubot to record and au-\ntomate their tasks, and to have fun on the Internet. If there were any robot suit-\ned for interacting with the GitHub Activity API, it’s Hubot, described on the site\nhubot.github.io as “a customizable, kegerator-powered life embetterment ro-\n\nbot.”\n   Hubot is a framework for developing chat robots. For many people, chat is at\nbest a way to waste time and at worst an annoyance or distraction. For GitHub-\n\n\n\n                                                                             235","initials":"ew"},"\"This seems like a way of saying: this isn't secure at all\"":{"page":237,"text":"                                                                                               Our Blueprint\n\n\ntil you find the one you want. If there is a developer service your team uses,\nodds are there is a Hubot plugin already available for it. By searching NPM, you\ncould, for example, find plugins for adding or searching tickets in JIRA, Asana\n\nand Pivotal Tracker or ZenDesk, all popular ticket tracking systems used by\nthousands of developer teams worldwide.\n\n   Though you can likely find a plugin to support whatever developer service\nyou search for on npmjs, it might not be perfectly suited to your workflow. So,\n\nlet’s build our own Hubot extension and get exactly what we want for our team\nof developers.\n\n\n\nOur Blueprint\n\n\nWe are going to build an extension to Hubot. When we are done, Hubot will be\ntransformed into a robot that..\n\n    • listens for pull request events from GitHub by subscribing to notifications\n\n      using the GitHub Activities API\n    • invites people in the chat room to comment on those pull requests\n\n    • guarantees that communication between it and GitHub is securely deliv-\n\n      ered (with a caveat)\n    • retrieves vital information from an external service (the Slack.com API)\n\n    • has functionality fully described by automated tests.\n\n    • allows easy simulation of inputs and outputs which map to the inputs\n      and outputs it gets from APIs and services.\n\n    • runs with ease host on a major Paas (Heroku)\n\n   Hubot provides the skeleton for our chat robot. We’ll add the above func-\n\ntionality to Hubot and see how easy it is to combine these features into a coher-\nent whole that solves a real problem.\n   Just so we are clear about the differences between a vanilla Hubot and our\n\nextended Hubot following the above blueprint, let’s give it a name: Probot.\nFrom now on, we will speak about unmodified Hubots by calling them Hubot,\n\nwhile we will call our amplified Hubot with PR delegation super powers the\nname “Probot.”\n\n\nConsiderations and Limitations\n\n\nIf you want stability with your Hubot, you need to host it on a server. Hubot is\n\nwritten in NodeJS and requires a hosting service that supports NodeJS. Our Hu-\nbot needs to sit on a public IP address (not inside the firewall) because we re-\nceive notifications from GitHub. It is not strictly required that you host Hubot\n\n\n\n\n                                                                                               237","initials":"ew"},"\"Good to have a layout of what we are going to cover\"":{"page":237,"text":"                                                                                               Our Blueprint\n\n\ntil you find the one you want. If there is a developer service your team uses,\nodds are there is a Hubot plugin already available for it. By searching NPM, you\ncould, for example, find plugins for adding or searching tickets in JIRA, Asana\n\nand Pivotal Tracker or ZenDesk, all popular ticket tracking systems used by\nthousands of developer teams worldwide.\n\n   Though you can likely find a plugin to support whatever developer service\nyou search for on npmjs, it might not be perfectly suited to your workflow. So,\n\nlet’s build our own Hubot extension and get exactly what we want for our team\nof developers.\n\n\n\nOur Blueprint\n\n\nWe are going to build an extension to Hubot. When we are done, Hubot will be\ntransformed into a robot that..\n\n    • listens for pull request events from GitHub by subscribing to notifications\n\n      using the GitHub Activities API\n    • invites people in the chat room to comment on those pull requests\n\n    • guarantees that communication between it and GitHub is securely deliv-\n\n      ered (with a caveat)\n    • retrieves vital information from an external service (the Slack.com API)\n\n    • has functionality fully described by automated tests.\n\n    • allows easy simulation of inputs and outputs which map to the inputs\n      and outputs it gets from APIs and services.\n\n    • runs with ease host on a major Paas (Heroku)\n\n   Hubot provides the skeleton for our chat robot. We’ll add the above func-\n\ntionality to Hubot and see how easy it is to combine these features into a coher-\nent whole that solves a real problem.\n   Just so we are clear about the differences between a vanilla Hubot and our\n\nextended Hubot following the above blueprint, let’s give it a name: Probot.\nFrom now on, we will speak about unmodified Hubots by calling them Hubot,\n\nwhile we will call our amplified Hubot with PR delegation super powers the\nname “Probot.”\n\n\nConsiderations and Limitations\n\n\nIf you want stability with your Hubot, you need to host it on a server. Hubot is\n\nwritten in NodeJS and requires a hosting service that supports NodeJS. Our Hu-\nbot needs to sit on a public IP address (not inside the firewall) because we re-\nceive notifications from GitHub. It is not strictly required that you host Hubot\n\n\n\n\n                                                                                               237","initials":"ew"},"\"Seems a bit trivial to cover how an account is made?\"":{"page":239,"text":"                                                                                           Our Blueprint\n\n\n   Now that we have a simple Hubot created we need to create the Slack site\nwhere our Hubot will live.\n\n\n\nCreating a Slack Account\n\nGoing to slack.com starts you on the process to create your own Slack site.\n\nYou’ll need to step through creating an account. Slack sites are segmented by\norganization, and you’ll want to establish a URL prefix for your Slack site. Typi-\n\ncally this is the name of your organization.\n\n\nNAMING THE CHANNEL\n\nOnce you have your slack site created, you need to create a channel.\n\n\n\n\n                                                                                   FIGURE 10-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   You can name the channel anything you want, but it is often a good\nmnemonic to use a name which suggests this is a channel where more serious\n\nwork gets done. You could use a name like “PR Discussion” to indicate this is\nthe channel where PRs are discussed. To keep things simple, we will use the\nname “#general”. Once you click on the link to create a channel, you’ll see a\n\npopup asking for the name and an optional description.  After you have created\nthe channel, you will see a link to “Add a service integration.”\n\n\n\n\n\n                                                                                           239","initials":"ew"},"\"Writing seems a little weird here.\"":{"page":240,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n\n FIGURE 10-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Slack supports many different service integrations, and one of them is Hu-\n                        bot.\n\n\n\n\n FIGURE 10-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                           Choosing Hubot takes you to a settings screen for your Hubot integration.\n                           Slack automatically generates an authentication token for you. This token is\n\n                        used to verify the connection from your Hubot. This token can be revoked, and\n                        in fact the token from the image below has been revoked and can no longer be\n\n                        used to authenticate into Slack. If you ever accidentally publicize this token,\n                        you can easily revoke and reassign a token to your Hubot on this screen.\n\n                           You will also need to specify a name. Use “probot” and if you’d like, change\n                        the avatar associated with the Hubot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        240","initials":"ew"},"\"The transition to calling it Probot was very sudden... is it pro just because it is configured to connect to Slack ?\"":{"page":242,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          This command runs the hubot script with the slack adapter. The slack adapt-\n                       er knows how to interact with the Slack.com service. It requires an authentica-\n                       tion token, and this is provided via the environment variable at the beginning of\n\n                       the line.\n\n\n                       A FIRST CONVERSATION\n\n                       Your bot should be setup and waiting in the #general room inside your Slack\n\n                       site. Go to the #general room. Then, you can test that probot is properly con-\n                       nectd by typing in the name of your Hubot and then a command like       the\n\n                       rules . For example, if our Hubot is namedprobot  , then we would type pro-\n                       bot the rules   .\n\n\n\n\n FIGURE 10-5\n\n\n\n\n\n\n\n\n\n                          We see that our hubot printed out the rules it abides by (published originally\n                       by Isaac Asimov in his “Runaround” short story in 1942).\n\n\n                       EXPLORING THE HUBOT VOCABULARY\n\n\n                       Hubot out of the box supports many commands. To get a list, type “help”.\n\n\n\n FIGURE 10-6\n\n\n\n\n\n\n\n\n\n\n\n\n                          The pug me  command is a favorite. Many people new to Hubot quickly get\n                       sucked into spending hours looking at cute pictures of pugs. Beware!\n\n\n\n\n\n\n\n       242","initials":"ew"},"\"Feels like this part was rushed slightly\"":{"page":244,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          $ heroku logs -t\n                          2014-11-18T07:07:18.716943+00:00 app[web.1]: Successfully 'connected'\n                          as hubot\n                          2014-11-18T07:07:18.576287+00:00 app[web.1]: Tue, 18 Nov 2014 07:07:18\n                          GMT connect deprecated limit: Restrict request size at location of\n\n                          read at\n                          node_modules/hubot/node_modules/express/node_modules/connect/lib/middleware/multipart.js:86:15\n                          ...\n\n                          When you send commands into your chat room you will notice events inside\n\n                       of Heroku. This is a good way to verify that your bot is wired into Slack properly.\n                          You might also want to publish this repository into GitHub. Heroku, as a part\n\n                       of hosting your live application, also hosts the full Git repository of your Hubot\n                       (Hubot, as friendly as it tries to be, is just another NodeJS application in the\n\n                       end). Heroku can host the entirety of the source code for your Hubot for you,\n                       but does not have the additional tools, like user management, that GitHub\n                       does. For this reason, use your GitHub account as your code repository, the\n\n                       place where team members develop new features of your chat bot, and then\n                       pull locally and push into Heroku using the ease of the Git workflow as a de-\n\n                       ployment layer.\n                          Now that we have created and installed Hubot, let’s look at the Activities API\n\n                       and determine how we want to code our extension.\n\n\n                       Activities API Overview\n\n\n                       The Activities API centers around notifications: notifications are similar to the\n\n                       notifications you see on social networking sites, events that occur which docu-\n                       ment important points of interest inside a timeline of activity. GitHub activity\n                       events are often tied to important milestones inside of a developer’s day, activi-\n\n                       ties like pushing commits into the main remote repository, asking questions on\n                       discussion threads associated with a repository, or assigning issues to a devel-\n\n                       oper for review.\n                          These notifications are accessible to team members without programmati-\n\n                       cally accessing the GitHub API. Team members are notified of events inside of\n                       their workflow using email based on several rules. GitHub will automatically\n                       send out notification emails when a user has watched a repository and issues\n\n                       or comments are added, a pull request is made, or there are comments made\n                       on a commit. In addition, even if a user has not watched a repository, they will\n\n                       be notified if that user is @mentioned (prefixing the   @ character to a team\n                       member’s name inside a comment), when an issue is assigned to them, or when\n\n                       that user participates in a discussion associated with any repository.\n                          The GitHub policy for notification is definitely to err on the side of being\n                       overly verbose. Many people live in their email, and making sure that all impor-\n\n\n\n\n        244","initials":"ew"},"\"@mention is something you can just say works exactly like Twitter\"":{"page":244,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          $ heroku logs -t\n                          2014-11-18T07:07:18.716943+00:00 app[web.1]: Successfully 'connected'\n                          as hubot\n                          2014-11-18T07:07:18.576287+00:00 app[web.1]: Tue, 18 Nov 2014 07:07:18\n                          GMT connect deprecated limit: Restrict request size at location of\n\n                          read at\n                          node_modules/hubot/node_modules/express/node_modules/connect/lib/middleware/multipart.js:86:15\n                          ...\n\n                          When you send commands into your chat room you will notice events inside\n\n                       of Heroku. This is a good way to verify that your bot is wired into Slack properly.\n                          You might also want to publish this repository into GitHub. Heroku, as a part\n\n                       of hosting your live application, also hosts the full Git repository of your Hubot\n                       (Hubot, as friendly as it tries to be, is just another NodeJS application in the\n\n                       end). Heroku can host the entirety of the source code for your Hubot for you,\n                       but does not have the additional tools, like user management, that GitHub\n                       does. For this reason, use your GitHub account as your code repository, the\n\n                       place where team members develop new features of your chat bot, and then\n                       pull locally and push into Heroku using the ease of the Git workflow as a de-\n\n                       ployment layer.\n                          Now that we have created and installed Hubot, let’s look at the Activities API\n\n                       and determine how we want to code our extension.\n\n\n                       Activities API Overview\n\n\n                       The Activities API centers around notifications: notifications are similar to the\n\n                       notifications you see on social networking sites, events that occur which docu-\n                       ment important points of interest inside a timeline of activity. GitHub activity\n                       events are often tied to important milestones inside of a developer’s day, activi-\n\n                       ties like pushing commits into the main remote repository, asking questions on\n                       discussion threads associated with a repository, or assigning issues to a devel-\n\n                       oper for review.\n                          These notifications are accessible to team members without programmati-\n\n                       cally accessing the GitHub API. Team members are notified of events inside of\n                       their workflow using email based on several rules. GitHub will automatically\n                       send out notification emails when a user has watched a repository and issues\n\n                       or comments are added, a pull request is made, or there are comments made\n                       on a commit. In addition, even if a user has not watched a repository, they will\n\n                       be notified if that user is @mentioned (prefixing the   @ character to a team\n                       member’s name inside a comment), when an issue is assigned to them, or when\n\n                       that user participates in a discussion associated with any repository.\n                          The GitHub policy for notification is definitely to err on the side of being\n                       overly verbose. Many people live in their email, and making sure that all impor-\n\n\n\n\n        244","initials":"ew"},"\"Seems really odd that we are so far into the book, near the end in fact, for all of this to be mentioned only now...\"":{"page":244,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          $ heroku logs -t\n                          2014-11-18T07:07:18.716943+00:00 app[web.1]: Successfully 'connected'\n                          as hubot\n                          2014-11-18T07:07:18.576287+00:00 app[web.1]: Tue, 18 Nov 2014 07:07:18\n                          GMT connect deprecated limit: Restrict request size at location of\n\n                          read at\n                          node_modules/hubot/node_modules/express/node_modules/connect/lib/middleware/multipart.js:86:15\n                          ...\n\n                          When you send commands into your chat room you will notice events inside\n\n                       of Heroku. This is a good way to verify that your bot is wired into Slack properly.\n                          You might also want to publish this repository into GitHub. Heroku, as a part\n\n                       of hosting your live application, also hosts the full Git repository of your Hubot\n                       (Hubot, as friendly as it tries to be, is just another NodeJS application in the\n\n                       end). Heroku can host the entirety of the source code for your Hubot for you,\n                       but does not have the additional tools, like user management, that GitHub\n                       does. For this reason, use your GitHub account as your code repository, the\n\n                       place where team members develop new features of your chat bot, and then\n                       pull locally and push into Heroku using the ease of the Git workflow as a de-\n\n                       ployment layer.\n                          Now that we have created and installed Hubot, let’s look at the Activities API\n\n                       and determine how we want to code our extension.\n\n\n                       Activities API Overview\n\n\n                       The Activities API centers around notifications: notifications are similar to the\n\n                       notifications you see on social networking sites, events that occur which docu-\n                       ment important points of interest inside a timeline of activity. GitHub activity\n                       events are often tied to important milestones inside of a developer’s day, activi-\n\n                       ties like pushing commits into the main remote repository, asking questions on\n                       discussion threads associated with a repository, or assigning issues to a devel-\n\n                       oper for review.\n                          These notifications are accessible to team members without programmati-\n\n                       cally accessing the GitHub API. Team members are notified of events inside of\n                       their workflow using email based on several rules. GitHub will automatically\n                       send out notification emails when a user has watched a repository and issues\n\n                       or comments are added, a pull request is made, or there are comments made\n                       on a commit. In addition, even if a user has not watched a repository, they will\n\n                       be notified if that user is @mentioned (prefixing the   @ character to a team\n                       member’s name inside a comment), when an issue is assigned to them, or when\n\n                       that user participates in a discussion associated with any repository.\n                          The GitHub policy for notification is definitely to err on the side of being\n                       overly verbose. Many people live in their email, and making sure that all impor-\n\n\n\n\n        244","initials":"ew"},"\"This is oddly phrased... everyone manages emails differently\"":{"page":245,"text":"                                                                                       Activities API Overview\n\n\ntant activities are distributed to the right people involved makes sense, and Git-\n\nHub has a good set of rules for making sure the correct notifications get to the\nright parties.\n\n   Email does falter as a to-do list, however, and at times the ease in which\nemail can be delivered breeds a secondary problem: overwhelm. It can be very\neasy to lose focus (vital to building software) when you are constantly context\n\nswitching by checking email, and notifications can often fly by. In addition,\nemail is privately directed and prevents easily collaboration; generally people\n\ndon’t share email inboxes. Let’s extend our Hubot to help us resolve these prob-\nlems by taking our GitHub notifications into a shared and “opt-in when you are\n\nlogged-in” communication channel.\n\n\nWriting a Hubot Extension\n\n\nHubot extensions are written in either JavaScript or CoffeeScript. CoffeeScript\n\nis a intermediate language which compiles directly to JavaScript. Many people\nprefer writing in CoffeeScript because it has a cleaner syntax and writes “safer”\n\nJavaScript (the syntax helps you avoid common tricky pitfalls in the JavaScript\nlanguage, like what “this” refers to). CoffeeScript is a indentation based lan-\n\nguage (much like Python) and after the initial learning curve, can feel easier to\nread than JavaScript, especially when you have many nested function callbacks\n\n(common in JavaScript programming); it is easier to see where a function be-\ngins and ends given the indentation levels. Hubot is itself written in Coffee-\nScript and we’ll write our extension in CoffeeScript as well.\n\n\n\n     CoffeeScript is a language where indentation is important. For readabili-\n     ty purposes, when we display a snippet of code from a longer file, there\n     are times where we have changed the indentation of that snippet and re-\n     moved the initial indentation. If you were to copy the code without re-\n\n     alignment, the snippet would not work until you re-indented it to fit the\n     context into which it sits.\n\n\n   The Hubot extension module format is exceedingly simple. You write Java-\n\nScript modules (using the   export  syntax) and Hubot passes you in a robot ob-\nject which you program using several API methods.\n\n   There are a few concepts useful to programming Hubot. You can find an ex-\nample of each of these methods inside the example.coffee file inside the scripts\n\ndirectory.\n\n    • Hubots have a “brain”. This is an internal state object, which means these\n      values persist across chat messages. This state is not persisted into a da-\n\n      tabase by default, so this state is not restored if you restart Hubot. How-\n      ever, a persistence mechanism is exposed via redis, though this is option-\n\n\n\n\n                                                                                                245","initials":"ew"},"\"Don't need a exhaustive list of features here, introduce them as we need them.\"":{"page":246,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                               al and requires configuration. The brain is they way you set and get values\n                               which are saved across discrete messages.\n\n                             • Hubots have different respose mechanisms. They can choose to respond\n                               only when they hear exact phrases or when keywords are found in any\n\n                               message, and you don’t need to do the grunt work inside your code to\n                               determine the differences between these communication types.\n\n                             • Hubots include an HTTP server. You might need your Hubot to accept re-\n                               quests from additional services beyond the chat service, and Hubot\n\n                               makes it easy to accept these kinds of requests.\n\n                             • Hubot has a built in HTTP client. You can easily access HTTP resources\n                               within Hubot; many popular extensions to Hubot access a web service\n\n                               when Hubot receives a request.\n                             • Hubot commands can include parameters. You can tell a Hubot to do\n\n                               something multiple times and write a generic function which accepts op-\n                               tions.\n\n                             • Hubots can handle events. Each chat service has a generalized set of\n                               events that are normalized to a common API. Hubots can be programmed\n\n                               to interact with these events. For example, Hubots can perform actions\n                               when a room topic changes or when users leave rooms.\n\n                             • Hubots can handle generic errors at the top level. Hubot can be program-\n\n                               med with a catch-all error handler so that no matter where you code\n                               failed, you can catch it without crashing your bot.\n\n                            Probot will use the first five of these features:\n\n                             • We will use the Hubot brain to store a PR review request. If Probot asks a\n\n                               user to review a PR, it needs to keep track of this so that when the user\n                               responds it has some context of the request.\n\n                             • We will use the respond method to program our Hubot to handle a re-\n                               quest when a user accepts or declines the review request.\n\n                             • We will use the HTTP server to accept PR notifications from GitHub web-\n\n                               hooks.\n                             • We will use the HTTP client to get a list of users from Slack.\n\n                             • We will use the parameterization of requests to Hubot to retrieve the spe-\n\n                               cific pull request ID from a chat user message.\n\n                            There are examples of the other two features (events and generic errors) in-\n                         side the examples script that ship with the Hubot source code but we won’t use\n\n                         those APIs in our Probot.\n\n\n\n\n\n\n\n        246","initials":"ew"},"\"This style is very \\\"chatty\\\" and reduces the signal to noise ratio.\"":{"page":247,"text":"                                                                                   Activities API Overview\n\n\nCode Reviews via Pull Requests\n\nAs we’ve seen in other chapters, pull requests are the mechanism used on Git-\n\nHub to easily integrate code changes into a project. Contributors either fork the\nmaster repository and then issues a pull request against that repository, or, if\n\nthey have write permission to the main repository, make a “feature” branch and\nthen issue a pull request against the “master” branch.\n   Pull requests often come with a chat message indicating several people who\n\nshould review the request. This tribal knowledge about who should be involved\nis only in the head of the developer who created the code. It could be that they\n\ninvited the correct people. Or, it could be that they invited the people who they\nprefer to review their code for various (and completely rationale reasons). This\ncan be an effective way to engage the right people around a new piece of code.\n\nAnd, it can have downsides as well: if the person is otherwise engaged, pull re-\nquests can linger when a notification email goes unread. And, there is good re-\n\nsearch to indicate that the best performing teams are those who share all tasks\nand responsibilities equally. It often does not scale to ask everyone to partici-\n\npate in all code reviews associated with a pull request. But, it might be the case\nthat randomly selecting developers involved in a project is a better (and more\nefficient) way to review code than asking the developer who created the code\n\nto determine these people.\n   Probot will assign active chat room users to do code reviews when a new\n\npull request is created. We will use the GitHub Activities API to subscribe to pull\nrequest events. When Probot becomes aware that a pull request needs review,\n\nit will randomly assign a user in the chat room to do the review and then ask\nthat user if they want to accept the challenge. If they accept, we will note that in\nthe pull request comments.\n\n\nEXTENSION BOILERPLATE\n\n\nWe will start writing our extension by defining the high level communication\nformat we expect from our users. Our script has a simple vocabulary: it needs to\n\nrecognize responses accepting a review request, or those that decline. Our ex-\ntension script should be in the        scripts   directory and named       pr-\n\ndelegator.coffee    . This is just the back and forth we will be having with\nusers; we are not yet writing any code to handle the pull request notifications.\n\n\n   module.exports = (robot) ->\n           robot.respond /accept/i, (res) ->\n\n                    accept( res )\n           robot.respond /decline/i, (res) ->\n                    decline( res )\n           accept = ( res ) ->\n\n\n\n\n                                                                                           247","initials":"ew"},"\"Bad phrasing/grammar.\"":{"page":247,"text":"                                                                                   Activities API Overview\n\n\nCode Reviews via Pull Requests\n\nAs we’ve seen in other chapters, pull requests are the mechanism used on Git-\n\nHub to easily integrate code changes into a project. Contributors either fork the\nmaster repository and then issues a pull request against that repository, or, if\n\nthey have write permission to the main repository, make a “feature” branch and\nthen issue a pull request against the “master” branch.\n   Pull requests often come with a chat message indicating several people who\n\nshould review the request. This tribal knowledge about who should be involved\nis only in the head of the developer who created the code. It could be that they\n\ninvited the correct people. Or, it could be that they invited the people who they\nprefer to review their code for various (and completely rationale reasons). This\ncan be an effective way to engage the right people around a new piece of code.\n\nAnd, it can have downsides as well: if the person is otherwise engaged, pull re-\nquests can linger when a notification email goes unread. And, there is good re-\n\nsearch to indicate that the best performing teams are those who share all tasks\nand responsibilities equally. It often does not scale to ask everyone to partici-\n\npate in all code reviews associated with a pull request. But, it might be the case\nthat randomly selecting developers involved in a project is a better (and more\nefficient) way to review code than asking the developer who created the code\n\nto determine these people.\n   Probot will assign active chat room users to do code reviews when a new\n\npull request is created. We will use the GitHub Activities API to subscribe to pull\nrequest events. When Probot becomes aware that a pull request needs review,\n\nit will randomly assign a user in the chat room to do the review and then ask\nthat user if they want to accept the challenge. If they accept, we will note that in\nthe pull request comments.\n\n\nEXTENSION BOILERPLATE\n\n\nWe will start writing our extension by defining the high level communication\nformat we expect from our users. Our script has a simple vocabulary: it needs to\n\nrecognize responses accepting a review request, or those that decline. Our ex-\ntension script should be in the        scripts   directory and named       pr-\n\ndelegator.coffee    . This is just the back and forth we will be having with\nusers; we are not yet writing any code to handle the pull request notifications.\n\n\n   module.exports = (robot) ->\n           robot.respond /accept/i, (res) ->\n\n                    accept( res )\n           robot.respond /decline/i, (res) ->\n                    decline( res )\n           accept = ( res ) ->\n\n\n\n\n                                                                                           247","initials":"ew"},"\"This seems to imply the coder will just keep raising pull requests until someone accepts... this isn't exactly the spirit of a code review!\"":{"page":248,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                                         res.reply \"Thanks, you got it!\"\n\n                                         console.log \"Accepted!\"\n                                decline = ( res ) ->\n                                         res.reply \"OK, I'll find someone else\"\n                                         console.log \"Declined!\"\n\n\n                         This is a dense piece of code and can be confusing if you are new to Coffee-\n                      Script. At the same time, hopefully you will agree this is amazingly powerful\n\n                      code for such a small snippet after reading these notes.\n\n                         All NodeJS modules work start by defining entrypoints using theexports\n\n                         syntax. This code defines a function that expects a single parameter; when\n                         the function is executed, the parameter will be called robot. The Hubot\n\n                         framework will pass in a robot object for us that we will program further\n                         down.\n\n\n                         The Hubot API defines a method on the robot object calledrespond  which\n                         we use here. It takes two parameters: a regular express to match against and\n\n                         a function which receives an instance of the chat response object (cresed\n                         here). The second line uses the API for this response object to call a method\n\n                         accept  with the response object. We define accept in a moment.\n\n                         We define another message expectation using the same respond syntax and\n\n                         then call a method decline  . If someone saysprobot accept    orprobot\n                         decline  in our chat room, these two calls will answer those statements.\n\n\n                         Now we define the   accept  method. The accept method receives the re-\n\n                         sponse object generated by the Hubot framework and calls thereply meth-\n                         od which, you guessed it, sends a message back into the chat channel with\n\n                         the text “Thanks, you got it!”.\n\n                         The accept method then also calls console.log   with information that is\n\n                         displayed on the console from which we started Probot. This is a simple way\n                         for us to assure everything worked correctly; if we don’t see this message,\n\n                         our code before this was broken. The  console.log   is not visible to any\n                         users in the channel. It is good practice to remove this code when you final-\n                         ize your production code, but if you forget, it won’tct anything happen-\n\n                         ing in the channel.\n\n\n                         We then define thedecline  method using the same APIs as for theaccept\n                         method.\n\n\n\n\n\n\n       248","initials":"ew"},"\"With great power...\\nYou need to make the code more approachable, don't just lead the reader in cold.\"":{"page":248,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                                         res.reply \"Thanks, you got it!\"\n\n                                         console.log \"Accepted!\"\n                                decline = ( res ) ->\n                                         res.reply \"OK, I'll find someone else\"\n                                         console.log \"Declined!\"\n\n\n                         This is a dense piece of code and can be confusing if you are new to Coffee-\n                      Script. At the same time, hopefully you will agree this is amazingly powerful\n\n                      code for such a small snippet after reading these notes.\n\n                         All NodeJS modules work start by defining entrypoints using theexports\n\n                         syntax. This code defines a function that expects a single parameter; when\n                         the function is executed, the parameter will be called robot. The Hubot\n\n                         framework will pass in a robot object for us that we will program further\n                         down.\n\n\n                         The Hubot API defines a method on the robot object calledrespond  which\n                         we use here. It takes two parameters: a regular express to match against and\n\n                         a function which receives an instance of the chat response object (cresed\n                         here). The second line uses the API for this response object to call a method\n\n                         accept  with the response object. We define accept in a moment.\n\n                         We define another message expectation using the same respond syntax and\n\n                         then call a method decline  . If someone saysprobot accept    orprobot\n                         decline  in our chat room, these two calls will answer those statements.\n\n\n                         Now we define the   accept  method. The accept method receives the re-\n\n                         sponse object generated by the Hubot framework and calls thereply meth-\n                         od which, you guessed it, sends a message back into the chat channel with\n\n                         the text “Thanks, you got it!”.\n\n                         The accept method then also calls console.log   with information that is\n\n                         displayed on the console from which we started Probot. This is a simple way\n                         for us to assure everything worked correctly; if we don’t see this message,\n\n                         our code before this was broken. The  console.log   is not visible to any\n                         users in the channel. It is good practice to remove this code when you final-\n                         ize your production code, but if you forget, it won’tct anything happen-\n\n                         ing in the channel.\n\n\n                         We then define thedecline  method using the same APIs as for theaccept\n                         method.\n\n\n\n\n\n\n       248","initials":"ew"},"\"This is a bit of dodgy advice. Ideally you want all the logging to be available both to the user and the developers...\"":{"page":248,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                                         res.reply \"Thanks, you got it!\"\n\n                                         console.log \"Accepted!\"\n                                decline = ( res ) ->\n                                         res.reply \"OK, I'll find someone else\"\n                                         console.log \"Declined!\"\n\n\n                         This is a dense piece of code and can be confusing if you are new to Coffee-\n                      Script. At the same time, hopefully you will agree this is amazingly powerful\n\n                      code for such a small snippet after reading these notes.\n\n                         All NodeJS modules work start by defining entrypoints using theexports\n\n                         syntax. This code defines a function that expects a single parameter; when\n                         the function is executed, the parameter will be called robot. The Hubot\n\n                         framework will pass in a robot object for us that we will program further\n                         down.\n\n\n                         The Hubot API defines a method on the robot object calledrespond  which\n                         we use here. It takes two parameters: a regular express to match against and\n\n                         a function which receives an instance of the chat response object (cresed\n                         here). The second line uses the API for this response object to call a method\n\n                         accept  with the response object. We define accept in a moment.\n\n                         We define another message expectation using the same respond syntax and\n\n                         then call a method decline  . If someone saysprobot accept    orprobot\n                         decline  in our chat room, these two calls will answer those statements.\n\n\n                         Now we define the   accept  method. The accept method receives the re-\n\n                         sponse object generated by the Hubot framework and calls thereply meth-\n                         od which, you guessed it, sends a message back into the chat channel with\n\n                         the text “Thanks, you got it!”.\n\n                         The accept method then also calls console.log   with information that is\n\n                         displayed on the console from which we started Probot. This is a simple way\n                         for us to assure everything worked correctly; if we don’t see this message,\n\n                         our code before this was broken. The  console.log   is not visible to any\n                         users in the channel. It is good practice to remove this code when you final-\n                         ize your production code, but if you forget, it won’tct anything happen-\n\n                         ing in the channel.\n\n\n                         We then define thedecline  method using the same APIs as for theaccept\n                         method.\n\n\n\n\n\n\n       248","initials":"ew"},"\"indenting seems a little aggressive here\"":{"page":249,"text":"                                                                                Activities API Overview\n\n\n   If Probot is running, you will need to restart it to reload any scripts. Kill Pro-\n\nbot (using Ctrl-C), and then restart it, and then play with commands inside your\nSlack site. Entering the commands probot accept   and probot decline    and\n\nyou’ll see Probot respoding inside the channel. You’ll also see the messAc-\ncepted!  orDeclined!   printed to the console on which Probot is running.\n\n\nWRITING TESTS FOR HUBOT EXTENSIONS\n\n\nNow that we have the basics of our Probot working, let’s make sure we certify\nour code with some tests. We’ll use the Jasmine testing framework for NodeJS.\n\nIt offers an elegant behavior driven testing syntax where you specify a behavior\nas the first parameter to ait function, and as a second parameter, a function\n\nwhich is run as the test itself. Jasmine manages running eachit call and dis-\nplays a nice output of passing and failed tests at the end of your run. Jasmine\ntests are typically written in JavaScript, but the latest versions of Jasmine sup-\n\nport tests also written in CoffeeScript. Hubot is written in CoffeeScript, so let’s\nwrite our tests in CoffeeScript as well. We need to put our tests inside a directo-\n\nry called “spec” and make sure our filename ends with  .spec.coffee   . Let’s\nuse spec/pr-delegator.spec.coffee        as the complete filename. Jasmine\n\nexpects spec files to hav.spec. at the end of their filename (before the exten-\nsion, eithe.js  or.coffee  ); if your filename does not match this pattern Jas-\n\nmine won’t recognize it as a test.\n\n   Probot = require \"../scripts/pr-delegator\"\n\n   Handler = require \"../lib/handler\"\n\n   pr = undefined\n   robot = undefined\n\n   describe \"#probot\", ->\n\n            beforeEach () ->\n                    robot = {\n                             respond: jasmine.createSpy( 'respond' )\n                             router: {\n                                     post: jasmine.createSpy( 'post' )\n\n                                     }\n                             }\n\n            it \"should verify our calls to respond\", (done) ->\n                    pr = Probot robot\n\n                    expect( robot.respond.calls.count() ).toEqual( 2 )\n                    done()\n\n   The first line in our test requires, or loads, the Hubot extension module into\n\nour test script, giving us a function we save as a Probot variable. We then create\n\n\n\n                                                                                       249","initials":"ew"},"\"Consider if using spies is too closely tying your test code to the implementation? What are we actually trying to test here?\"":{"page":250,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                      a describe  function which is an organizing function to group testdescribe\n\n                      functions take an indentifier (in this c#probot  ) and a function which con-\n                      tains multipleit calls. In additiondescribe   function can also contain be-\n\n                      foreEach   function which configures common elements inside our  it calls; in\n\n                      this case we create a faked robot object which we will pass into ourProbot\n                      function call. When we are running Hubot itself, Hubot creates the robot and\n                      passes it into theProbot  function but when we run our tests, we generate a\n\n                      fake one and query it to make sure that it is receiving the proper configuration.\n                      If we make a change inside our actual Hubot code and forget to update our\n\n                      tests to verify those changes, our tests will fail and we’ll know we need to either\n                      augment our tests, or something broke inside our robot, a good automated\n                      sanity check for us when we are feverishly coding away, animating our helpful\n\n                      Probot.\n                         You should see some similarities between the calls made to our robot ro-\n\n                      bot.respond   and robot.router.post    ) and the tests. We setup “spies” using\n                      Jasmine that generate fake function calls capable of recording any interaction\n\n                      from outside sources (either our production code or the test code harness). In-\n                      side our it call, we then verify that those calls were made. We use expect\n\n                      function to verify that we have made two calls to trespond  function defined\n                      on the robot, and thatrobot.router.post    has been called as well.\n\n                         We need to install Jasmine, and we do this by adding to oupackage.json\n                      file. Append\"jasmine-node\": \"^1.14.5\"      to the file, and make sure to add a\n\n                      comma to the tuple above it. Adding this code specifies that the minimum ver-\n                      sion of jasmine node we will use is “1.14.5”.\n\n\n                         ...\n                           \"hubot-shipit\": \"^0.1.1\",\n                           \"hubot-slack\": \"^3.2.1\",\n                           \"hubot-youtube\": \"^0.1.2\",\n                           \"jasmine-node\": \"^2.0.0\"\n\n                         },\n                         \"engines\": {\n                         ...\n\n\n                         Runing the following commands will then install Jasmine (the library and a\n                      test runner command line tool) and run our tests. We abbreviate some of the\n                      installation output to save space.\n\n\n                         $ npm install\n                         ...\n                         hubot-slack@3.2.1 node_modules/hubot-slack\n\n                         └── slack-client@1.2.2 (log@1.4.0, coffee-script@1.6.3, ws@0.4.31)\n\n\n\n\n\n       250","initials":"ew"},"\"This output isn't that helpful ...\"":{"page":251,"text":"                                                                               Activities API Overview\n\n\n   jasmine-node@2.0.0 node_modules/jasmine-node\n   ├── minimist@0.0.8\n   ├── underscore@1.6.0\n   ├── mkdirp@0.3.5\n\n   ├── walkdir@0.0.7\n   ├── jasmine-growl-reporter@0.2.1 (growl@1.7.0)\n   ├── coffee-script@1.7.1\n   └── gaze@0.5.1 (globule@0.1.0)\n\n\n   hubot-scripts@2.5.16 node_modules/hubot-scripts\n   └── redis@0.8.4\n\n   hubot@2.11.0 node_modules/hubot\n\n   ├── readline-history@1.2.0\n   ├── optparse@1.0.4\n   ├── scoped-http-client@0.10.0\n   ├── log@1.4.0\n   ├── coffee-script@1.6.3\n\n   └── express@3.18.1 (basic-auth@1.0.0, utils-merge@1.0.0,\n   merge-descriptors@0.0.2, fresh@0.2.4, cookie@0.1.2, escape-html@1.0.1,\n   range-parser@1.0.2, cookie-signature@1.0.5, vary@1.0.0,\n   media-typer@0.3.0, parseurl@1.3.0, methods@1.1.0,\n   content-disposition@0.5.0, depd@1.0.0, debug@2.1.1, commander@1.3.2,\n\n   etag@1.5.1, proxy-addr@1.0.5, send@0.10.1, mkdirp@0.5.0, connect@2.27.1)\n   ...\n   $ ./node_modules/.bin/jasmine-node --coffee spec/\n\n\n   .\n\n   Finished in 0.009 seconds\n   1 test, 1 assertions, 0 failures, 0 skipped\n\n\n   Our tests pass and we now have a way to document and verify that our code\ndoes what we think it does.\n\n\nSETTING UP OUR WEBHOOK\n\n\nWe are now in a position to start adding the actual functionality to our Probot.\nOur first requirement is to register for pull request events. We could do this\n\nfrom within the GitHub website, but another way is to use the cURL tool to cre-\nate the webhook from the command line. In order to do this, we need to first\n\ncreate an authorization token, and then we can use that token to create a web-\nhook.\n\n   To create the token, run this command, setting the proper variables for your\nusername instead of mine (“xrd”).\n\n\n\n\n\n\n\n                                                                                       251","initials":"ew"},"\"Whilst all these codes/tokens are temporary, it still feels like bad practice to publish all of them.\"":{"page":252,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                         $ USERNAME=xrd\n                         $ curl https://api.github.com/authorizations --user $USERNAME --data\n                         '{\"scopes\":[\"repo\"], \"note\": \"Probot access to PRs\" }' -X POST\n\n\n                         If you are using two-factor authentication then you will see a response mes-\n                      sage like this:\n\n\n                         {\n                            \"message\": \"Must specify two-factor authentication OTP code.\",\n                            \"documentation_url\":\n\n                            \"https://developer.github.com/v3/auth#working-with-two-factor-authentication\"\n                         }\n\n                         If you see this, then you will be receiving a one time password via your\n\n                      choice of two factor authentication alternative endpoint (either SMS or a two\n                      factor authentication app like Google Authenticator or recovery codes that you\n\n                      printed out). If you use text messaging, check your text messages and then re-\n                      send the request appending a header using cURL.\n\n\n                         $ curl https://api.github.com/authorizations --user $USERNAME --data\n                         '{\"scopes\":[\"repo\"], \"note\": \"Probot access to PRs\" }' -X POST\n                         --header \"X-GitHub-OTP: 423584\"\n                         Enter host password for user 'xrd':\n\n\n                         If all these steps complete successfully (regardless of whether you are using\n\n                      2-factor auth or not) you will then receive an oauth token.\n\n                         {\n                            \"id\": 1234567,\n\n                            \"url\": \"https://api.github.com/authorizations/1234567\",\n                            \"app\": {\n                              \"name\": \"Probot access to PRs (API)\",\n                              \"url\": \"https://developer.github.com/v3/oauth_authorizations/\",\n\n                              \"client_id\": \"00000000000000000000\"\n                            },\n                            \"token\": \"ad5a36c3b7322c4ae8bb9069d4f20fdf2e454266\",\n                            \"note\": \"Probot access to PRs\",\n                            \"note_url\": null,\n\n                            \"created_at\": \"2015-01-13T06:23:53Z\",\n                            \"updated_at\": \"2015-01-13T06:23:53Z\",\n                            \"scopes\": [\n                              \"notifications\"\n                            ]\n\n                         }\n\n\n\n\n\n\n\n\n       252","initials":"ew"},"\"Feels like this book should just have a single section on Oauth so we don't have to keep covering it again and again\"":{"page":253,"text":"                                                                              Activities API Overview\n\n\nUsing the oAuth token to register for events\n\nOnce this is completed we now have our token which we can use to create a\n\nwebhook. Make sure to use the correct repository name and access token be-\nfore running the cURL command. We will also need the endpoint that we cre-\n\nated when we published into Heroku (in our case          https://inqry-\nchatbot.herokuapp.com    )\n\n\n   $ REPOSITORY=testing_repostory\n   $ TOKEN=ad5a36c3b7322c4ae8bb9069d4f20fdf2e454266\n   $ WEBHOOK_URL=https://inqry-chatbot.herokuapp.com/pr\n   $ CONFIG=$(echo '{\n\n     \"name\": \"web\",\n     \"active\": true,\n     \"events\": [\n       \"push\",\n       \"pull_request\"\n\n     ],\n     \"config\": {\n       \"url\": \"'$WEBHOOK_URL'\",\n       \"content_type\": \"form\",\n\n       \"secret\" : \"XYZABC\"\n     }\n   }')\n   $ curl -H \"Authorization: token $TOKEN\" \\\n   -H \"Content-Type: application/json\" -X POST \\\n\n   -d \"$CONFIG\" https://api.github.com/repos/$USERNAME/$REPOSITORY/hooks\n   {\n     \"url\": \"https://api.github.com/repos/xrd/testing_repostory/hooks/3846063\",\n     \"test_url\":\n     \"https://api.github.com/repos/xrd/testing_repostory/hooks/3846063/test\",\n\n     \"ping_url\":\n     \"https://api.github.com/repos/xrd/testing_repostory/hooks/3846063/pings\",\n     \"id\": 3846063,\n     \"name\": \"web\",\n     \"active\": true,\n\n     \"events\": [\n       \"push\",\n       \"pull_request\"\n     ],\n     \"config\": {\n\n       \"url\": \"https://inqry-chatbot.herokuapp.com/pr\",\n       \"content_type\": \"json\"\n     },\n     \"last_response\": {\n       \"code\": null,\n\n       \"status\": \"unused\",\n       \"message\": null\n\n\n\n\n                                                                                      253","initials":"ew"},"\"Again there is a lot of preamble here that isn't necessary\"":{"page":256,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          3. Commit the content\n\n                          4. Push the new branch into GitHub\n                          5. Issue a pull request.\n\n                          All of this can be automated using a combination of git commands and\n\n                       cURL. We’ve seen some of these commands before and can reuse previous\n                       command line invocations and variables that we used when generating our\n\n                       webhook using the API via cURL. Our config variable is similar, but the required\n                       fields in this case are the title and body for the pull request, the “head” key\n\n                       which matches the name of the branch, and where to merge it to using the\n                       “base” key.\n\n                          Creating a new branch, adding some content and then issuing a pull request\n                       against the branch might be something we need to do several (or more) times\n                       as we experiment and learn about the Hubot extension API. The examples here\n\n                       work right out of the box, but don’t be fooled into thinking that it all went ex-\n                       actly as we expected the first time. Given that, these are commands you might\n\n                       want to perform multiple times as you are experimenting, so let’s put the com-\n                       mands described in the prior paragraph into a bash script that is generic and\n\n                       can be run multiple times. We can call iissue-pull-request.sh      and place\n                       the script inside the test directory.\n\n\n                          # Modify these three variables\n                          AUTH_TOKEN=b2ac1f43aeb8d73b69754d2fe337de7035ec9df7\n                          USERNAME=xrd\n\n                          REPOSITORY=test_repository\n\n                          DATE=$(date \"+%s\")\n                          NEW_BRANCH=$DATE\n                          git checkout -b $NEW_BRANCH\n\n                          echo \"Adding some content\" >> test-$DATE.txt\n                          git commit -m \"Adding test file to test branch at $DATE\" -a\n                          git push origin $NEW_BRANCH\n                          CONFIG=$(echo '\n\n                          { \"title\": \"PR on '$DATE'\",\n                            \"body\" : \"Pull this PR'$DATE'\",\n                            \"head\": \"'$NEW_BRANCH'\",\n                            \"base\": \"master\"\n                          }' )\n\n                          URL=https://api.github.com/repos/$USERNAME/$REPOSITORY/pulls\n                          curl -H \"Authorization: token $AUTH_TOKEN\" \\\n                          -H \"Content-Type: application/json\" -X POST -d \"$CONFIG\" \"$URL\"\n\n\n                          This script generates a unique string based on the current time. It then cre-\n                       ates and checks out a new branch based on that name, adds some content to a\n\n                       unique file, commits it, pushes it into GitHub, and generates a pull request us-\n\n\n\n\n       256","initials":"ew"},"\"This shouldn't have been published?\"":{"page":256,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          3. Commit the content\n\n                          4. Push the new branch into GitHub\n                          5. Issue a pull request.\n\n                          All of this can be automated using a combination of git commands and\n\n                       cURL. We’ve seen some of these commands before and can reuse previous\n                       command line invocations and variables that we used when generating our\n\n                       webhook using the API via cURL. Our config variable is similar, but the required\n                       fields in this case are the title and body for the pull request, the “head” key\n\n                       which matches the name of the branch, and where to merge it to using the\n                       “base” key.\n\n                          Creating a new branch, adding some content and then issuing a pull request\n                       against the branch might be something we need to do several (or more) times\n                       as we experiment and learn about the Hubot extension API. The examples here\n\n                       work right out of the box, but don’t be fooled into thinking that it all went ex-\n                       actly as we expected the first time. Given that, these are commands you might\n\n                       want to perform multiple times as you are experimenting, so let’s put the com-\n                       mands described in the prior paragraph into a bash script that is generic and\n\n                       can be run multiple times. We can call iissue-pull-request.sh      and place\n                       the script inside the test directory.\n\n\n                          # Modify these three variables\n                          AUTH_TOKEN=b2ac1f43aeb8d73b69754d2fe337de7035ec9df7\n                          USERNAME=xrd\n\n                          REPOSITORY=test_repository\n\n                          DATE=$(date \"+%s\")\n                          NEW_BRANCH=$DATE\n                          git checkout -b $NEW_BRANCH\n\n                          echo \"Adding some content\" >> test-$DATE.txt\n                          git commit -m \"Adding test file to test branch at $DATE\" -a\n                          git push origin $NEW_BRANCH\n                          CONFIG=$(echo '\n\n                          { \"title\": \"PR on '$DATE'\",\n                            \"body\" : \"Pull this PR'$DATE'\",\n                            \"head\": \"'$NEW_BRANCH'\",\n                            \"base\": \"master\"\n                          }' )\n\n                          URL=https://api.github.com/repos/$USERNAME/$REPOSITORY/pulls\n                          curl -H \"Authorization: token $AUTH_TOKEN\" \\\n                          -H \"Content-Type: application/json\" -X POST -d \"$CONFIG\" \"$URL\"\n\n\n                          This script generates a unique string based on the current time. It then cre-\n                       ates and checks out a new branch based on that name, adds some content to a\n\n                       unique file, commits it, pushes it into GitHub, and generates a pull request us-\n\n\n\n\n       256","initials":"ew"},"\"Seems odd to have created this with an error\"":{"page":258,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          These requests all failed; our Probot is not correctly configured to handle re-\n                       al HTTP requests from GitHub. This does show that GitHub is trying to do some-\n                       thing when a pull request is received. We’ll work on getting our handler code\n\n                       written and pushed into Heroku, and then issue another PR.\n\n\n                       Handling PR Notifications as Post Requests over HTTP\n\n\n                       Let’s build our HTTP handler when PRs notifications arrive from GitHub. At first\n                       glance, we might take the easy route, adding it directly into the top level script.\n\n                       But, given the fact that JavaScript handles events inside of callbacks and the\n                       fact that Hubot extensions only export a single constructor (using the  mod-\n\n                       ule.exports   syntax) it is easier to create, and more importantly test, a sepa-\n                       rate module which we require in our main extension script.\n\n                          We start by writing our tests. We’ve already created a test which verifies the\n                       call torobot.router.post    . Our new functionality will actually handle the PR\n\n                       notification, so let’s add a new grouping using the describe syntax and call it\n                       “#pr”. The new functionality is simple: if the Probot receives the proper parame-\n                       ters (most importantly that the internal secret matches the secret sent on the\n\n                       request) then we accept the PR as valid and message our room with further in-\n                       structions, namely inviting some user to review this pull request. Our handler\n\n                       then needs to expose two methods:    prHandler   which is where we delegate\n                       any information coming from an HTTP request to the   /pr route, and a method\n\n                       where we can configure the secret, which we callsetSecret  . Once we have es-\n                       tablished this internal signature for our handler library, we can add two simple\n\n                       tests and then our library.\n                          We have two tests: one which handles the correct flow and one which han-\n\n                       dles the incorrect flow. In a before block (this happens before each test) we set-\n                       up a fake robot, and set the secret on our handler module. Our faked robot im-\n                       plements the same methods that a real Hubot robot does (the “messageRoom”\n\n                       and “send” methods), but we create Jasmine spies to verify these functions are\n                       called inside our implementation code.\n\n\n                          describe \"#pr\", ->\n                                   secret = \"ABCDEF\"\n                                   robot = undefined\n\n                                   res = undefined\n\n                                   beforeEach ->\n                                            robot = {\n                                                    messageRoom: jasmine.createSpy()\n\n                                                    }\n                                            res = { send: jasmine.createSpy() }\n                                            Handler.setSecret secret\n\n\n\n\n        258","initials":"ew"},"\"phrasing here is strange, missing word perhaps?\"":{"page":260,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          $ node_modules/jasmine-node/bin/jasmine-node \\\n                          --coffee spec/pr-delegator.spec.coffee\n                          ....\n\n\n                          Finished in 0.01 seconds\n                          4 tests, 6 assertions, 0 failures, 0 skipped\n\n                          Hubot will spawn the HTTP server wherever it runs so we can talk to it on\n\n                       our local machine (though this will likely be inside a firewall and inaccessible to\n                       GitHub), so we can test it using cURL locally. Remember that our robot router\n\n                       accepts commands as HTTP POST requests, so we need to specify a post re-\n                       quest (using the --data  switch with cURL).\n\n\n                          $ ( HUBOT_SLACK_TOKEN=xoxb-3295776784-nZxl1H3nyLsVcgdD29r1PZCq \\\n                          ./bin/hubot -a slack 2> /dev/null | grep -i secret & )\n                          $ curl --data '' http://localhost:8080/pr\n\n                          Invalid secret\n                          OK\n                          $ curl --data 'secret=XYZABC' http://localhost:8080/pr\n                          Secret verified\n                          OK\n\n                          $ kill `ps a | grep node | grep -v grep | awk -F ' ' '{ print $1 }'`\n\n                          These commands verify that things are working properly. First, we start the\n\n                       server and pipe the output to grep to only display output which is related to our\n                       secret processing (we also background the entire chain using an ampersand\n\n                       and parentheses, a bash trick). Then, we hit the server running locally without\n                       the secret: the server (as it is running in the same shell) prints out the message\n\n                       “Invalid secret” using console.log   , and then curl prints out “OK” which is\n                       what was returned from our server. If we run the command again, this time in-\n                       cluding the secret as post parameters, we see that Hubot verified the secret in-\n\n                       ternally against its own secret, and then curl again prints “OK” which was what\n                       the express server inside of Hubot returned to the calling client. The final line\n\n                       quits Hubot: this command finds the PID for the Hubot client (which runs as a\n                       node process) and then sends it a SIGHUP signal, signaling to Hubot that it\n\n                       should quit.\n                          Provided you connected correctly to your Slack site, you’ll also see a mes-\n\n                       sage inside your #general channel which says “OMG, GitHub is on my caller-\n                       id!?!” We now have a simple way to trigger a pull request notification without\n                       going through the formality of actually generating a pull request. Between our\n\n                       script which issues real pull requests through the GitHub API and this one that\n                       fakes a webhook notification, we have the ability to test our code externally as\n\n                       we develop it. Of course, our tests are valuable, but sometimes we it is impossi-\n\n\n\n\n\n\n        260","initials":"ew"},"\"This seems like a terrible idea, selecting a reviewer should be done consciously!\"":{"page":261,"text":"                                                                                      Activities API Overview\n\n\nble to understand what is happening inside of our Probot without running\n\nagainst the real Probot and not a test harness.\n\n\nASSIGNING AN ACTIVE CHAT ROOM USER\n\n\nNow that we have an incoming pull request (albeit one which we are faking), we\n\nneed to write the code to find a random user and assign them to the pull re-\n\nquest.\n\n\n     This next section is redundant; our Probot will function exactly as we\n     need it to if you were to disregard any code from this section. As I was\n\n     writing this book, I mistakenly missed the fact that the Hubobraincon-\n     tains a list of users and found another avenue to getting that data, the\n     Slack API. I wrote the chapter using the Slack API, and then discovered\n\n     my mistake. Initially I planned to remove this entire section. However, it\n     does demonstrate the ease of using an external service through the built\n     in HTTP client, which is a powerful feature of Hubot. And, it also demon-\n\n     strates how powerful tests aid you when developing a Hubot extension; I\n     was able to refactor to use a radically different internal code path for get-\n     ting the list of users and maintain faith that the end to end process of my\n\n     code works by refactoring and then fixing broken tests. And, though not\n     important for this section per se, the Slack API provides much richer data\n\n     on the users logged into a room, which could be valuable in other situa-\n     tions. If you want to skip to the next section, you will have all the code to\n     build our Probot as we described earlier. But, I think it is a worthwhile\n\n     read for general Hubot understanding.\n\n\n   To find a user in the room, one option is to go outside the Hubot API and use\n\nthe Slack.com API to query for a list of users. The Slack.com API provides an\nendpoint giving you all users currently in a room. To access the Slack.com API,\n\nwe will use the built in Hubot HTTP client. Once we have the the list of mem-\nbers in the room we can look over this list and randomly choose a member and\n\ndeliver the PR request to them.\n\n\n   _SECRET = undefined\n\n\n   anyoneButProbot = (members) ->\n            user = undefined\n\n            while not user\n                      user = members[ parseInt( Math.random() * members.length ) ].name\n                      user = undefined if \"probot\" == user\n\n            user\n\n   sendPrRequest = ( robot, body, room, url ) ->\n\n            parsed = JSON.parse( body )\n            user = anyoneButProbot( parsed.members )\n\n\n\n\n\n                                                                                              261","initials":"ew"},"\"This highlights the issue running throughout this book of reinventing the wheel for very little value.\"":{"page":261,"text":"                                                                                      Activities API Overview\n\n\nble to understand what is happening inside of our Probot without running\n\nagainst the real Probot and not a test harness.\n\n\nASSIGNING AN ACTIVE CHAT ROOM USER\n\n\nNow that we have an incoming pull request (albeit one which we are faking), we\n\nneed to write the code to find a random user and assign them to the pull re-\n\nquest.\n\n\n     This next section is redundant; our Probot will function exactly as we\n     need it to if you were to disregard any code from this section. As I was\n\n     writing this book, I mistakenly missed the fact that the Hubobraincon-\n     tains a list of users and found another avenue to getting that data, the\n     Slack API. I wrote the chapter using the Slack API, and then discovered\n\n     my mistake. Initially I planned to remove this entire section. However, it\n     does demonstrate the ease of using an external service through the built\n     in HTTP client, which is a powerful feature of Hubot. And, it also demon-\n\n     strates how powerful tests aid you when developing a Hubot extension; I\n     was able to refactor to use a radically different internal code path for get-\n     ting the list of users and maintain faith that the end to end process of my\n\n     code works by refactoring and then fixing broken tests. And, though not\n     important for this section per se, the Slack API provides much richer data\n\n     on the users logged into a room, which could be valuable in other situa-\n     tions. If you want to skip to the next section, you will have all the code to\n     build our Probot as we described earlier. But, I think it is a worthwhile\n\n     read for general Hubot understanding.\n\n\n   To find a user in the room, one option is to go outside the Hubot API and use\n\nthe Slack.com API to query for a list of users. The Slack.com API provides an\nendpoint giving you all users currently in a room. To access the Slack.com API,\n\nwe will use the built in Hubot HTTP client. Once we have the the list of mem-\nbers in the room we can look over this list and randomly choose a member and\n\ndeliver the PR request to them.\n\n\n   _SECRET = undefined\n\n\n   anyoneButProbot = (members) ->\n            user = undefined\n\n            while not user\n                      user = members[ parseInt( Math.random() * members.length ) ].name\n                      user = undefined if \"probot\" == user\n\n            user\n\n   sendPrRequest = ( robot, body, room, url ) ->\n\n            parsed = JSON.parse( body )\n            user = anyoneButProbot( parsed.members )\n\n\n\n\n\n                                                                                              261","initials":"ew"},"\"Why not just remove Probot from the members list and then chose someone?\"":{"page":261,"text":"                                                                                      Activities API Overview\n\n\nble to understand what is happening inside of our Probot without running\n\nagainst the real Probot and not a test harness.\n\n\nASSIGNING AN ACTIVE CHAT ROOM USER\n\n\nNow that we have an incoming pull request (albeit one which we are faking), we\n\nneed to write the code to find a random user and assign them to the pull re-\n\nquest.\n\n\n     This next section is redundant; our Probot will function exactly as we\n     need it to if you were to disregard any code from this section. As I was\n\n     writing this book, I mistakenly missed the fact that the Hubobraincon-\n     tains a list of users and found another avenue to getting that data, the\n     Slack API. I wrote the chapter using the Slack API, and then discovered\n\n     my mistake. Initially I planned to remove this entire section. However, it\n     does demonstrate the ease of using an external service through the built\n     in HTTP client, which is a powerful feature of Hubot. And, it also demon-\n\n     strates how powerful tests aid you when developing a Hubot extension; I\n     was able to refactor to use a radically different internal code path for get-\n     ting the list of users and maintain faith that the end to end process of my\n\n     code works by refactoring and then fixing broken tests. And, though not\n     important for this section per se, the Slack API provides much richer data\n\n     on the users logged into a room, which could be valuable in other situa-\n     tions. If you want to skip to the next section, you will have all the code to\n     build our Probot as we described earlier. But, I think it is a worthwhile\n\n     read for general Hubot understanding.\n\n\n   To find a user in the room, one option is to go outside the Hubot API and use\n\nthe Slack.com API to query for a list of users. The Slack.com API provides an\nendpoint giving you all users currently in a room. To access the Slack.com API,\n\nwe will use the built in Hubot HTTP client. Once we have the the list of mem-\nbers in the room we can look over this list and randomly choose a member and\n\ndeliver the PR request to them.\n\n\n   _SECRET = undefined\n\n\n   anyoneButProbot = (members) ->\n            user = undefined\n\n            while not user\n                      user = members[ parseInt( Math.random() * members.length ) ].name\n                      user = undefined if \"probot\" == user\n\n            user\n\n   sendPrRequest = ( robot, body, room, url ) ->\n\n            parsed = JSON.parse( body )\n            user = anyoneButProbot( parsed.members )\n\n\n\n\n\n                                                                                              261","initials":"ew"},"\"This feels sloppy.\"":{"page":262,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                                  robot.messageRoom room, \"#{user}: Hey, want a PR? #{url}\"\n\n                          exports.prHandler = ( robot, req, res ) ->\n                                  slack_users_url =\n                                           \"https://slack.com/api/users.list?token=\" +\n\n                                           process.env.HUBOT_SLACK_TOKEN\n                                  secret = req.body?.secret\n                                  url = req.body?.url\n\n                                  if secret == _SECRET and url\n\n                                           room = \"general\"\n                                           robot.http( slack_users_url )\n                                                    .get() (err, response, body) ->\n                                                            sendPrRequest( robot, body, room, url ) unless err\n\n                                  else\n                                           console.log \"Invalid secret or no URL specified\"\n                                  res.send \"OK\\n\"\n\n                          exports.setSecret = (secret) ->\n\n                                  _SECRET = secret\n\n\n                          We define a method called anyoneButProbot    that takes a list of users and\n                          finds a random one, as long as it is not the probot. You might notice there\n\n                          are some edge cases we should account for in our tests (endless while loop\n                          anyone?) which are left as an exercise for the reader.\n\n\n                          The sendPrRequest   method parses the JSON returned from the Slack API\n                          and then sends the members inside of the object into the anyoneButPro-\n\n                          bot call. It then uses the Hubot API to send a message to the room asking if\n                          that user will accept the pull request review invitation.\n\n\n                          We build the URL to the Slack service by tacking on the Slack API token to\n                          the base Slack API URL.\n\n\n                          As we did before, we pull out the secret and the PR URL, and then make sure\n\n                          they both exist.\n\n                          We use the built in HTTP client to make a GET request to the Slack API. Un-\n\n                          less we receive an error in the response callback, we use the data provided\n                          by the Slack API to initiate the PR review request.\n\n                          To test this using our cURL command, we need to modify the invocation\n\n                       slightly.\n\n\n                          $ curl --data 'secret=XYZABC&url=http://pr/1' \\\n                          http://localhost:8080/pr\n\n\n\n       262","initials":"ew"},"\"This comes back to the question of whether the right things are being tested. Faking a http call seems like a major code smell.\"":{"page":263,"text":"                                                                                Activities API Overview\n\n\n   Our randomly selected user will see the teusername: Hey, want a PR?\nhttp://pr/1   (and the Slack client will format that link as a clickable URL).\n\n   Unfortunately, our tests are now broken: we now have the failureTypeEr-\nror: Object #<Object> has no method 'http'          . Our mocked Robot ob-\n\nject that we pass into our tests does not have the http interface that comes with\nHubot, so we should add it to our custom Robot. The method signature for the\n\nhttp client (which comes from the node-scoped-http-client     NodeJS pack-\nage) is hairy: you chain calls together to build up an HTTP client request and\nend up with a function returned into which you pass a callback where you han-\n\ndle the response body. This module makes you write code that is not particular-\nly testable (said another way, it was challenging for me to understand what the\n\nfaked test implementation should look like) but the setup code does work and\nthe tests itself document an interface to our robot which is easily understanda-\n\nble. We simulate the same chain, defining ahttp  attribute on the mocked ro-\nbot object, an attribute which resolves to a function call itself. Calling that func-\n\ntion returns an object which has a get method, and calling that function re-\nturns a function callback which when called executes that function with three\nparameters. In real life that function callback would contain the error code, the\n\nresponse object, and the JSON. In our case, as long as the error code is empty,\nour implementation will parse the JSON for members, and then issue the PR\n\nrequest.\n\n   json = '{ \"members\" : [ { \"name\" : \"bar\" } , { \"name\" : \"foo\" } ] }'\n\n\n   httpSpy = jasmine.createSpy( 'http' ).and.returnValue(\n            { get: () -> ( func ) ->\n                    func( undefined, undefined, json ) } )\n\n\n   beforeEach ->\n            robot = {\n                    messageRoom: jasmine.createSpy( 'messageRoom' )\n                    http: httpSpy\n                    }\n\n\n            res = { send: jasmine.createSpy( 'send' ) }\n            Handler.setSecret secret\n\n   it \"should disallow calls without the secret\", (done) ->\n\n            req = {}\n            Handler.prHandler( robot, req, res )\n            expect( robot.messageRoom ).not.toHaveBeenCalled()\n            expect( httpSpy ).not.toHaveBeenCalled()\n            expect( res.send ).toHaveBeenCalled()\n\n            done()\n\n\n\n\n\n                                                                                       263","initials":"ew"},"\"Think about what these tests are actually asserting. Do they actually confirm the behaviour of the Probot? Are we actually connecting to the right place? Has the right information been sent to GitHub? is the code fault tolerant?\"":{"page":264,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          it \"should disallow calls without the url\", (done) ->\n                                  req = { body: { secret: secret } }\n                                  Handler.prHandler( robot, req, res )\n                                  expect( robot.messageRoom ).not.toHaveBeenCalled()\n                                  expect( httpSpy ).not.toHaveBeenCalled()\n\n                                  expect( res.send ).toHaveBeenCalled()\n                                  done()\n\n                          it \"should allow calls with the secret\", (done) ->\n                                  req = { body: { secret: secret, url: \"http://pr/1\" } }\n\n                                  Handler.prHandler( robot, req, res )\n                                  expect( robot.messageRoom ).toHaveBeenCalled()\n                                  expect( httpSpy ).toHaveBeenCalled()\n                                  expect( res.send ).toHaveBeenCalled()\n                                  done()\n\n\n                          The code we write here was definitely not a piece of code where testing\n                       came easy; I refactored this multiple times to find a balance between an easy to\n\n                       read test and easy to read code. Writing test code takes effort, but when both\n                       your tests and code are readable and minimal, you generally can be sure you\n\n                       have a good implementation.\n                          We now have a functional and complete implementation of the code to re-\n                       trieve a list of users and assign an incoming pull request out to a randomly se-\n\n                       lected user from that list.\n\n\n                       THE USER LIST FROM THE HUBOT BRAIN\n\n                       Instead of using the Slack API, we can replace the code with a much simpler call\n\n                       torobot.brain.users    . Calling into the Slack users API takes a callback, but\n                       thebrain.users    call does not, which simplifies our code. We do verify inside\n\n                       our tests that we make a call to the HTTP Jasmine spy on theget function, so\n                       we will want to remove that inside our tests. We will need to provide a new\n\n                       function calleusers  to the Probot inside the faked brain we created\n                          Unfortunately, things don’t just work when we change our code to this:\n\n\n                          ...\n                          users = robot.brain.users()\n                          sendPrRequest( robot, users, room, url, number )\n\n                          ...\n\n                          It is likely that what we got back from the Slack API and what Hubot stores\n\n                       inside its brain for users are functionally the same information, but structural\n                       stored very differently. How can we investigate whether this assumption is cor-\n\n                       rect? NodeJS has a standard library module calledutil  which includes useful\n                       utility functions, as you might expect from the name. One of them iinspect\n\n\n\n\n       264","initials":"ew"},"\"This code could be made a bit easier to read\"":{"page":266,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          ...\n                          flattenUsers = (users) ->\n                                  rv = []\n                                  for x in Object.keys( users )\n\n                                          rv.push users[x]\n                                  rv\n\n                          anyoneButProbot = ( users ) ->\n                                  user = undefined\n\n                                  flattened = flattenUsers( users )\n                                  while not user\n                                           user = flattened[ parseInt( Math.random() * flattened.length ) ].name\n                                           user = undefined if \"probot\" == user\n\n                                  user\n\n                          ...\n\n\n                       SENDING PR DATA VIA WEBHOOK\n\n                       Our wiring is almost complete, so let’s actually send real pull request informa-\n\n                       tion. If we run our scriissue-pull-request.sh     we will see it sending data\n                       out to our Probot. Once we have deployed to Heroku, our Probot is listening on\n\n                       a public hostname. GitHub will accept the pull request and then send a JSON\n                       inside the body of a POST request made to our Probot. This JSON looks very\n\n                       different from the url encoded parameters we provide in our cURL script, so we\n                       need to modify our code to fit.\n\n                          If we retrieve the JSON from a POST, it will look something like this (refor-\n                       matted for clarity and brevity):\n\n\n                          {\n                              \"action\":\"opened\",\n                              \"number\":13,\n                              \"pull_request\": {\n\n                                \"locked\" : false,\n                                \"comments_url\" :\n                                \"https://api.github.com/repos/xrd/test_repository/issues/13/comments\",\n                                \"url\" : \"https://api.github.com/repos/xrd/test_repository/pulls/13\",\n\n                                \"html_url\" : \"https://github.com/xrd/test_repository/pulls/13\",\n                                }\n                                ...\n                          }\n\n\n                          Most importantly, you see a URL (thehtml_url  more specifically) which we\n                       will use inside our Probot message to the user. Retrieving the json and parsing\n\n                       it is trivial inside our Probot.\n\n\n\n\n\n\n       266","initials":"ew"},"\"It would be useful to recap things with the reader every so often. so they know where we are up to and where we are heading\"":{"page":266,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          ...\n                          flattenUsers = (users) ->\n                                  rv = []\n                                  for x in Object.keys( users )\n\n                                          rv.push users[x]\n                                  rv\n\n                          anyoneButProbot = ( users ) ->\n                                  user = undefined\n\n                                  flattened = flattenUsers( users )\n                                  while not user\n                                           user = flattened[ parseInt( Math.random() * flattened.length ) ].name\n                                           user = undefined if \"probot\" == user\n\n                                  user\n\n                          ...\n\n\n                       SENDING PR DATA VIA WEBHOOK\n\n                       Our wiring is almost complete, so let’s actually send real pull request informa-\n\n                       tion. If we run our scriissue-pull-request.sh     we will see it sending data\n                       out to our Probot. Once we have deployed to Heroku, our Probot is listening on\n\n                       a public hostname. GitHub will accept the pull request and then send a JSON\n                       inside the body of a POST request made to our Probot. This JSON looks very\n\n                       different from the url encoded parameters we provide in our cURL script, so we\n                       need to modify our code to fit.\n\n                          If we retrieve the JSON from a POST, it will look something like this (refor-\n                       matted for clarity and brevity):\n\n\n                          {\n                              \"action\":\"opened\",\n                              \"number\":13,\n                              \"pull_request\": {\n\n                                \"locked\" : false,\n                                \"comments_url\" :\n                                \"https://api.github.com/repos/xrd/test_repository/issues/13/comments\",\n                                \"url\" : \"https://api.github.com/repos/xrd/test_repository/pulls/13\",\n\n                                \"html_url\" : \"https://github.com/xrd/test_repository/pulls/13\",\n                                }\n                                ...\n                          }\n\n\n                          Most importantly, you see a URL (thehtml_url  more specifically) which we\n                       will use inside our Probot message to the user. Retrieving the json and parsing\n\n                       it is trivial inside our Probot.\n\n\n\n\n\n\n       266","initials":"ew"},"\"Didn't you mention the ?. operator earlier? Couldn't it be used here?\"":{"page":267,"text":"                                                                                Activities API Overview\n\n\n   ...\n   exports.prHandler = ( robot, req, res ) ->\n            body = req.body\n            pr = JSON.parse body if body\n\n            url = pr.pull_request.html_url if pr\n            secret = pr.secret if pr\n\n            if secret == _SECRET and url\n                    room = \"general\"\n\n   ...\n\n   Here you see we pull out the body contents, process them as JSON, extract\n\nthe secret and the URL from the parsed JSON, and then go through our normal\nroutine.\n\n   Our tests are simple, and require that we send in JSON.\n\n   ...\n\n   it \"should disallow calls without the secret and url\", (done) ->\n            req = {}\n            Handler.prHandler( robot, req, res )\n            expect( robot.messageRoom ).not.toHaveBeenCalled()\n            expect( httpSpy ).not.toHaveBeenCalled()\n\n            expect( res.send ).toHaveBeenCalled()\n          done()\n\n   it \"should allow calls with the secret and url\", (done) ->\n            req = { body: '{ \"pull_request\" : { \"html_url\" : \"http://pr/1\" },\n\n            \"secret\": \"ABCDEF\" }' }\n            Handler.prHandler( robot, req, res )\n            expect( robot.messageRoom ).toHaveBeenCalled()\n            expect( httpSpy ).toHaveBeenCalled()\n\n            expect( res.send ).toHaveBeenCalled()\n            done()\n   ...\n\n\n   We are putting the secret inside the JSON as a convenience. The secret will\nnot come in with the JSON when GitHub sends us JSON via the webhook, but\nthis is an easy way to provide it to our handler for the moment. If we run our\n\ntests, they should pass now.\n\n\nSECURING THE WEBHOOK\n\n\nOur Probot is now in a position where it will operate correctly if the secret pass-\nes validation and the webhook data is passed properly. Now we need to secure\nthe webhook. GitHub signs your data inside the webhook payload which pro-\n\nvides you with a way to verify the data really came from an authorized host. We\nneed to decode it inside our handler. To do this, we will need to retrieve the se-\n\n\n\n\n                                                                                       267","initials":"ew"},"\"This doesn't sound secure?\"":{"page":267,"text":"                                                                                Activities API Overview\n\n\n   ...\n   exports.prHandler = ( robot, req, res ) ->\n            body = req.body\n            pr = JSON.parse body if body\n\n            url = pr.pull_request.html_url if pr\n            secret = pr.secret if pr\n\n            if secret == _SECRET and url\n                    room = \"general\"\n\n   ...\n\n   Here you see we pull out the body contents, process them as JSON, extract\n\nthe secret and the URL from the parsed JSON, and then go through our normal\nroutine.\n\n   Our tests are simple, and require that we send in JSON.\n\n   ...\n\n   it \"should disallow calls without the secret and url\", (done) ->\n            req = {}\n            Handler.prHandler( robot, req, res )\n            expect( robot.messageRoom ).not.toHaveBeenCalled()\n            expect( httpSpy ).not.toHaveBeenCalled()\n\n            expect( res.send ).toHaveBeenCalled()\n          done()\n\n   it \"should allow calls with the secret and url\", (done) ->\n            req = { body: '{ \"pull_request\" : { \"html_url\" : \"http://pr/1\" },\n\n            \"secret\": \"ABCDEF\" }' }\n            Handler.prHandler( robot, req, res )\n            expect( robot.messageRoom ).toHaveBeenCalled()\n            expect( httpSpy ).toHaveBeenCalled()\n\n            expect( res.send ).toHaveBeenCalled()\n            done()\n   ...\n\n\n   We are putting the secret inside the JSON as a convenience. The secret will\nnot come in with the JSON when GitHub sends us JSON via the webhook, but\nthis is an easy way to provide it to our handler for the moment. If we run our\n\ntests, they should pass now.\n\n\nSECURING THE WEBHOOK\n\n\nOur Probot is now in a position where it will operate correctly if the secret pass-\nes validation and the webhook data is passed properly. Now we need to secure\nthe webhook. GitHub signs your data inside the webhook payload which pro-\n\nvides you with a way to verify the data really came from an authorized host. We\nneed to decode it inside our handler. To do this, we will need to retrieve the se-\n\n\n\n\n                                                                                       267","initials":"ew"},"\"I'd add a citation explaining what timing attacks are and why constant time comparison is important\"":{"page":268,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                      cure hash GitHub provides inside the request headers. Then, we will need to\n                      calculate the hash ourselves using the secret we maintain internally. If these\n\n                      hashes match, then we know the incoming request and JSON is truly from Git-\n                      Hub and not an attacker.\n\n\n                         ...\n                         getSecureHash = (body, secret) ->\n                                  hash = crypto.\n                                           createHmac( 'sha1', secret ).\n\n                                           update( \"sha1=\" + body ).\n                                           digest('hex')\n                                  console.log \"Hash: #{hash}\"\n                                  hash\n\n\n                         exports.prHandler = ( robot, req, res ) ->\n                                  slack_users_url =\n                                           \"https://slack.com/api/users.list?token=\" +\n                                           process.env.HUBOT_SLACK_TOKEN\n\n                                  body = req.body\n                                  pr = JSON.parse body if body\n                                  url = pr.pull_request.html_url if pr\n                                  secureHash = getSecureHash( body, _SECRET ) if body\n                                  webhookProvidedHash = req.headers['HTTP_X_HUB_SIGNATURE' ] if req?.headers\n\n                                  secureCompare = require 'secure-compare'\n\n                                  if secureCompare( secureHash, webhookProvidedHash ) and url\n                                           room = \"general\"\n\n                                           robot.http( slack_users_url ) ->\n                                                   .get() (err, response, body) ->\n                                                            sendPrRequest( robot, body, room, url ) unless err\n                                  else\n                         ...\n\n\n                         HMAC cryptography is vulnerable to timing attacks. When you use this en-\n                      cryption technique, the time it takes to complete a comparison of the compu-\n\n                      ted hash and the sent hash can be the starting point for an attacker to gain\n                      forced access to a server. More specifically to JavaScript, when using naive\n\n                      comparison operators like ==  you can accidentally provide attackers with val-\n                      uable information. To eliminate this risk, we use a module called secure-\n\n                      compare that obscures this timing information when making a comparison. To\n                      load this module, we need to add it to our package.json manifest file with the\n\n                      command    npm install secure-compare --save       .\n                         Now we can adjust our tests to fit the new reality of our handler.\n\n\n                         ...\n                         it \"should disallow calls without the secret and url\", (done) ->\n                                  req = {}\n\n\n\n\n       268","initials":"ew"},"\"We seem to be meandering between topics a bit here?\"":{"page":269,"text":"                                                                                  Activities API Overview\n\n\n            Handler.prHandler( robot, req, res )\n            expect( robot.messageRoom ).not.toHaveBeenCalled()\n            expect( httpSpy ).not.toHaveBeenCalled()\n            expect( res.send ).toHaveBeenCalled()\n            done()\n\n\n   it \"should allow calls with the secret and url\", (done) ->\n            req = { body: '{ \"pull_request\" : { \"html_url\" : \"http://pr/1\" }}', headers: { \"HTTP_X_HUB_SIGNATURE\" : \"cd970490d83c01b▯678fa9af55f3c7854b5d22918\" } }\n            Handler.prHandler( robot, req, res )\n            expect( robot.messageRoom ).toHaveBeenCalled()\n\n            expect( httpSpy ).toHaveBeenCalled()\n            expect( res.send ).toHaveBeenCalled()\n            done()\n   ...\n\n\n   You’ll notice we moved the secret out of the JSON and into the headers. This\nis the same structure our Probot will see when the GitHub webhook encodes\n\nthe content of the JSON and provides us with a secure hash in the\nHTTP_X_HUB_SIGNATURE key. Inside our test we will need to provide the same\nsignature inside our mocked request object. We could duplicate our secure\n\nhash generation code from the handler implementation, or we could be lazy\nand just run our tests once (knowing they will fail this time), watch for the con-\n\nsole.log output which says “Hash: cd970490d83c…” and copy this hash into our\nmocked request object. Once we do this, our tests will pass.\n\n   Now, after reloading our Probot, if we issue a pull request using oissue-\npull-request.sh    script, we should see the matching hashes. But, we won’t\n\n(at least if you used the same package.json    file as we specified above) be-\ncause of a critical bug inside of Hubot at the time of this writing.\n   As we mentioned earlier, Hubot bundles Express.js, a high performance web\n\nframework for NodeJS. Express.js has a modular architecture, where middle-\nware is inserted into a request and response chain. This approach to building\n\nfunctionality and the wide array of middleware allows web developers to string\ntogether various standardized middleware components to use only those fea-\n\ntures needed for the problem at hand. Common middleware includes static file\nhandlers (for serving static files), cookie handlers, session handlers, and body\nparsers. You can imagine circumstances where you would not need all of the list\n\nabove (or you might need others) and this flexibility makes Express.js a popular\nchoice for building NodeJS web applications.\n\n   The body parser middleware is of particular interest to us here: the body\nparser middleware is used to convert the “body” of a request into a JavaScript\n\nobject attached to the request object. Above you saw us access it inside a vari-\nable we called req  inside our callback; obviously this stands for request. The\n\nbody parser takes on converting whatever data content comes from inside the\nbody of the HTTP request into a structured JavaScript associative array inside\n\n\n\n\n                                                                                         269","initials":"ew"},"\"The fact that we need to hack this in kind of begs the question of whether we should even be doing any of this? What is the reader going to gain from reading about this hack?\"":{"page":270,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                       the body  object inside our request object. If the body is url encoded (as the PR\n\n                       information is encoded if we create the webhook with the content_type   set to\n                       form ), then the body parser url decodes the content, parses it as JSON, and\n\n                       then sets the inflated object to the body attribute on our request object. Nor-\n                       mally, this is a very handy process that removes a lot of grunt work for web ap-\n\n                       plication authors.\n                          Unfortunately, because express is bundled and configured for us long before\n\n                       our extension is loaded, we cannot interrupt the load order of the body parser\n                       middleware inside our extension and this means we cannot get access to the\n                       raw body content. The body parser middleware processes the stream of data by\n\n                       registering for events inside of the HTTP request flow. NodeJS made a mark on\n                       web application development by providing a network application toolkit cen-\n\n                       tered around one of the most controversial features of JavaScript: the asyn-\n                       chronous callback. In NodeJS, processes register for events and then return\n                       control to the host program. In other languages, like Ruby for example, when\n\n                       building services which receive data from clients, by default, you listen for in-\n                       coming data, and the moment you tell your program to listen, you have blocked\n\n                       other processing. Asynchronous programming is by no means a new concept\n                       (threading in many languages, for example), but NodeJS offers a simple way to\n                       interact with asynchronous functions through event registration. In the case of\n\n                       express middleware, however, this event registration process bites us, because\n                       middleware loaded first gets first access to incoming data, and once the body\n\n                       parser has processed our body content, we no longer can access the original\n                       content. We need access to the raw body content, and there is no way to install\n                       our own middleware which would provide it inside our Probot extension when\n\n                       a PR request is received on the router.\n                          What options do we have then? Well, fortunately, every bit of our stack here\n\n                       is open source, and we can modify the code inside Hubot which sets up our ex-\n                       press server to fit our needs. This code is installed by thenpm  tool into the\n\n                       node_modules    directory and we can easily find where express is configured in-\n                       side of Hubot. There are issues with doing it this way: if we renpm install\n\n                       we will blow away our  node_modules    directory, and this is something Heroku\n                       will do if it is not told otherwise. A better way might be to fork Hubot and store\n                       our own copy of Hubot inside of GitHub and then specify our forked copy inside\n\n                       of thepackage.json    file. This has issues too; if Hubot gets updated with a crit-\n                       ical security flaw, we need to merge those changes into our fork, a maintenance\n\n                       issue which we would avoid if we use tagged releases from the main repository.\n                       There is, unfortunately, no perfect way to resolve this problem that does not it-\n\n                       self create other problems.\n                          If you do choose to modify the built in hubot code, modify the file    ro-\n\n                       bot.coffee     inside   the   node_modules/hubot/src/         directory.  The\n\n\n\n\n        270","initials":"ew"},"\"This code is quite ugly... Do we really need to cover this?\"":{"page":272,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                                  expect( res.send ).toHaveBeenCalled()\n                                  done()\n                         ...\n\n\n                         Our implementation breaks our tests, so we will need to modify the cost to\n                      use the rawBody  attribute on the request object, break it apart from the pay-\n\n                      load key-value pair, URI decode it, and then if all that works, parse the JSON\n                      and start the verification process. Our tests describe all this for us. The new\n\n                      prHandler   method looks like this:\n\n                         ...\n\n                         exports.prHandler = ( robot, req, res ) ->\n\n                                  rawBody = req.rawBody\n                                  body = rawBody.split( '=' ) if rawBody\n\n                                  payloadData = body[1] if body and body.length == 2\n                                  if payloadData\n                                          decodedJson = decodeURIComponent payloadData\n                                          pr = JSON.parse decodedJson\n\n\n                                          if pr and pr.pull_request\n                                                   url = pr.pull_request.html_url\n                                                   secureHash = getSecureHash( rawBody )\n                                                   signatureKey = \"x-hub-signature\"\n                                                   webhookProvidedHash = req.headers[ signatureKey ] if req?.headers\n\n                                                   secureCompare = require 'secure-compare'\n                                                   if secureCompare( \"sha1=#{secureHash}\", webhookProvidedHash ) and url\n                                                           room = \"general\"\n                                                           users = robot.brain.users()\n\n                                                           sendPrRequest( robot, users, room, url )\n                                                   else\n                                                           console.log \"Invalid secret or no URL specified\"\n                                          else\n                                                   console.log \"No pull request in here\"\n\n\n                                  res.send \"OK\\n\"\n\n                         _GITHUB = undefined\n\n                         ...\n\n                         When all is said and done, is verifying the signature even worth it? If we are\n\n                      not hosting our Probot on a service which handles our router requests over\n                      HTTPS, this HMAC verification could be compromised. And, given the issues\n                      with maintaining our own copy of the Hubot code in order to permit the valida-\n\n                      tion inside our Hubot extension, it might be best to ignore the validation head-\n                      er. The worst case, as our extension is written now, would be that an attacker\n\n                      could fake a pull request notification, and falsely engage chat room users\n\n\n\n\n       272","initials":"ew"},"\"Whilst comedy is appreciated, this is a little unsettling. Again it begs the question of why are we doing this random allocation?\"":{"page":273,"text":"                                                                                 Activities API Overview\n\n\naround it. If the PR the attacker used was fake, it might confuse our Probot, but\nno real harm would be done. If they used an existing real PR, an attacker could\ntrick our Probot into adding data to the PR, adding confusion in the comments\n\nabout who accepted the review request. We won’t solve that potential problem\nwith this code, but you can imagine adding code to our Probot that handles a\n\ncase like this (for example, by checking first to see if someone was already tag-\nged on the PR, and ignoring successive incoming webhooks associated with\n\nthat PR).\n\n\nRESPONDING TO THE PR REQUEST\n\nOur Probot is now programmed to generate a pull request review message and\nsend it to a random user. What happens when they respond? They can respond\n\nin two ways obviously: accepting the request or declining the request. We put\nplaceholders in our Probot extension to notify us with a debugging message\n\nwhen the user responds and send a message back to whoever sent us a mes-\nsage, but now we can actually wire up handling the response and adding to the\n\npull request on GitHub based on the user who we are interacting with (provided\nthey accepted).\n   There are multiple ways in which a Hubot can interact with chat room mes-\n\nsages. We chose the respond  method, but there is another method hear  which\n\nwe could have used.  respond  is used when the message is preceeded by the\nHubot name, so only messages that look like    probot: accept    or @probot\n\ndecline  or / accept  (if the Hubot name alias is enabled) will be processed by\nour Probot. We could have used  hear but in our case we are processing a sim-\n\nple response, and without a clear direction for the message, it would be difficult\nto always make sure we were interpreting the message in the correct context.\nrespond  makes more sense here.\n\n   If they decline the request, there are a few options for what we could do. We\ncould publicly shame the user inside the channel. This seems counter to a cul-\n\nture which supports creative individuals like software engineers, so let’s not do\nthat. We could ask another user in the slack channel to help. This is a better\n\noption. It will require modification to our code, but these modifications don’t\ninvolve anything new inside the Hubot API and are a little tedious to explain. If\nyou review the source code for the Probot repository that accompanies this\n\nchapter hosted on GitHub, you’ll see a working version of decline that performs\nthis second option. For purposes of this chapter, let’s just graciously note that\n\nthe offer was declined.\n\n   ...\n\n   exports.decline = ( res ) ->\n            res.reply \"No problem, we'll go through this PR in a bug scrub\"\n\n\n\n\n                                                                                         273","initials":"ew"},"\"The writing style has a very repetitive flow and it reads very much like: and then.... and then..... and then..... and then...\\n\\nConsider changing how information is presented. Vary the cadence. Break things into smaller sections.\"":{"page":275,"text":"                                                                                Activities API Overview\n\n\nhandler works correctly with an accept  and decline   method (they don’t yet\nexist in our handler code so we’ll add them next).\n\n   Our accept request handler has code which triggers our Probot to contact\nGitHub and add a comment to the pull request noting our targetted chat user\naccepted the request, and the network connection to the GitHub API is done\n\nthrough the GitHub API bindings on the   node-github    module. We want to\nmake this testable, so we should pass in the GitHub binding object inside our\n\ninterface, and during the test, pass in a mocked object. If we review the docu-\nmentation for thecreateComment    in the GitHub API binding, we see it requires\n\ninformation about the repository such as the user or organization which owns\nthe repository, the repository name, the issue number (pull requests also are\n\nreferenced by issue numbers) and the comment itself. To get this information\nwe simply need to decode it from the Probot handler which receives the pull\nrequest information, and we will add code which does this (and is exposed in\n\nour handler for testing). We saw that a pull request comes in through a large\nJSON response, and we can use the URL we used earlier as the way we decode\n\nthis information. So, we’ll need to have two more tests inside ou#response\nblock, one for the decoding of the URL into a message object, and another to\n\nretrieve the username which we insert into the comment stored in the pull re-\nquest on the repository. We know what our test URL looks like since we saw it in\nour PR webhook message, but we don’t yet have the structure of the chat mes-\n\nsage from which we can pull out our username, so our test will need to be ad-\njusted when we know what it really looks like.\n\n   Declining the request means nothing happens. If we mock out our GitHub\nAPI binding, acceptance should login (using the authenticate   method) and\n\nthen callcreateComment   . These are directly pulled from the GitHub API No-\ndeJS documentation. Finally, we should record the result of this operation in-\n\nside the chat room which happens using the reply method on our response ob-\nject.\n\n\n   ...\n   describe \"#response\", ->\n            createComment = jasmine.createSpy( 'createComment' ).and.\n                    callFake( ( msg, cb ) -> cb( false, \"some data\" ) )\n            issues = { createComment: createComment }\n\n            authenticate = jasmine.createSpy( 'ghAuthenticate' )\n            responder = { reply: jasmine.createSpy( 'reply' ),\n            send: jasmine.createSpy( 'send' ) }\n\n\n            beforeEach ->\n                    githubBinding = { authenticate: authenticate, issues: issues }\n                    github = Handler.setApiToken( githubBinding, \"ABCDEF\" )\n                    req = { body: '{ \"pull_request\" : { \"url\" : \"http://pr/1\" }}', headers: { \"HTTP_X_HUB_SIGNATURE\" : \"cd970490d83c▯01b678fa9af55f3c7854b5d22918\" } }\n                    Handler.prHandler( robot, req, responder )\n\n\n\n\n                                                                                        275","initials":"ew"},"\"Use more footnotes to leave links!\"":{"page":276,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n\n                                  it \"should tag the PR on GitHub if the user accepts\", (done) ->\n                                           Handler.accept( responder )\n                                           expect( authenticate ).toHaveBeenCalled()\n\n                                           expect( createComment ).toHaveBeenCalled()\n                                           expect( responder.reply ).toHaveBeenCalled()\n                                           done()\n\n                                  it \"should not tag the PR on GitHub if the user declines\", (done) ->\n\n                                           Handler.decline( responder )\n                                           expect( authenticate ).toHaveBeenCalled()\n                                           expect( createComment ).not.toHaveBeenCalledWith()\n                                           expect( responder.reply ).toHaveBeenCalled()\n\n                                           done()\n\n                                  it \"should decode the URL into a proper message object for the createMessage call\", (done) ->\n                                           url = \"https://github.com/xrd/testing_repository/pull/1\"\n                                           msg = Handler.decodePullRequest( url )\n\n                                           expect( msg.user ).toEqual( \"xrd\" )\n                                           expect( msg.repository ).toEqual( \"testing_repository\" )\n                                           expect( msg.number ).toEqual( \"1\" )\n                                           done()\n\n\n                                  it \"should get the username from the response object\", (done) ->\n                                           res = { username: { name: \"Chris Dawson\" } }\n                                           expect( Handler.getUsernameFromResponse( res ) ).toEqual \"Chris Dawson\"\n                                           done()\n\n\n                          Our tests will fail if we run them now. So, let’s write the code at the end of\n                       our delegator extension. We need code which parses the URL into the appropri-\n\n                       ate structured message object, code to put the reminder into the pull request\n                       comment on GitHub and code which pulls the user out of the response object\n\n                       passed to us. The first two of these are within reach; basic JavaScript and read-\n                       ing the GitHub API binding documentation will get us to these two. The third\n\n                       one requires a little more investigation, so we will leave this as a placeholder\n                       for now.\n\n                          To convert the URL into the object necessary for the createMessage   call,\n                       we just need to split the message into pieces by the slash character, and then\n                       retrieve the correct items by index. We probably could add some additional\n\n                       tests which cover passing in empty strings, or other edge cases, but we’ll leave\n                       it as an exercise to the reader (or you can review the final test cases on the asso-\n\n                       ciated GitHub project page). Our code does not crash in these cases, but it\n                       would be nice to have coverage of our expectations represented in our tests.\n\n\n                          ...\n                          _GITHUB = undefined\n                          _PR_URL = undefined\n\n\n\n\n       276","initials":"ew"},"\"1) There is a lot of uncertainty in the tone of this line, it takes away the creditability.\\n2) Whilst this code may look simple, I'd argue that all the pages of explanation to get here would indicate that it actually isn't.\"":{"page":278,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                         handler = require '../lib/handler'\n\n                         handler.setSecret \"XYZABC\"\n                         github = require 'node-github'\n                         handler.setApiToken github, \"12345ABCDEF\"\n\n\n                         module.exports = (robot) ->\n                                  robot.respond /accept/i, ( res ) ->\n                                          handler.accept( res )\n\n\n                                  robot.respond /decline/i, ( res ) ->\n                                          handler.decline( res )\n\n                                  robot.router.post '/pr', ( req, res ) ->\n                                          handler.prHandler( robot, req, res )\n\n\n                         Hopefully you will agree this is a very simple starting point for our extension,\n                      with the bulk of the work handled by our very testable handler.\n\n\n                      PEERING INTO THE RESPONSE OBJECT\n\n\n                      We need to get the username and it stands to reason the object passed to us\n                      when we get a respond callback might have it in there. The respond  method\n\n                      provided by the Hubot API is documented mostly by way of the example scripts\n                      which come with hubot. There is very little information on what the parameter\n\n                      passed to your callback looks like. Let’s useutil  library to inspect the data\n                      and print it to the console. We abbreviate the full output here, and show you\n\n                      that it contains information on the user who sent the message to our Probot.\n                      We can access this information by usingresponse.message.user.name      if, for\n\n                      example, we wanted to retrieve the name of the user.\n\n                         { robot:\n                            { name: 'probot',\n\n                               events: { domain: null, _events: [Object], _maxListeners: 10 },\n                               brain:\n                                { data: [Object],\n                                  autoSave: false,\n                                  saveInterval: [Object],\n\n                                  _events: [Object] },\n                               alias: false,\n                               adapter:\n                                { customMessage: [Function],\n\n                                  message: [Function],\n                           ...\n                           message:\n                            { user:\n                                { id: '...',\n\n\n\n\n       278","initials":"ew"},"\"Seems a bit much to just dump this on the reader\"":{"page":279,"text":"                                                                               Activities API Overview\n\n\n           name: 'xrd',\n           real_name: 'Chris Dawson',\n           email_address: 'cdawson@webiphany.com',\n           room: 'xrd' },\n\n        text: 'probot accept',\n        rawText: 'accept',\n        rawMessage:\n         { _client: [Object],\n           deleteMessage: [Function],\n\n           updateMessage: [Function],\n           type: 'message',\n           channel: 'D038PNPU6t',\n           user: '030YMBJYU',\n\n           text: 'accept',\n           ts: '1428436496.000012',\n           team: '0T03MYBJU' },\n        id: '1428436496.000012',\n        done: false,\n\n        room: 'xrd' },\n     match: [ 'probot accept', index: 0, input: 'probot accept' ],\n     envelope:\n      { room: 'xrd',\n        user:\n\n         { id: '5AY9MBQZ',\n           name: 'xrd',\n           real_name: 'Chris Dawson',\n           email_address: 'cdawson@webiphany.com',\n\n           room: 'xrd' },\n        message:\n         { user: [Object],\n           text: 'probot accept',\n           rawText: 'accept',\n\n           rawMessage: [Object],\n           id: '1428436496.000012',\n           done: false,\n           room: 'xrd' } } }\n\n\n   Inside it all we can find information we need, specifically the user name and\nemail. So, let’s update our test and our handler code. The last test in our spec\n\nfile can be modified to look like this:\n\n\n   ...\n   it \"should get the username from the response object\", (done) ->\n           res = { message: { user: { name: \"Chris Dawson\" } } }\n           expect( Handler.getUsernameFromResponse( res ) ).toEqual \"Chris Dawson\"\n           done()\n\n\n   ...\n\n\n\n\n\n                                                                                      279","initials":"ew"},"\"What is the Slack user is different from the GitHub user but they share a username?\"":{"page":280,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                          And, our handler code defining   getUsernameFromResponse       simply turns\n                       into this:\n\n\n                          ...\n                          exports.getUsernameFromResponse = ( res ) ->\n                                   res.message.user.name\n\n\n                          ...\n\n                          With this information in hand, we can properly comment on the pull request.\n\n                       Well, almost.\n\n\n                       UNIFYING USERNAMES VIA THE COLLABORATORS API\n\n                       If the Slack username for the person who accepted the pull request is an exact\n\n                       match with their GitHub username, then we can assume they are the same per-\n                       son in real life and create a comment inside the pull request reminding them\n                       (and anyone else) that they will be reviewing the PR. We can use the collabora-\n\n                       tor sub section of the Repository API to look up their name on GitHub.\n                          If we don’t find them inside the list of users and there is not an exact match\n\n                       with their Slack name then we have at least one problem, maybe two. First, we\n                       could just have a mismatch in their identities (their usernames are different on\n\n                       each site). If this is the case, we could ask them to clarify this inside the slack\n                       room. We do have another case: the user is not a collaborator on the repository\n                       hosted on GitHub. If this is the case, clarifying their username is not going to\n\n                       help. The Repository API does support adding a user to the list of collaborators\n                       so we could do that here, but this arguably is a moment where a larger discus-\n\n                       sion should happen (write access to a repository is a big resposibility in a way\n                       that being inside a chat room is not). Adding a user as a repository collaborator\n                       should not be automated inside a chat room. Because of the complexity here,\n\n                       we will write code to unify a username inside the chat room, but we won’t han-\n                       dle the case where there is no clarification to be made because they are not in\n\n                       the repository collaborator list.\n                          Using the GitHub API binding binding we passed into our   setApiToken   call\n\n                       we will verify the user exists as a collaborator on the repository. The API binding\n                       provides a method called    getCollaborator     inside the  repos  namespace\n\n                       which we can use to verify that a username is on the list of collaborators. It\n                       takes as the first parameter a message which is used to specify the repository\n\n                       and owner, and then an attribute called   collabuser   which is the name you\n                       want to verify is a collaborator. The second parameter to the function is a call-\n                       back that is executed once the request has completed. If the callback returns\n\n                       without an error code, then our Probot should tag the pull request with a com-\n                       ment confirming and message the room.\n\n\n\n\n        280","initials":"ew"},"\"code is cut off\"":{"page":281,"text":"                                                                                Activities API Overview\n\n\n   Our new test reflects usage of trepos.getCollaborator     call. In our test\nsetup block we mocking out the call togetCollaborator    and using Jasmine\n\nto “spy on” it so we can assure it was called later in our actual test. Our setup is\nmore beefy than before, but we are following the same patterns of generating\n\nspies to watch methods, and implementing our fake callbacks when necessary.\nWe also can move our message inside the response object into the one created\n\nin our setup block so that we can use it inside all of our sub-tests, rather than\ncreating a new object for each test inside the test body.\n\n\n   ...\n   send: jasmine.createSpy( 'send' ),\n   message: { user: { name: \"Chris Dawson\" } } }\n   getCollaborator = jasmine.createSpy( 'getCollaborator' ).and.\n\n            callFake( ( msg, cb ) -> cb( false, true ) )\n   repos = { getCollaborator: getCollaborator }\n\n   ...\n\n\n   it \"should tag the PR on GitHub if the user accepts\", (done) ->\n            Handler.accept( robot, responder )\n            expect( authenticate ).toHaveBeenCalled()\n            expect( createComment ).toHaveBeenCalled()\n\n            expect( responder.reply ).toHaveBeenCalled()\n            expect( repos.getCollaborator ).toHaveBeenCalled()\n            done()\n\n\n   Our handler then can implement the accept and decline methods in full.\n\n   ...\n\n   exports.accept = ( robot, res ) ->\n\n            prNumber = res.match[1]\n            url = robot.brain.get( prNumber )\n\n\n            msg = exports.decodePullRequest( url )\n            username = exports.getUsernameFromResponse( res )\n            msg.collabuser = username\n\n\n            _GITHUB.repos.getCollaborator msg, ( err, collaborator ) ->\n                    msg.body = \"@#{username} will review this (via Probot).\"\n\n                    _GITHUB.issues.createComment msg, ( err, data ) ->\n                             unless err\n\n                                     res.reply \"Thanks, I've noted that in a PR comment. Review the PR here: \"\n                             else\n                                     res.reply \"Something went wrong, I could not tag you on the PR comment: #{require('util').insp▯ect( err )}\"\n\n   exports.decline = ( res ) ->\n\n\n\n\n                                                                                       281","initials":"ew"},"\"cut off\"":{"page":282,"text":"CHAPTER 10: CoffeScript, Hubot and the Activities API\n\n\n                                  res.reply \"No problem, we'll go through this PR in a bug scrub\"\n                         ...\n\n                         We now have a full implementation of both the accept and  decline  meth-\n\n                      ods inside our Probot.\n\n\n                      SANITIZING OUR SOURCE CODE\n\n                      It is typically bad form to save passwords (or other access credentials, like\n\n                      oAuth tokens or secrets) inside of source code. Right now we have hard coded\n                      them into our application inside of thepr-delegator.coffee    file. We could\n\n                      instead retrieve them from the environment of the running process.\n\n                         ...\n\n                         handler.setSecret process.env.PROBOT_SECRET\n                         github = require 'github'\n                         ginst = new github version: '3.0.0'\n                         handler.setApiToken ginst, process.env.PROBOT_API_TOKEN\n                         ...\n\n\n                         When we launch our probot from the command line, we will need to use a\n                      command like this as we are testing locally from our laptop.\n\n\n                         $ PROBOT_SECRET=XYZABC PROBOT_API_TOKEN=926a701550d4dfae93250dbdc068cce887531 HUBOT_SLACK_TOKEN=xoxb-3295776784-nZxl1H3nyLsVcgdD▯29r1PZCq ./bin/hubot -a slack\n\n\n                         When we publish into Heroku, we will want to set these as environment vari-\n                      ables using the appropriate Heroku commands.\n\n\n                         $ heroku config:set PROBOT_API_TOKEN=926a701550d4dfae93250dbdc068cce887531\n                         Adding config vars and restarting myapp... done, v12\n                         PROBOT_API_TOKEN=926a701550d4dfae93250dbdc068cce887531\n\n\n                         $ heroku config:set PROBOT_SECRET=XYZABC\n                         Adding config vars and restarting myapp... done, v12\n                         PROBOT_SECRET=XYZABC\n\n\n                         Don’t forget that when we run our tests, we will need to specify the environ-\n                      ment variables on the command line as well.\n\n\n                         $ PROBOT_SECRET=XYZABC PROBOT_API_TOKEN=926a701550d4dfae93250dbdc068cce887531 node_modules/jasmine-node/bin/jasmine-node --coffe▯e spec/pr-de\n                         legator.spec.coffee\n\n\n\n\n\n\n\n\n\n\n       282","initials":"ew"},"\"This chapter feels incredibly slow to begin with and then the ending seems incredibly rushed in comparison.\\n\\nAgain I'm not sure what the reader has learnt here...\"":{"page":283,"text":"                                                                                        Summary\n\n\n\nSummary\n\n\nOur Probot is alive! We went through building a robot which can interact with\nus inside a chat room, then refactored the robot so that its functionality is con-\ntained into a highly testable module. Along the way, we got intimate with the\n\nHubot API, and even discussed how to modify (and the drawbacks surrounding)\nmodifying the source code to Hubot itself. Finally we demonstrated how to use\n\nthe Activities API receiving (and faking data) coming a GitHub webhook. There\nis a natural fit to Hubot and the GitHub API, and this chapter shows how easy\nand fun it can be. Using Hubot you can amplify and streamline developer work-\n\nflow within the dialog happening between your vibrant developers and this\nhelpful robot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                     283","initials":"ew"},"\"There is no conclusion for the entire book!\"":{"page":283,"text":"                                                                                        Summary\n\n\n\nSummary\n\n\nOur Probot is alive! We went through building a robot which can interact with\nus inside a chat room, then refactored the robot so that its functionality is con-\ntained into a highly testable module. Along the way, we got intimate with the\n\nHubot API, and even discussed how to modify (and the drawbacks surrounding)\nmodifying the source code to Hubot itself. Finally we demonstrated how to use\n\nthe Activities API receiving (and faking data) coming a GitHub webhook. There\nis a natural fit to Hubot and the GitHub API, and this chapter shows how easy\nand fun it can be. Using Hubot you can amplify and streamline developer work-\n\nflow within the dialog happening between your vibrant developers and this\nhelpful robot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                     283","initials":"ew"},"\"What would you recommend as a starting point for people without any experience using GitHub or Git?\"":{"page":9,"text":"                                               Preface            1\n\n\n\n\n\n\n\n\nIn the 1930s, The Eagle and Child, a pub in Oxford simmered with creativity as\nJRR Tolkien and CS Lewis philosophized and created their fantasy worlds.\nHeian court life in 11th century Kyoto cradled Murasaki Shibu and produced\nThe Tale of Genji, Japan’s greatest novel. In the 7th century during the\nUmayyad Caliphate of modern day Damascus, glittering Arabic palaces provid-\ned fertile ground for the creation of a new form of poetry, the ghazal (“love po-\n\nems”) the influence of which still courses through modern Arabic poetry.\n  Each era of major creative innovation has been backdropped by a unique\nand plumb space where collaboration and creativity flourished. Today, the col-\nlaborative gathering space for some of the world’s greatest artists (artists who\nwork as software developers) is a virtual town square, a site called GitHub.\n\n\nWho You Are\n\nThis book should be an interesting source of information for people who have\n\nused Git or GitHub and want to “level-up” their skills related to these technolo-\ngies. People without any experience using GitHub or Git should start with an in-\ntroductory book on these technologies.\n  You should have a good familiarity with at least one imperative modern pro-\ngramming language. You don’t need to be an expert programmer to read this\nbook, but having some programming experience and familiarity with at least\none language is essential.\n\n  You should understand the basics of the HTTP protocol. The GitHub team\nuses a very standard RESTful approach for its API. You should understand the\ndifference between a GET request and POST request and what HTTP status\ncodes mean at the very least.\n  Familiariy with web APIs is helpful, although this book simultaneously as-\npires to provide a guide showing how a well thought out, well designed, and\nwell tested web API creates a foundation for building fun and powerful tools. If\n\n\n\n\n\n                                                                         9","initials":"tr"},"\"First impression: This first paragraph is very dense. It makes me worry this book is going to be hard to read. I understand what you are trying to accomplish but I think it can be accomplished with fewer words and fewer examples.\"":{"page":9,"text":"                                               Preface            1\n\n\n\n\n\n\n\n\nIn the 1930s, The Eagle and Child, a pub in Oxford simmered with creativity as\nJRR Tolkien and CS Lewis philosophized and created their fantasy worlds.\nHeian court life in 11th century Kyoto cradled Murasaki Shibu and produced\nThe Tale of Genji, Japan’s greatest novel. In the 7th century during the\nUmayyad Caliphate of modern day Damascus, glittering Arabic palaces provid-\ned fertile ground for the creation of a new form of poetry, the ghazal (“love po-\n\nems”) the influence of which still courses through modern Arabic poetry.\n  Each era of major creative innovation has been backdropped by a unique\nand plumb space where collaboration and creativity flourished. Today, the col-\nlaborative gathering space for some of the world’s greatest artists (artists who\nwork as software developers) is a virtual town square, a site called GitHub.\n\n\nWho You Are\n\nThis book should be an interesting source of information for people who have\n\nused Git or GitHub and want to “level-up” their skills related to these technolo-\ngies. People without any experience using GitHub or Git should start with an in-\ntroductory book on these technologies.\n  You should have a good familiarity with at least one imperative modern pro-\ngramming language. You don’t need to be an expert programmer to read this\nbook, but having some programming experience and familiarity with at least\none language is essential.\n\n  You should understand the basics of the HTTP protocol. The GitHub team\nuses a very standard RESTful approach for its API. You should understand the\ndifference between a GET request and POST request and what HTTP status\ncodes mean at the very least.\n  Familiariy with web APIs is helpful, although this book simultaneously as-\npires to provide a guide showing how a well thought out, well designed, and\nwell tested web API creates a foundation for building fun and powerful tools. If\n\n\n\n\n\n                                                                         9","initials":"tr"},"\"\\xFE\\xFF\\x00S\\x00h\\x00o\\x00u\\x00l\\x00d\\x00 \\x00t\\x00h\\x00i\\x00s\\x00 \\x00s\\x00a\\x00y\\x00  \\x1C\\x00T\\x00h\\x00e\\x00 \\x00G\\x00i\\x00t\\x00H\\x00u\\x00b\\x00 \\x00t\\x00e\\x00a\\x00m \\x1D\\x00 \\x00o\\x00r\\x00 \\x00j\\x00u\\x00s\\x00t\\x00  \\x1C\\x00G\\x00i\\x00t\\x00H\\x00u\\x00b \\x1D\\x00?\"":{"page":9,"text":"                                               Preface            1\n\n\n\n\n\n\n\n\nIn the 1930s, The Eagle and Child, a pub in Oxford simmered with creativity as\nJRR Tolkien and CS Lewis philosophized and created their fantasy worlds.\nHeian court life in 11th century Kyoto cradled Murasaki Shibu and produced\nThe Tale of Genji, Japan’s greatest novel. In the 7th century during the\nUmayyad Caliphate of modern day Damascus, glittering Arabic palaces provid-\ned fertile ground for the creation of a new form of poetry, the ghazal (“love po-\n\nems”) the influence of which still courses through modern Arabic poetry.\n  Each era of major creative innovation has been backdropped by a unique\nand plumb space where collaboration and creativity flourished. Today, the col-\nlaborative gathering space for some of the world’s greatest artists (artists who\nwork as software developers) is a virtual town square, a site called GitHub.\n\n\nWho You Are\n\nThis book should be an interesting source of information for people who have\n\nused Git or GitHub and want to “level-up” their skills related to these technolo-\ngies. People without any experience using GitHub or Git should start with an in-\ntroductory book on these technologies.\n  You should have a good familiarity with at least one imperative modern pro-\ngramming language. You don’t need to be an expert programmer to read this\nbook, but having some programming experience and familiarity with at least\none language is essential.\n\n  You should understand the basics of the HTTP protocol. The GitHub team\nuses a very standard RESTful approach for its API. You should understand the\ndifference between a GET request and POST request and what HTTP status\ncodes mean at the very least.\n  Familiariy with web APIs is helpful, although this book simultaneously as-\npires to provide a guide showing how a well thought out, well designed, and\nwell tested web API creates a foundation for building fun and powerful tools. If\n\n\n\n\n\n                                                                         9","initials":"tr"},"\"This paragraph feels redundant.\"":{"page":9,"text":"                                               Preface            1\n\n\n\n\n\n\n\n\nIn the 1930s, The Eagle and Child, a pub in Oxford simmered with creativity as\nJRR Tolkien and CS Lewis philosophized and created their fantasy worlds.\nHeian court life in 11th century Kyoto cradled Murasaki Shibu and produced\nThe Tale of Genji, Japan’s greatest novel. In the 7th century during the\nUmayyad Caliphate of modern day Damascus, glittering Arabic palaces provid-\ned fertile ground for the creation of a new form of poetry, the ghazal (“love po-\n\nems”) the influence of which still courses through modern Arabic poetry.\n  Each era of major creative innovation has been backdropped by a unique\nand plumb space where collaboration and creativity flourished. Today, the col-\nlaborative gathering space for some of the world’s greatest artists (artists who\nwork as software developers) is a virtual town square, a site called GitHub.\n\n\nWho You Are\n\nThis book should be an interesting source of information for people who have\n\nused Git or GitHub and want to “level-up” their skills related to these technolo-\ngies. People without any experience using GitHub or Git should start with an in-\ntroductory book on these technologies.\n  You should have a good familiarity with at least one imperative modern pro-\ngramming language. You don’t need to be an expert programmer to read this\nbook, but having some programming experience and familiarity with at least\none language is essential.\n\n  You should understand the basics of the HTTP protocol. The GitHub team\nuses a very standard RESTful approach for its API. You should understand the\ndifference between a GET request and POST request and what HTTP status\ncodes mean at the very least.\n  Familiariy with web APIs is helpful, although this book simultaneously as-\npires to provide a guide showing how a well thought out, well designed, and\nwell tested web API creates a foundation for building fun and powerful tools. If\n\n\n\n\n\n                                                                         9","initials":"tr"},"\"\\\"this is good company\\\" doesn't seem right here.\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n\n                        of APIs, this is good company.\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n\n                        projects and are now interested in using Git within a larger community, this\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n\n                        Hub.\n                           Like any seasoned developer, automation of your tools is important to you,\n\n                        and this book provides examples of mundane tasks that we then convert them\n                        into automated and repeatable processes, and we show how to do this using a\n\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n                        understanding of how to use it, and we bet you will find great power there. To\n\n                        make this book accessible to everyone, regardless of their editor or operating\n                        system, many of the programming samples work within the command line. If\n\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n\n                        very likely see a new way of working in the modern age. We focus on these\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n                        you can review the code discussed. Fork away and take these samples into your\n\n                        own projects and tools!\n                           Finally, we help you write testable API backed code. Even the most experi-\n\n                        enced developers often find that writing tests for their code is a challenge, de-\n                        spite the massive body of literature connecting quality code with tests. Testing\n\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n        10","initials":"tr"},"\"I don't find these examples to be very compelling. I feel like the corporate engineer who is trying to improve processes and automation within his team is a more powerful and likely example.\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n\n                        of APIs, this is good company.\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n\n                        projects and are now interested in using Git within a larger community, this\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n\n                        Hub.\n                           Like any seasoned developer, automation of your tools is important to you,\n\n                        and this book provides examples of mundane tasks that we then convert them\n                        into automated and repeatable processes, and we show how to do this using a\n\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n                        understanding of how to use it, and we bet you will find great power there. To\n\n                        make this book accessible to everyone, regardless of their editor or operating\n                        system, many of the programming samples work within the command line. If\n\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n\n                        very likely see a new way of working in the modern age. We focus on these\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n                        you can review the code discussed. Fork away and take these samples into your\n\n                        own projects and tools!\n                           Finally, we help you write testable API backed code. Even the most experi-\n\n                        enced developers often find that writing tests for their code is a challenge, de-\n                        spite the massive body of literature connecting quality code with tests. Testing\n\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n        10","initials":"tr"},"\"\\\"accidentally\\\" feels like a strange word choice here. Is it really necessary?\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n\n                        of APIs, this is good company.\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n\n                        projects and are now interested in using Git within a larger community, this\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n\n                        Hub.\n                           Like any seasoned developer, automation of your tools is important to you,\n\n                        and this book provides examples of mundane tasks that we then convert them\n                        into automated and repeatable processes, and we show how to do this using a\n\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n                        understanding of how to use it, and we bet you will find great power there. To\n\n                        make this book accessible to everyone, regardless of their editor or operating\n                        system, many of the programming samples work within the command line. If\n\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n\n                        very likely see a new way of working in the modern age. We focus on these\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n                        you can review the code discussed. Fork away and take these samples into your\n\n                        own projects and tools!\n                           Finally, we help you write testable API backed code. Even the most experi-\n\n                        enced developers often find that writing tests for their code is a challenge, de-\n                        spite the massive body of literature connecting quality code with tests. Testing\n\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n        10","initials":"tr"},"\"\\\"that we then convert them into automated\\\" - I think \\\"them\\\" should be removed\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n\n                        of APIs, this is good company.\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n\n                        projects and are now interested in using Git within a larger community, this\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n\n                        Hub.\n                           Like any seasoned developer, automation of your tools is important to you,\n\n                        and this book provides examples of mundane tasks that we then convert them\n                        into automated and repeatable processes, and we show how to do this using a\n\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n                        understanding of how to use it, and we bet you will find great power there. To\n\n                        make this book accessible to everyone, regardless of their editor or operating\n                        system, many of the programming samples work within the command line. If\n\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n\n                        very likely see a new way of working in the modern age. We focus on these\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n                        you can review the code discussed. Fork away and take these samples into your\n\n                        own projects and tools!\n                           Finally, we help you write testable API backed code. Even the most experi-\n\n                        enced developers often find that writing tests for their code is a challenge, de-\n                        spite the massive body of literature connecting quality code with tests. Testing\n\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n        10","initials":"tr"},"\"I think this should be two sentences split at \\\"and we show you\\\"\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n\n                        of APIs, this is good company.\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n\n                        projects and are now interested in using Git within a larger community, this\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n\n                        Hub.\n                           Like any seasoned developer, automation of your tools is important to you,\n\n                        and this book provides examples of mundane tasks that we then convert them\n                        into automated and repeatable processes, and we show how to do this using a\n\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n                        understanding of how to use it, and we bet you will find great power there. To\n\n                        make this book accessible to everyone, regardless of their editor or operating\n                        system, many of the programming samples work within the command line. If\n\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n\n                        very likely see a new way of working in the modern age. We focus on these\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n                        you can review the code discussed. Fork away and take these samples into your\n\n                        own projects and tools!\n                           Finally, we help you write testable API backed code. Even the most experi-\n\n                        enced developers often find that writing tests for their code is a challenge, de-\n                        spite the massive body of literature connecting quality code with tests. Testing\n\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n        10","initials":"tr"},"\"Swap the first and second sentence of this paragraph?\"":{"page":10,"text":"CHAPTER 1: Preface\n\n\n                        you have not used web APIs extensively, but have experience using other types\n\n                        of APIs, this is good company.\n\n\n                        What You Will Learn\n\n\n                        Much of the book focuses on the technical capabilities exposed by GitHub and\n\n                        the powerful GitHub API. Perhaps you feel constrained by using Git only from\n                        within a certain toolset; for example, if you are an Android developer acciden-\n\n                        tally using Git to manage your app source code and want to unlock Git in other\n                        places in your life as a developer, this book provides a wider vista to learn about\n                        the power of Git and GitHub. If you have fallen into using Git for your own\n\n                        projects and are now interested in using Git within a larger community, this\n                        book can teach you all about the “social coding” style pioneered and dogfoo-\n\n                        ded by the GitHub team. This book provides a stepping stone for software de-\n                        velopers who have used other distributed version control systems and are look-\n                        ing for a bridge to using their skills with Git and within a web service like Git-\n\n                        Hub.\n                           Like any seasoned developer, automation of your tools is important to you,\n\n                        and this book provides examples of mundane tasks that we then convert them\n                        into automated and repeatable processes, and we show how to do this using a\n\n                        variety of languages talking to the GitHub API.\n                           If you are unfamiliar with the “command line” this book will give you a firm\n                        understanding of how to use it, and we bet you will find great power there. To\n\n                        make this book accessible to everyone, regardless of their editor or operating\n                        system, many of the programming samples work within the command line. If\n\n                        you have hated the command line since your father forced you to use it when\n                        you were five, this is the perfect book to rekindle a loving relationship with the\n\n                        bash shell.\n                           If you absorb not only the technical facets of using GitHub but also pay at-\n                        tention to the cultural and ideological changes offered behind the tools, you’ll\n\n                        very likely see a new way of working in the modern age. We focus on these\n                        “meta” viewpoints as we discuss the tools themselves to help you see these ex-\n\n                        tra opportunities.\n                           Almost every chapter has an associated repository hosted on GitHub where\n                        you can review the code discussed. Fork away and take these samples into your\n\n                        own projects and tools!\n                           Finally, we help you write testable API backed code. Even the most experi-\n\n                        enced developers often find that writing tests for their code is a challenge, de-\n                        spite the massive body of literature connecting quality code with tests. Testing\n\n                        can be especially challenging when you are testing something backed by an\n                        API; it requires a different level of thinking than is found in strict unit testing. To\n\n\n\n\n        10","initials":"tr"},"\"The title implies I need to know these languages before I start but the first sentence tells me I need to install them and use them - but not \\\"know\\\" them. \"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\n\nto write code which interacts with the GitHub API and is testable.\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\n\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n      founders of GitHub.\n\n    • JavaScript: the only ubiquitous browser side programming language, its\n      importance has grown to new heights with the introduction of NodeJS,\n\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\n\ntween them you will have a solid toolset to begin exploration of the GitHub API.\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\n\nlearn at least a little bit will make the journey through this book richer for you.\n   Undoubtedly, many of you picking up this book already have familiarity with\n\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\nlanguages; we expect you have experience with other languages as a prerequi-\n\nsite and can read code from any imperative language regardless of the syntax.\nLater chapters which do discuss a facet of the API go into language details at\n\ntimes, and the code is readable regardless of your familiarity with that particu-\nlar language. These explanatory appendices discuss the history of these tools\n\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\ndifferent languages. We do this to describe the API from not only the way the\n\nGitHub team designed it to work, but the aspirational way that client library\nauthors made it work within diverse programming languages and communities.\n\nWe think there is a lot to learn from this approach, but if you are interested in\nonly a specific language and how it works with the GitHub API, this is not the\n\nbook for you.\n\n\n\n\n\n                                                                                            11","initials":"tr"},"\"Not just the founders! \"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\n\nto write code which interacts with the GitHub API and is testable.\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\n\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n      founders of GitHub.\n\n    • JavaScript: the only ubiquitous browser side programming language, its\n      importance has grown to new heights with the introduction of NodeJS,\n\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\n\ntween them you will have a solid toolset to begin exploration of the GitHub API.\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\n\nlearn at least a little bit will make the journey through this book richer for you.\n   Undoubtedly, many of you picking up this book already have familiarity with\n\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\nlanguages; we expect you have experience with other languages as a prerequi-\n\nsite and can read code from any imperative language regardless of the syntax.\nLater chapters which do discuss a facet of the API go into language details at\n\ntimes, and the code is readable regardless of your familiarity with that particu-\nlar language. These explanatory appendices discuss the history of these tools\n\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\ndifferent languages. We do this to describe the API from not only the way the\n\nGitHub team designed it to work, but the aspirational way that client library\nauthors made it work within diverse programming languages and communities.\n\nWe think there is a lot to learn from this approach, but if you are interested in\nonly a specific language and how it works with the GitHub API, this is not the\n\nbook for you.\n\n\n\n\n\n                                                                                            11","initials":"tr"},"\"Do I need to do this before I can go on reading? If so, what resources do you recommend I use to learn enough to comprehend this book?\"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\n\nto write code which interacts with the GitHub API and is testable.\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\n\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n      founders of GitHub.\n\n    • JavaScript: the only ubiquitous browser side programming language, its\n      importance has grown to new heights with the introduction of NodeJS,\n\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\n\ntween them you will have a solid toolset to begin exploration of the GitHub API.\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\n\nlearn at least a little bit will make the journey through this book richer for you.\n   Undoubtedly, many of you picking up this book already have familiarity with\n\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\nlanguages; we expect you have experience with other languages as a prerequi-\n\nsite and can read code from any imperative language regardless of the syntax.\nLater chapters which do discuss a facet of the API go into language details at\n\ntimes, and the code is readable regardless of your familiarity with that particu-\nlar language. These explanatory appendices discuss the history of these tools\n\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\ndifferent languages. We do this to describe the API from not only the way the\n\nGitHub team designed it to work, but the aspirational way that client library\nauthors made it work within diverse programming languages and communities.\n\nWe think there is a lot to learn from this approach, but if you are interested in\nonly a specific language and how it works with the GitHub API, this is not the\n\nbook for you.\n\n\n\n\n\n                                                                                            11","initials":"tr"},"\"Instead of \\\"so\\\" can you say \\\"if not\\\"?\"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\n\nto write code which interacts with the GitHub API and is testable.\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\n\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n      founders of GitHub.\n\n    • JavaScript: the only ubiquitous browser side programming language, its\n      importance has grown to new heights with the introduction of NodeJS,\n\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\n\ntween them you will have a solid toolset to begin exploration of the GitHub API.\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\n\nlearn at least a little bit will make the journey through this book richer for you.\n   Undoubtedly, many of you picking up this book already have familiarity with\n\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\nlanguages; we expect you have experience with other languages as a prerequi-\n\nsite and can read code from any imperative language regardless of the syntax.\nLater chapters which do discuss a facet of the API go into language details at\n\ntimes, and the code is readable regardless of your familiarity with that particu-\nlar language. These explanatory appendices discuss the history of these tools\n\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\ndifferent languages. We do this to describe the API from not only the way the\n\nGitHub team designed it to work, but the aspirational way that client library\nauthors made it work within diverse programming languages and communities.\n\nWe think there is a lot to learn from this approach, but if you are interested in\nonly a specific language and how it works with the GitHub API, this is not the\n\nbook for you.\n\n\n\n\n\n                                                                                            11","initials":"tr"},"\"remove the word \\\"do\\\"\"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\n\nto write code which interacts with the GitHub API and is testable.\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\n\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n      founders of GitHub.\n\n    • JavaScript: the only ubiquitous browser side programming language, its\n      importance has grown to new heights with the introduction of NodeJS,\n\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\n\ntween them you will have a solid toolset to begin exploration of the GitHub API.\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\n\nlearn at least a little bit will make the journey through this book richer for you.\n   Undoubtedly, many of you picking up this book already have familiarity with\n\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\nlanguages; we expect you have experience with other languages as a prerequi-\n\nsite and can read code from any imperative language regardless of the syntax.\nLater chapters which do discuss a facet of the API go into language details at\n\ntimes, and the code is readable regardless of your familiarity with that particu-\nlar language. These explanatory appendices discuss the history of these tools\n\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\ndifferent languages. We do this to describe the API from not only the way the\n\nGitHub team designed it to work, but the aspirational way that client library\nauthors made it work within diverse programming languages and communities.\n\nWe think there is a lot to learn from this approach, but if you are interested in\nonly a specific language and how it works with the GitHub API, this is not the\n\nbook for you.\n\n\n\n\n\n                                                                                            11","initials":"tr"},"\"\\\"you will be disappointed\\\" feels a little harsh. Maybe \\\"you should know we have chosen to look at the API ...\\\"\"":{"page":11,"text":"                                                                     First Class Languages You Need to Know\n\n\nhelp you get past this roadblock, whenever possible, this book shows you how\n\nto write code which interacts with the GitHub API and is testable.\n\n\nFirst Class Languages You Need to Know\n\n\nThere are two languages which are so fundamentally linked to GitHub that you\n\ndo need to install and use them in order to get the most out of this book.\n\n    • Ruby: a simple, readable programming language used heavily by the\n      founders of GitHub.\n\n    • JavaScript: the only ubiquitous browser side programming language, its\n      importance has grown to new heights with the introduction of NodeJS,\n\n      rivaling even the popularity of Ruby on Rails as a server side toolkit for\n      web applications, especially for independent developers.\n\n   Your time will not be wasted if you install and play with these two tools. Be-\n\ntween them you will have a solid toolset to begin exploration of the GitHub API.\nSeveral chapters in this book use Ruby or JavaScript, so putting in some time to\n\nlearn at least a little bit will make the journey through this book richer for you.\n   Undoubtedly, many of you picking up this book already have familiarity with\n\nRuby or JavaScript/NodeJS. So, the basics and installation of them are in ap-\npendices in the back of the book. The appendices don’t cover syntax of these\nlanguages; we expect you have experience with other languages as a prerequi-\n\nsite and can read code from any imperative language regardless of the syntax.\nLater chapters which do discuss a facet of the API go into language details at\n\ntimes, and the code is readable regardless of your familiarity with that particu-\nlar language. These explanatory appendices discuss the history of these tools\n\nwithin the GitHub story as well as important usage notes like special files and\ninstallation options.\n\n\n\nWho This Book is Not For\n\n\nIf you are looking for a discussion of the GitHub API that focuses on a single lan-\nguage, you will be disappointed to find that we look at the API through many\ndifferent languages. We do this to describe the API from not only the way the\n\nGitHub team designed it to work, but the aspirational way that client library\nauthors made it work within diverse programming languages and communities.\n\nWe think there is a lot to learn from this approach, but if you are interested in\nonly a specific language and how it works with the GitHub API, this is not the\n\nbook for you.\n\n\n\n\n\n                                                                                            11","initials":"tr"},"\"should I see icons here?\"":{"page":12,"text":"CHAPTER 1: Preface\n\n\n                       Conventions Used in This Book\n\n\n                       The following typographical conventions are used in this book:\n\n                       Italic\n\n                          Indicates new terms, URLs, email addresses, filenames, and file extensions.\n\n                       Constant width\n                          Used for program listings, as well as within paragraphs to refer to program\n\n                          elements such as variable or function names, databases, data types, envi-\n                          ronment variables, statements, and keywords.\n\n\n                       Constant width bold\n                          Shows commands or other text that should be typed literally by the user.\n\n                       Constant width italic\n\n                          Shows text that should be replaced with user-supplied values or by values\n                          determined by context.\n\n\n                            This icon signifies a tip, suggestion, or general note.\n\n\n\n                            This icon indicates a warning or caution.\n\n\n\n                       Using Code Examples\n\n\n                       PROD: Please reach out to author to find out if they will be uploading code\n                       examples to oreilly.com or their own site (e.g., GitHub). If there is no code\n                       download, delete this whole section. If there is, when you email digidist with\n\n                       the link, let them know what you filled in for title_title (should be as close to\n                       book title as possible, i.e., learning_python_2e). This info will determine\n                       where digidist loads the files.\n                          Supplemental material (code examples, exercises, etc.) is available for\n\n                       download at https://github.com/oreillymedia/title_title.\n                          This book is here to help you get your job done. In general, if example code is\n                       offered with this book, you may use it in your programs and documentation.\n\n                       You do not need to contact us for permission unless you’re reproducing a signif-\n                       icant portion of the code. For example, writing a program that uses several\n\n                       chunks of code from this book does not require permission. Selling or distribut-\n                       ing a CD-ROM of examples from O’Reilly books does require permission. An-\n                       swering a question by citing this book and quoting example code does not re-\n\n                       quire permission. Incorporating a significant amount of example code from this\n                       book into your product’s documentation does require permission.\n\n\n\n\n        12","initials":"tr"},"\"This note is obviously not for me. Just highlighting in case someone forgot to delete it! The author also promised code samples in an earlier section.\"":{"page":12,"text":"CHAPTER 1: Preface\n\n\n                       Conventions Used in This Book\n\n\n                       The following typographical conventions are used in this book:\n\n                       Italic\n\n                          Indicates new terms, URLs, email addresses, filenames, and file extensions.\n\n                       Constant width\n                          Used for program listings, as well as within paragraphs to refer to program\n\n                          elements such as variable or function names, databases, data types, envi-\n                          ronment variables, statements, and keywords.\n\n\n                       Constant width bold\n                          Shows commands or other text that should be typed literally by the user.\n\n                       Constant width italic\n\n                          Shows text that should be replaced with user-supplied values or by values\n                          determined by context.\n\n\n                            This icon signifies a tip, suggestion, or general note.\n\n\n\n                            This icon indicates a warning or caution.\n\n\n\n                       Using Code Examples\n\n\n                       PROD: Please reach out to author to find out if they will be uploading code\n                       examples to oreilly.com or their own site (e.g., GitHub). If there is no code\n                       download, delete this whole section. If there is, when you email digidist with\n\n                       the link, let them know what you filled in for title_title (should be as close to\n                       book title as possible, i.e., learning_python_2e). This info will determine\n                       where digidist loads the files.\n                          Supplemental material (code examples, exercises, etc.) is available for\n\n                       download at https://github.com/oreillymedia/title_title.\n                          This book is here to help you get your job done. In general, if example code is\n                       offered with this book, you may use it in your programs and documentation.\n\n                       You do not need to contact us for permission unless you’re reproducing a signif-\n                       icant portion of the code. For example, writing a program that uses several\n\n                       chunks of code from this book does not require permission. Selling or distribut-\n                       ing a CD-ROM of examples from O’Reilly books does require permission. An-\n                       swering a question by citing this book and quoting example code does not re-\n\n                       quire permission. Incorporating a significant amount of example code from this\n                       book into your product’s documentation does require permission.\n\n\n\n\n        12","initials":"tr"},"\"Technically it is not just what is stored and associated with the git repository. It also accesses some features like orgs and PRs that are GitHub features.\"":{"page":15,"text":"                                          Introduction                     2\n\n\n\n\n\n\n\n\n\nThe GitHub API is extremely comprehensive, permitting access and modifica-\ntion of almost all data and metadata stored or associated with a Git repository.\nHere is a summary of those sections ordered alphabetically as they are on the\nGitHub API documentation site (https://developer.github.com/v3/):\n\n    • Activity: notifications of interesting events in your developer life\n\n    • Gists: programmatically create and share code snippets\n    • Git Data: raw access to Git data over a remote API\n    • Issues: add and modify issues\n\n    • Miscellaneous: whatever does not fit into the general API categorization\n    •Organizations: access and retrieve organizational membership data\n\n    •Pull Requests: a powerful API layer on the popular merge process\n    •Repositories: modify everything and anything related to repositories\n\n    •Search: code driven search within the entire GitHub database\n    •Users: access user data\n\n    •Enterprise: specifics about using the API when using the private corporate\n     GitHub\n\n   In addition, though not a part of the API, there are other important technolo-\ngies you should know about when using GitHub which are not covered in the\nAPI documentation:\n\n    •Jekyll: hosting blogs and static documentation\n    •Gollum: wikis tied to a repository\n\n    •Hubot: a programmable chat robot used extensively at GitHub\n\n   With just one or two exceptions, Each of these sections of the GitHub tech-\nnology stack are covered in various chapters. The GitHub API documentation is\na stellar reference which you will use constantly when writing any application\nthat talks to the API, but the chapters in this book serve a different purpose:\n\n\n\n\n                                                                                   15","initials":"tr"},"\"Are these technologies really essential for GitHub? Or just for the exercises we will complete in this book.\"":{"page":15,"text":"                                          Introduction                     2\n\n\n\n\n\n\n\n\n\nThe GitHub API is extremely comprehensive, permitting access and modifica-\ntion of almost all data and metadata stored or associated with a Git repository.\nHere is a summary of those sections ordered alphabetically as they are on the\nGitHub API documentation site (https://developer.github.com/v3/):\n\n    • Activity: notifications of interesting events in your developer life\n\n    • Gists: programmatically create and share code snippets\n    • Git Data: raw access to Git data over a remote API\n    • Issues: add and modify issues\n\n    • Miscellaneous: whatever does not fit into the general API categorization\n    •Organizations: access and retrieve organizational membership data\n\n    •Pull Requests: a powerful API layer on the popular merge process\n    •Repositories: modify everything and anything related to repositories\n\n    •Search: code driven search within the entire GitHub database\n    •Users: access user data\n\n    •Enterprise: specifics about using the API when using the private corporate\n     GitHub\n\n   In addition, though not a part of the API, there are other important technolo-\ngies you should know about when using GitHub which are not covered in the\nAPI documentation:\n\n    •Jekyll: hosting blogs and static documentation\n    •Gollum: wikis tied to a repository\n\n    •Hubot: a programmable chat robot used extensively at GitHub\n\n   With just one or two exceptions, Each of these sections of the GitHub tech-\nnology stack are covered in various chapters. The GitHub API documentation is\na stellar reference which you will use constantly when writing any application\nthat talks to the API, but the chapters in this book serve a different purpose:\n\n\n\n\n                                                                                   15","initials":"tr"},"\"I like the fact that the search chapter description (and some of the others) talks about the project we will create. I think this section would be more interesting if this was a consistent element of each description. \"":{"page":16,"text":"CHAPTER 2: Introduction\n\n\n                         these chapters are stories about building applications on top of the technolo-\n\n                         gies provided by GitHub. Within these stories you will learn the tradeoffs and\n                         considerations you will face when you use the GitHub API. Chapters in this book\n\n                         often cover multiple pieces of the API when appropriate for the story we are\n                         telling. We’ve generally tried to focus on a major API section and limit exposure\n\n                         to other pieces as much as possible, but most chapters do need to bring in\n                         small pieces of more than one section.\n\n                             • The “cURL” chapter: the chapter you are reading now covers a first look at\n\n                               the API through the command line HTTP client called cURL. We talk a bit\n                               about the response format and how to parse it within the command line,\n                               and also document authentication.\n\n                             • The “Gist” chapter: covers the Gist API, as well as command line tools and\n\n                               the Ruby language “Octokit” API client.\n                             • The “Gollum” chapter: explains usage of the Gollum command line tool\n\n                               and associated Ruby library (gem) which is backed by Grit, the C-\n                               language bindings for accessing Git repositories. We also document some\n\n                               details of the Git storage format and how it applies to storing large files\n                               inside of a Git repository, and show how to use the git command line\n\n                               tools to play with this information.\n                             • The “Search” chapter: we build a GUI search app using Python.\n\n                             • The “Commit Status” chapter: our final chapter documents a relatively\n\n                               new part of the API which documents the interactions between third par-\n                               ty tools and your code. This chapter builds an application using C# and\n\n                               the Nancy .NET GitHub API libraries.\n                             • The “Jekyll” chapter: if you push a specifically organized repository into\n\n                               GitHub, GitHub will host a fully featured blog, equivalent in most ways to\n                               a Wordpress site (well, except for the complexity part). This chapter docu-\n\n                               ments how to format your repository, how to use Markdown within Je-\n                               kyll, how to use programmatic looping constructs provided by Liquid\n                               Templates, and then shows how to import an entire web site from the In-\n\n                               ternet Archive into the Jekyll format using Ruby. We show how to respect-\n                               fully spider a site using caching, a valuable technique when using APIs or\n\n                               third party public information.\n\n                             • The “Android” chapter: in this chapter we create a mobile application tar-\n                               geting the Android OS. Our application reads and writes information into\n                               a Jekyll repository from the Git Data section of the API. We show how to\n\n                               create user interface tests for Android which verify GitHub API responses\n                               using the Calabash UI testing tool.\n\n                             • The “JavaScript” chapter: did you know you can host an entire “single\n\n                               page application” on GitHub? We show how you can build an application\n\n\n\n        16","initials":"tr"},"\"This says \\\"our final chapter\\\" but it is not listed last. Is that a mistake or are the chapters listed here out of order?\"":{"page":16,"text":"CHAPTER 2: Introduction\n\n\n                         these chapters are stories about building applications on top of the technolo-\n\n                         gies provided by GitHub. Within these stories you will learn the tradeoffs and\n                         considerations you will face when you use the GitHub API. Chapters in this book\n\n                         often cover multiple pieces of the API when appropriate for the story we are\n                         telling. We’ve generally tried to focus on a major API section and limit exposure\n\n                         to other pieces as much as possible, but most chapters do need to bring in\n                         small pieces of more than one section.\n\n                             • The “cURL” chapter: the chapter you are reading now covers a first look at\n\n                               the API through the command line HTTP client called cURL. We talk a bit\n                               about the response format and how to parse it within the command line,\n                               and also document authentication.\n\n                             • The “Gist” chapter: covers the Gist API, as well as command line tools and\n\n                               the Ruby language “Octokit” API client.\n                             • The “Gollum” chapter: explains usage of the Gollum command line tool\n\n                               and associated Ruby library (gem) which is backed by Grit, the C-\n                               language bindings for accessing Git repositories. We also document some\n\n                               details of the Git storage format and how it applies to storing large files\n                               inside of a Git repository, and show how to use the git command line\n\n                               tools to play with this information.\n                             • The “Search” chapter: we build a GUI search app using Python.\n\n                             • The “Commit Status” chapter: our final chapter documents a relatively\n\n                               new part of the API which documents the interactions between third par-\n                               ty tools and your code. This chapter builds an application using C# and\n\n                               the Nancy .NET GitHub API libraries.\n                             • The “Jekyll” chapter: if you push a specifically organized repository into\n\n                               GitHub, GitHub will host a fully featured blog, equivalent in most ways to\n                               a Wordpress site (well, except for the complexity part). This chapter docu-\n\n                               ments how to format your repository, how to use Markdown within Je-\n                               kyll, how to use programmatic looping constructs provided by Liquid\n                               Templates, and then shows how to import an entire web site from the In-\n\n                               ternet Archive into the Jekyll format using Ruby. We show how to respect-\n                               fully spider a site using caching, a valuable technique when using APIs or\n\n                               third party public information.\n\n                             • The “Android” chapter: in this chapter we create a mobile application tar-\n                               geting the Android OS. Our application reads and writes information into\n                               a Jekyll repository from the Git Data section of the API. We show how to\n\n                               create user interface tests for Android which verify GitHub API responses\n                               using the Calabash UI testing tool.\n\n                             • The “JavaScript” chapter: did you know you can host an entire “single\n\n                               page application” on GitHub? We show how you can build an application\n\n\n\n        16","initials":"tr"},"\"\\\"backed by a database called GitHub\\\" is this a mistake?\"":{"page":17,"text":"                                                                                                Introduction\n\n\n      backed by a database called GitHub using the JavaScript language. Im-\n\n      portanly, we show how you can write a testable JavaScript application\n      that mocks out the GitHub API when needed.\n\n    • The “Hubot” chapter: Hubot is a JavaScript (NodeJS) chat robot enabling\n      technologists to go beyond developer operations (“DevOps”) to a new\n\n      frontier called “ChatOps.” The Hubot chapter illustrates using the Activi-\n      ties and Pull Requests section of the API. In addition we show how you\n\n      can simulate GitHub notifications and how to write testable Hubot exten-\n      sions (which is often a challenge when writing JavaScript code).\n\n   We don’t cover the organization API: this is a small facet of the API with only\n\nthe ability to list organizations and modify metadata about your organization;\nonce you have used other parts of the API this nook of the API will be very intu-\n\nitive.\n   We also don’t cover the users section of the API. While you might expect it to\n\nbe an important part of the API, the users API is really nothing more than an\nendpoint to list information about users, add or remove SSH keys, adjust email\naddresses and modify your list of followers.\n\n   There is not a specific chapter on issues. Historically GitHub used to group\nissues and pull requests into the same API section, but with the growing impor-\n\ntance of pull requests they have separated them in the API documentation. In\nfact, they are still internally stored in the same database and pull requests are,\n\nat least for now, just another type of issue. The Hubot chapter documents using\npull requests and is a good reference for issues in that way.\n\n   The enterprise API works almost exactly the same as the GitHub.com site\nAPI. We don’t have a chapter telling a story about the enterprise API, but we do\nprovide an appendix which provides a few notes about how to use it with a few\n\nAPI client libraries.\n   With these chapters we cover the entire API and hope to give you an inside\n\nlook into the inner workings of the brain of a developer building on top of the\nGitHub API.\n\n   As you might have noticed, this book will take you on an exploration of sev-\neral different language clients for the GitHub API. Along the way, we’ll point out\nthe different idioms and methodologies inherent to those client libraries and\n\nshed light on the darker corners of the GitHub API. Don’t be alarmed if you\nthumb through the chapters and see a language which you don’t know at all:\n\neach chapter is designed so that you can follow along without intimacy to the\nlanguage or toolkit. You will get the most value if you install the language and\n\nassociated tools, but the story behind the projects we will build will be interest-\ning even if you don’t actually type a line of code from the chapter.\n\n   Enough of the theoretical: let’s jump into using the API with the powerful\ncURL tool.\n\n\n\n\n                                                                                                 17","initials":"tr"},"\"spelling error: importantly\"":{"page":17,"text":"                                                                                                Introduction\n\n\n      backed by a database called GitHub using the JavaScript language. Im-\n\n      portanly, we show how you can write a testable JavaScript application\n      that mocks out the GitHub API when needed.\n\n    • The “Hubot” chapter: Hubot is a JavaScript (NodeJS) chat robot enabling\n      technologists to go beyond developer operations (“DevOps”) to a new\n\n      frontier called “ChatOps.” The Hubot chapter illustrates using the Activi-\n      ties and Pull Requests section of the API. In addition we show how you\n\n      can simulate GitHub notifications and how to write testable Hubot exten-\n      sions (which is often a challenge when writing JavaScript code).\n\n   We don’t cover the organization API: this is a small facet of the API with only\n\nthe ability to list organizations and modify metadata about your organization;\nonce you have used other parts of the API this nook of the API will be very intu-\n\nitive.\n   We also don’t cover the users section of the API. While you might expect it to\n\nbe an important part of the API, the users API is really nothing more than an\nendpoint to list information about users, add or remove SSH keys, adjust email\naddresses and modify your list of followers.\n\n   There is not a specific chapter on issues. Historically GitHub used to group\nissues and pull requests into the same API section, but with the growing impor-\n\ntance of pull requests they have separated them in the API documentation. In\nfact, they are still internally stored in the same database and pull requests are,\n\nat least for now, just another type of issue. The Hubot chapter documents using\npull requests and is a good reference for issues in that way.\n\n   The enterprise API works almost exactly the same as the GitHub.com site\nAPI. We don’t have a chapter telling a story about the enterprise API, but we do\nprovide an appendix which provides a few notes about how to use it with a few\n\nAPI client libraries.\n   With these chapters we cover the entire API and hope to give you an inside\n\nlook into the inner workings of the brain of a developer building on top of the\nGitHub API.\n\n   As you might have noticed, this book will take you on an exploration of sev-\neral different language clients for the GitHub API. Along the way, we’ll point out\nthe different idioms and methodologies inherent to those client libraries and\n\nshed light on the darker corners of the GitHub API. Don’t be alarmed if you\nthumb through the chapters and see a language which you don’t know at all:\n\neach chapter is designed so that you can follow along without intimacy to the\nlanguage or toolkit. You will get the most value if you install the language and\n\nassociated tools, but the story behind the projects we will build will be interest-\ning even if you don’t actually type a line of code from the chapter.\n\n   Enough of the theoretical: let’s jump into using the API with the powerful\ncURL tool.\n\n\n\n\n                                                                                                 17","initials":"tr"},"\"should these be in the earlier section about what this book will not cover? It seems strange to mention them here when we didn't mention them earlier. It also seems like future updates to the API could date this section of the book.\"":{"page":17,"text":"                                                                                                Introduction\n\n\n      backed by a database called GitHub using the JavaScript language. Im-\n\n      portanly, we show how you can write a testable JavaScript application\n      that mocks out the GitHub API when needed.\n\n    • The “Hubot” chapter: Hubot is a JavaScript (NodeJS) chat robot enabling\n      technologists to go beyond developer operations (“DevOps”) to a new\n\n      frontier called “ChatOps.” The Hubot chapter illustrates using the Activi-\n      ties and Pull Requests section of the API. In addition we show how you\n\n      can simulate GitHub notifications and how to write testable Hubot exten-\n      sions (which is often a challenge when writing JavaScript code).\n\n   We don’t cover the organization API: this is a small facet of the API with only\n\nthe ability to list organizations and modify metadata about your organization;\nonce you have used other parts of the API this nook of the API will be very intu-\n\nitive.\n   We also don’t cover the users section of the API. While you might expect it to\n\nbe an important part of the API, the users API is really nothing more than an\nendpoint to list information about users, add or remove SSH keys, adjust email\naddresses and modify your list of followers.\n\n   There is not a specific chapter on issues. Historically GitHub used to group\nissues and pull requests into the same API section, but with the growing impor-\n\ntance of pull requests they have separated them in the API documentation. In\nfact, they are still internally stored in the same database and pull requests are,\n\nat least for now, just another type of issue. The Hubot chapter documents using\npull requests and is a good reference for issues in that way.\n\n   The enterprise API works almost exactly the same as the GitHub.com site\nAPI. We don’t have a chapter telling a story about the enterprise API, but we do\nprovide an appendix which provides a few notes about how to use it with a few\n\nAPI client libraries.\n   With these chapters we cover the entire API and hope to give you an inside\n\nlook into the inner workings of the brain of a developer building on top of the\nGitHub API.\n\n   As you might have noticed, this book will take you on an exploration of sev-\neral different language clients for the GitHub API. Along the way, we’ll point out\nthe different idioms and methodologies inherent to those client libraries and\n\nshed light on the darker corners of the GitHub API. Don’t be alarmed if you\nthumb through the chapters and see a language which you don’t know at all:\n\neach chapter is designed so that you can follow along without intimacy to the\nlanguage or toolkit. You will get the most value if you install the language and\n\nassociated tools, but the story behind the projects we will build will be interest-\ning even if you don’t actually type a line of code from the chapter.\n\n   Enough of the theoretical: let’s jump into using the API with the powerful\ncURL tool.\n\n\n\n\n                                                                                                 17","initials":"tr"},"\"\\\"used to\\\" can be removed and replaced with an -ed on the end of group\"":{"page":17,"text":"                                                                                                Introduction\n\n\n      backed by a database called GitHub using the JavaScript language. Im-\n\n      portanly, we show how you can write a testable JavaScript application\n      that mocks out the GitHub API when needed.\n\n    • The “Hubot” chapter: Hubot is a JavaScript (NodeJS) chat robot enabling\n      technologists to go beyond developer operations (“DevOps”) to a new\n\n      frontier called “ChatOps.” The Hubot chapter illustrates using the Activi-\n      ties and Pull Requests section of the API. In addition we show how you\n\n      can simulate GitHub notifications and how to write testable Hubot exten-\n      sions (which is often a challenge when writing JavaScript code).\n\n   We don’t cover the organization API: this is a small facet of the API with only\n\nthe ability to list organizations and modify metadata about your organization;\nonce you have used other parts of the API this nook of the API will be very intu-\n\nitive.\n   We also don’t cover the users section of the API. While you might expect it to\n\nbe an important part of the API, the users API is really nothing more than an\nendpoint to list information about users, add or remove SSH keys, adjust email\naddresses and modify your list of followers.\n\n   There is not a specific chapter on issues. Historically GitHub used to group\nissues and pull requests into the same API section, but with the growing impor-\n\ntance of pull requests they have separated them in the API documentation. In\nfact, they are still internally stored in the same database and pull requests are,\n\nat least for now, just another type of issue. The Hubot chapter documents using\npull requests and is a good reference for issues in that way.\n\n   The enterprise API works almost exactly the same as the GitHub.com site\nAPI. We don’t have a chapter telling a story about the enterprise API, but we do\nprovide an appendix which provides a few notes about how to use it with a few\n\nAPI client libraries.\n   With these chapters we cover the entire API and hope to give you an inside\n\nlook into the inner workings of the brain of a developer building on top of the\nGitHub API.\n\n   As you might have noticed, this book will take you on an exploration of sev-\neral different language clients for the GitHub API. Along the way, we’ll point out\nthe different idioms and methodologies inherent to those client libraries and\n\nshed light on the darker corners of the GitHub API. Don’t be alarmed if you\nthumb through the chapters and see a language which you don’t know at all:\n\neach chapter is designed so that you can follow along without intimacy to the\nlanguage or toolkit. You will get the most value if you install the language and\n\nassociated tools, but the story behind the projects we will build will be interest-\ning even if you don’t actually type a line of code from the chapter.\n\n   Enough of the theoretical: let’s jump into using the API with the powerful\ncURL tool.\n\n\n\n\n                                                                                                 17","initials":"tr"},"\"\\\"we cover the entire API\\\" contradicts what you said in the paragraphs above\"":{"page":17,"text":"                                                                                                Introduction\n\n\n      backed by a database called GitHub using the JavaScript language. Im-\n\n      portanly, we show how you can write a testable JavaScript application\n      that mocks out the GitHub API when needed.\n\n    • The “Hubot” chapter: Hubot is a JavaScript (NodeJS) chat robot enabling\n      technologists to go beyond developer operations (“DevOps”) to a new\n\n      frontier called “ChatOps.” The Hubot chapter illustrates using the Activi-\n      ties and Pull Requests section of the API. In addition we show how you\n\n      can simulate GitHub notifications and how to write testable Hubot exten-\n      sions (which is often a challenge when writing JavaScript code).\n\n   We don’t cover the organization API: this is a small facet of the API with only\n\nthe ability to list organizations and modify metadata about your organization;\nonce you have used other parts of the API this nook of the API will be very intu-\n\nitive.\n   We also don’t cover the users section of the API. While you might expect it to\n\nbe an important part of the API, the users API is really nothing more than an\nendpoint to list information about users, add or remove SSH keys, adjust email\naddresses and modify your list of followers.\n\n   There is not a specific chapter on issues. Historically GitHub used to group\nissues and pull requests into the same API section, but with the growing impor-\n\ntance of pull requests they have separated them in the API documentation. In\nfact, they are still internally stored in the same database and pull requests are,\n\nat least for now, just another type of issue. The Hubot chapter documents using\npull requests and is a good reference for issues in that way.\n\n   The enterprise API works almost exactly the same as the GitHub.com site\nAPI. We don’t have a chapter telling a story about the enterprise API, but we do\nprovide an appendix which provides a few notes about how to use it with a few\n\nAPI client libraries.\n   With these chapters we cover the entire API and hope to give you an inside\n\nlook into the inner workings of the brain of a developer building on top of the\nGitHub API.\n\n   As you might have noticed, this book will take you on an exploration of sev-\neral different language clients for the GitHub API. Along the way, we’ll point out\nthe different idioms and methodologies inherent to those client libraries and\n\nshed light on the darker corners of the GitHub API. Don’t be alarmed if you\nthumb through the chapters and see a language which you don’t know at all:\n\neach chapter is designed so that you can follow along without intimacy to the\nlanguage or toolkit. You will get the most value if you install the language and\n\nassociated tools, but the story behind the projects we will build will be interest-\ning even if you don’t actually type a line of code from the chapter.\n\n   Enough of the theoretical: let’s jump into using the API with the powerful\ncURL tool.\n\n\n\n\n                                                                                                 17","initials":"tr"},"\"I feel like a lot of this paragraph is redundant. \"":{"page":17,"text":"                                                                                                Introduction\n\n\n      backed by a database called GitHub using the JavaScript language. Im-\n\n      portanly, we show how you can write a testable JavaScript application\n      that mocks out the GitHub API when needed.\n\n    • The “Hubot” chapter: Hubot is a JavaScript (NodeJS) chat robot enabling\n      technologists to go beyond developer operations (“DevOps”) to a new\n\n      frontier called “ChatOps.” The Hubot chapter illustrates using the Activi-\n      ties and Pull Requests section of the API. In addition we show how you\n\n      can simulate GitHub notifications and how to write testable Hubot exten-\n      sions (which is often a challenge when writing JavaScript code).\n\n   We don’t cover the organization API: this is a small facet of the API with only\n\nthe ability to list organizations and modify metadata about your organization;\nonce you have used other parts of the API this nook of the API will be very intu-\n\nitive.\n   We also don’t cover the users section of the API. While you might expect it to\n\nbe an important part of the API, the users API is really nothing more than an\nendpoint to list information about users, add or remove SSH keys, adjust email\naddresses and modify your list of followers.\n\n   There is not a specific chapter on issues. Historically GitHub used to group\nissues and pull requests into the same API section, but with the growing impor-\n\ntance of pull requests they have separated them in the API documentation. In\nfact, they are still internally stored in the same database and pull requests are,\n\nat least for now, just another type of issue. The Hubot chapter documents using\npull requests and is a good reference for issues in that way.\n\n   The enterprise API works almost exactly the same as the GitHub.com site\nAPI. We don’t have a chapter telling a story about the enterprise API, but we do\nprovide an appendix which provides a few notes about how to use it with a few\n\nAPI client libraries.\n   With these chapters we cover the entire API and hope to give you an inside\n\nlook into the inner workings of the brain of a developer building on top of the\nGitHub API.\n\n   As you might have noticed, this book will take you on an exploration of sev-\neral different language clients for the GitHub API. Along the way, we’ll point out\nthe different idioms and methodologies inherent to those client libraries and\n\nshed light on the darker corners of the GitHub API. Don’t be alarmed if you\nthumb through the chapters and see a language which you don’t know at all:\n\neach chapter is designed so that you can follow along without intimacy to the\nlanguage or toolkit. You will get the most value if you install the language and\n\nassociated tools, but the story behind the projects we will build will be interest-\ning even if you don’t actually type a line of code from the chapter.\n\n   Enough of the theoretical: let’s jump into using the API with the powerful\ncURL tool.\n\n\n\n\n                                                                                                 17","initials":"tr"},"\"\\\"like the HTTP protocol which it speaks intimately\\\" this phrase reads a little weird. I feel like it is missing a word.\"":{"page":18,"text":"CHAPTER 2: Introduction\n\n\n                       cURL: a starting point for API exploration\n\n\n                       There will be times when you want to quickly access information from the API\n\n                       without writing a formal program. Or, when you want to quickly get access to\n                       the raw HTTP request headers and content. Or, where you might even question\n\n                       the implementation of a client library and need confirmation it is doing the\n                       right thing from another vantage point. In these situations, cURL, a simple com-\n\n                       mand line HTTP tool, is the perfect fit. cURL, like the best unix tools, is a small\n                       program with a very specific and purposefully limited set of features for access-\n\n                       ing HTTP servers.\n                          cURL gives you the most naked vantage point you will find for the GitHub API\n\n                       (barring using a network traffic analysis tool). cURL, like the HTTP protocol\n                       which it speaks intimately, is stateless, meaning it is challenging to use cURL to\n\n                       hit a service and then use the results with a secondary request. We will use\n                       cURL in a later chapter within a shell script that explores solutions to this prob-\n\n                       lem, but note that cURL works best with one-off requests.\n\n\n                            INSTALLING CURL\n\n                            If you are running these examples on a Linux box, you should be able use\n                            your native package management tool to install cURL - either a “sudo\n\n                            yum install curl*” on a RedHat variant or “sudo apt-get install curl” on\n                            an Ubuntu (or Debian) system. If you are on any recent version of OSX,\n                            you already have cURL installed, but it you can’t find it, take a look at the\n\n                            HomeBrew project (http://brew.sh/) or MacPorts project (http://\n                            www.macports.org/). If you are running on Windows or another operating\n                            system, you best bet is to download cURL from the cURL web site here:\n                            http://curl.haxx.se/download.html\n\n\n\n                          Let’s make a request. We’ll start with the most basic GitHub API endpoint\n                       found at https://api.github.com.\n\n\n                          $ curl https://api.github.com\n                          {\n\n                             \"current_user_url\": \"https://api.github.com/user\",\n                             \"current_user_authorizations_html_url\":\n                             \"https://github.com/settings/connections/applications{/client_id}\",\n\n                             \"authorizations_url\": \"https://api.github.com/authorizations\",\n                             \"code_search_url\":\n                             \"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\n                             \"emails_url\": \"https://api.github.com/user/emails\",\n\n                             \"emojis_url\": \"https://api.github.com/emojis\",\n                             ...\n                          }\n\n\n\n\n\n\n        18","initials":"tr"},"\"TMI on what we will do in the later chapter. Perhaps just \\\"We will explore solutions to this limitation in a later chapter, but note that cURL works best with one-off requests.\\\"\"":{"page":18,"text":"CHAPTER 2: Introduction\n\n\n                       cURL: a starting point for API exploration\n\n\n                       There will be times when you want to quickly access information from the API\n\n                       without writing a formal program. Or, when you want to quickly get access to\n                       the raw HTTP request headers and content. Or, where you might even question\n\n                       the implementation of a client library and need confirmation it is doing the\n                       right thing from another vantage point. In these situations, cURL, a simple com-\n\n                       mand line HTTP tool, is the perfect fit. cURL, like the best unix tools, is a small\n                       program with a very specific and purposefully limited set of features for access-\n\n                       ing HTTP servers.\n                          cURL gives you the most naked vantage point you will find for the GitHub API\n\n                       (barring using a network traffic analysis tool). cURL, like the HTTP protocol\n                       which it speaks intimately, is stateless, meaning it is challenging to use cURL to\n\n                       hit a service and then use the results with a secondary request. We will use\n                       cURL in a later chapter within a shell script that explores solutions to this prob-\n\n                       lem, but note that cURL works best with one-off requests.\n\n\n                            INSTALLING CURL\n\n                            If you are running these examples on a Linux box, you should be able use\n                            your native package management tool to install cURL - either a “sudo\n\n                            yum install curl*” on a RedHat variant or “sudo apt-get install curl” on\n                            an Ubuntu (or Debian) system. If you are on any recent version of OSX,\n                            you already have cURL installed, but it you can’t find it, take a look at the\n\n                            HomeBrew project (http://brew.sh/) or MacPorts project (http://\n                            www.macports.org/). If you are running on Windows or another operating\n                            system, you best bet is to download cURL from the cURL web site here:\n                            http://curl.haxx.se/download.html\n\n\n\n                          Let’s make a request. We’ll start with the most basic GitHub API endpoint\n                       found at https://api.github.com.\n\n\n                          $ curl https://api.github.com\n                          {\n\n                             \"current_user_url\": \"https://api.github.com/user\",\n                             \"current_user_authorizations_html_url\":\n                             \"https://github.com/settings/connections/applications{/client_id}\",\n\n                             \"authorizations_url\": \"https://api.github.com/authorizations\",\n                             \"code_search_url\":\n                             \"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\n                             \"emails_url\": \"https://api.github.com/user/emails\",\n\n                             \"emojis_url\": \"https://api.github.com/emojis\",\n                             ...\n                          }\n\n\n\n\n\n\n        18","initials":"tr"},"\"Can we have some line breaks here so I can find my OS without reading about all of the others?\"":{"page":18,"text":"CHAPTER 2: Introduction\n\n\n                       cURL: a starting point for API exploration\n\n\n                       There will be times when you want to quickly access information from the API\n\n                       without writing a formal program. Or, when you want to quickly get access to\n                       the raw HTTP request headers and content. Or, where you might even question\n\n                       the implementation of a client library and need confirmation it is doing the\n                       right thing from another vantage point. In these situations, cURL, a simple com-\n\n                       mand line HTTP tool, is the perfect fit. cURL, like the best unix tools, is a small\n                       program with a very specific and purposefully limited set of features for access-\n\n                       ing HTTP servers.\n                          cURL gives you the most naked vantage point you will find for the GitHub API\n\n                       (barring using a network traffic analysis tool). cURL, like the HTTP protocol\n                       which it speaks intimately, is stateless, meaning it is challenging to use cURL to\n\n                       hit a service and then use the results with a secondary request. We will use\n                       cURL in a later chapter within a shell script that explores solutions to this prob-\n\n                       lem, but note that cURL works best with one-off requests.\n\n\n                            INSTALLING CURL\n\n                            If you are running these examples on a Linux box, you should be able use\n                            your native package management tool to install cURL - either a “sudo\n\n                            yum install curl*” on a RedHat variant or “sudo apt-get install curl” on\n                            an Ubuntu (or Debian) system. If you are on any recent version of OSX,\n                            you already have cURL installed, but it you can’t find it, take a look at the\n\n                            HomeBrew project (http://brew.sh/) or MacPorts project (http://\n                            www.macports.org/). If you are running on Windows or another operating\n                            system, you best bet is to download cURL from the cURL web site here:\n                            http://curl.haxx.se/download.html\n\n\n\n                          Let’s make a request. We’ll start with the most basic GitHub API endpoint\n                       found at https://api.github.com.\n\n\n                          $ curl https://api.github.com\n                          {\n\n                             \"current_user_url\": \"https://api.github.com/user\",\n                             \"current_user_authorizations_html_url\":\n                             \"https://github.com/settings/connections/applications{/client_id}\",\n\n                             \"authorizations_url\": \"https://api.github.com/authorizations\",\n                             \"code_search_url\":\n                             \"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\n                             \"emails_url\": \"https://api.github.com/user/emails\",\n\n                             \"emojis_url\": \"https://api.github.com/emojis\",\n                             ...\n                          }\n\n\n\n\n\n\n        18","initials":"tr"},"\"I think \\\"it\\\" should be \\\"if\\\". So this would read \\\"but if you can't find it\\\"\"":{"page":18,"text":"CHAPTER 2: Introduction\n\n\n                       cURL: a starting point for API exploration\n\n\n                       There will be times when you want to quickly access information from the API\n\n                       without writing a formal program. Or, when you want to quickly get access to\n                       the raw HTTP request headers and content. Or, where you might even question\n\n                       the implementation of a client library and need confirmation it is doing the\n                       right thing from another vantage point. In these situations, cURL, a simple com-\n\n                       mand line HTTP tool, is the perfect fit. cURL, like the best unix tools, is a small\n                       program with a very specific and purposefully limited set of features for access-\n\n                       ing HTTP servers.\n                          cURL gives you the most naked vantage point you will find for the GitHub API\n\n                       (barring using a network traffic analysis tool). cURL, like the HTTP protocol\n                       which it speaks intimately, is stateless, meaning it is challenging to use cURL to\n\n                       hit a service and then use the results with a secondary request. We will use\n                       cURL in a later chapter within a shell script that explores solutions to this prob-\n\n                       lem, but note that cURL works best with one-off requests.\n\n\n                            INSTALLING CURL\n\n                            If you are running these examples on a Linux box, you should be able use\n                            your native package management tool to install cURL - either a “sudo\n\n                            yum install curl*” on a RedHat variant or “sudo apt-get install curl” on\n                            an Ubuntu (or Debian) system. If you are on any recent version of OSX,\n                            you already have cURL installed, but it you can’t find it, take a look at the\n\n                            HomeBrew project (http://brew.sh/) or MacPorts project (http://\n                            www.macports.org/). If you are running on Windows or another operating\n                            system, you best bet is to download cURL from the cURL web site here:\n                            http://curl.haxx.se/download.html\n\n\n\n                          Let’s make a request. We’ll start with the most basic GitHub API endpoint\n                       found at https://api.github.com.\n\n\n                          $ curl https://api.github.com\n                          {\n\n                             \"current_user_url\": \"https://api.github.com/user\",\n                             \"current_user_authorizations_html_url\":\n                             \"https://github.com/settings/connections/applications{/client_id}\",\n\n                             \"authorizations_url\": \"https://api.github.com/authorizations\",\n                             \"code_search_url\":\n                             \"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\n                             \"emails_url\": \"https://api.github.com/user/emails\",\n\n                             \"emojis_url\": \"https://api.github.com/emojis\",\n                             ...\n                          }\n\n\n\n\n\n\n        18","initials":"tr"}}